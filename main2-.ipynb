{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL Brain project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O10test1.txt   O2valid3.txt  O4valid2.txt  O6valid1.txt  O8train.txt\r\n",
      "O10test2.txt   O3test1.txt   O4valid3.txt  O6valid2.txt  O8valid1.txt\r\n",
      "O10test3.txt   O3test2.txt   O5test1.txt   O6valid3.txt  O8valid2.txt\r\n",
      "O10train.txt   O3test3.txt   O5test2.txt   O7test1.txt   O8valid3.txt\r\n",
      "O10valid1.txt  O3train.txt   O5test3.txt   O7test2.txt   O9test1.txt\r\n",
      "O10valid2.txt  O3valid1.txt  O5train.txt   O7test3.txt   O9test2.txt\r\n",
      "O10valid3.txt  O3valid2.txt  O5valid1.txt  O7train.txt   O9test3.txt\r\n",
      "O2test1.txt    O3valid3.txt  O5valid2.txt  O7valid1.txt  O9train.txt\r\n",
      "O2test2.txt    O4test1.txt   O5valid3.txt  O7valid2.txt  O9valid1.txt\r\n",
      "O2test3.txt    O4test2.txt   O6test1.txt   O7valid3.txt  O9valid2.txt\r\n",
      "O2train.txt    O4test3.txt   O6test2.txt   O8test1.txt   O9valid3.txt\r\n",
      "O2valid1.txt   O4train.txt   O6test3.txt   O8test2.txt\r\n",
      "O2valid2.txt   O4valid1.txt  O6train.txt   O8test3.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls /home/arasdar/datasets/DL-BrainBody/Control/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P10test1.txt   P11valid3.txt  P4valid2.txt  P6valid1.txt  P8train.txt\r\n",
      "P10test2.txt   P1test1.txt    P4valid3.txt  P6valid2.txt  P8valid1.txt\r\n",
      "P10test3.txt   P1test2.txt    P5test1.txt   P6valid3.txt  P8valid2.txt\r\n",
      "P10train.txt   P1test3.txt    P5test2.txt   P7test1.txt   P8valid3.txt\r\n",
      "P10valid1.txt  P1train.txt    P5test3.txt   P7test2.txt   P9test1.txt\r\n",
      "P10valid2.txt  P1valid1.txt   P5train.txt   P7test3.txt   P9test2.txt\r\n",
      "P10valid3.txt  P1valid2.txt   P5valid1.txt  P7train.txt   P9test3.txt\r\n",
      "P11test1.txt   P1valid3.txt   P5valid2.txt  P7valid1.txt  P9train.txt\r\n",
      "P11test2.txt   P4test1.txt    P5valid3.txt  P7valid2.txt  P9valid1.txt\r\n",
      "P11test3.txt   P4test2.txt    P6test1.txt   P7valid3.txt  P9valid2.txt\r\n",
      "P11train.txt   P4test3.txt    P6test2.txt   P8test1.txt   P9valid3.txt\r\n",
      "P11valid1.txt  P4train.txt    P6test3.txt   P8test2.txt\r\n",
      "P11valid2.txt  P4valid1.txt   P6train.txt   P8test3.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls /home/arasdar/datasets/DL-BrainBody/PD/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_02train = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/Control/O2train.txt', header=None, \n",
    "                        delim_whitespace=True)\n",
    "X_03train = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/Control/O3train.txt', header=None, \n",
    "                        delim_whitespace=True)\n",
    "X_04train = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/Control/O4train.txt', header=None, \n",
    "                        delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_02val1 = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/Control/O2valid1.txt', header=None, \n",
    "                        delim_whitespace=True)\n",
    "X_02val2 = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/Control/O2valid2.txt', header=None, \n",
    "                        delim_whitespace=True)\n",
    "X_02val3 = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/Control/O2valid3.txt', header=None, \n",
    "                        delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_02test1 = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/Control/O2test1.txt', header=None, \n",
    "                       delim_whitespace=True)\n",
    "X_02test2 = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/Control/O2test2.txt', header=None, \n",
    "                       delim_whitespace=True)\n",
    "X_02test3 = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/Control/O2test3.txt', header=None, \n",
    "                       delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_10train = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/PD/P10train.txt', header=None, \n",
    "                        delim_whitespace=True)\n",
    "X_11train = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/PD/P11train.txt', header=None, \n",
    "                        delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtrain = np.concatenate([X_02train, X_03train, X_04train, X_10train, X_11train], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = np.array([X_02train.values, X_03train.values, X_04train.values, X_10train.values, X_11train.values])\n",
    "Ytrain = np.array([               0,                0,                0,                1,                1])\n",
    "# Ytrain = np.array([Controllllllllll, Controllllllllll, Controllllllllll, PDDDDDDDDDDDDDDD, PDDDDDDDDDDDDD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 81920, 56), (5,), dtype('float64'), dtype('int64'))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape, Ytrain.shape, Xtrain.dtype, Ytrain.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79872, 56), (1, 39, 8, 256, 56))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_02test1.shape, X_02test1.values.reshape([1, -1, 8, 256, 56]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79872, 56), (1, 39, 8, 256, 56))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_02test2.shape, X_02test2.values.reshape([1, -1, 8, 256, 56]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((81920, 56), (1, 40, 8, 256, 56))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_02test3.shape, X_02test3.values.reshape([1, -1, 8, 256, 56]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40960, 56), (1, 20, 8, 256, 56))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_02val1.shape, X_02val1.values.reshape([1, -1, 8, 256, 56]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40960, 56), (1, 20, 8, 256, 56))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_02val2.shape, X_02val2.values.reshape([1, -1, 8, 256, 56]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38912, 56), (1, 19, 8, 256, 56))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_02val3.shape, X_02val3.values.reshape([1, -1, 8, 256, 56]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 1, 81920, 56), (5,))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.reshape([5, 1, -1, 56]).shape, Ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_02train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_03train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79872, 56), (79872, 56), (81920, 56), (40960, 56), (40960, 56), (38912, 56))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_02train.shape, X_03train.shape, X_04train.shape, X_10train.shape, X_11train.shape, \\\n",
    "X_02test1.shape, X_02test2.shape, X_02test3.shape, X_02val1.shape, X_02val2.shape, X_02val3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_02train.values.dtype, X_03train.values.dtype, X_04train.values.dtype, X_10train.values.dtype, X_11train.values.dtype, \\\n",
    "X_02test1.values.dtype, X_02test2.values.dtype, X_02test3.values.dtype, X_02val1.values.dtype, \\\n",
    "X_02val2.values.dtype, X_02val3.values.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 81920, 56), (5,))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# five subjects, 8 activities, 10 seconds each, sr=256 samples/sec\n",
    "Xtrain.shape, Ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 40, 8, 256, 56)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 256 sampling rate: 256 samples per second\n",
    "Xtrain.reshape(5, 40, 8, 256, 56).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = Xtrain[0].shape\n",
    "input_size = (8, 256, 56) # one second of each activity which is 1024=256sample/sec*4sec\n",
    "output_size = 2 # num classes\n",
    "hidden_size = 56*2*2*2*2*2*2\n",
    "batch_size = 40\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size, output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(input_size, hidden_size, batch_size):\n",
    "    # CNN and MLP\n",
    "    inputs = tf.placeholder(dtype=tf.float32, shape=[None, *input_size], name='inputs')\n",
    "    labels = tf.placeholder(dtype=tf.int32, shape=[None], name='labels')    \n",
    "    # RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(num_units=hidden_size)\n",
    "    #cell = tf.nn.rnn_cell.LSTMCell(hidden_size)\n",
    "    cells = tf.nn.rnn_cell.MultiRNNCell(cells=[cell], state_is_tuple=True)\n",
    "    initial_state = cells.zero_state(batch_size, tf.float32)\n",
    "    # Batchnorm training\n",
    "    training = tf.placeholder(dtype=tf.bool, shape=[], name='training')\n",
    "    return inputs, labels, cells, initial_state, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs, labels, cell, initial_state = model_input(input_size=input_size, batch_size=batch_size, \n",
    "#                                                   hidden_size=hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs.shape, labels.shape, cell, initial_state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h1 = tf.layers.conv2d(inputs=inputs, filters=56*2, kernel_size=(1, 5), strides=(1, 2), padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h2 = tf.layers.conv2d(inputs=h1, filters=56*2*2, kernel_size=(1, 5), strides=(1, 2), padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h3 = tf.layers.conv2d(inputs=h2, filters=56*2*2*2, kernel_size=(1, 5), strides=(1, 2), padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NHWC format\n",
    "# h1 = tf.layers.conv2d(inputs=inputs, filters=56*2, kernel_size=[8, 3], strides=[1, 4], padding='same')\n",
    "# h2 = tf.layers.conv2d(inputs=h1, filters=56*2*2, kernel_size=[8, 3], strides=[1, 4], padding='same')\n",
    "# h3 = tf.layers.conv2d(inputs=h2, filters=56*2*2*2, kernel_size=[8, 3], strides=[1, 4], padding='same')\n",
    "# h4 = tf.layers.conv2d(inputs=h3, filters=56*2*2*2*2, kernel_size=[8, 3], strides=[1, 4], padding='same')\n",
    "# h5 = tf.layers.conv2d(inputs=h4, filters=56*2*2*2*2*2, kernel_size=[8, 3], strides=[8, 4], padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h1.shape, h2.shape, h3.shape, h4.shape, h5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(inputs, input_size, output_size, # CNN\n",
    "                  batch_size, initial_state, cells, # RNN/MLP/FC\n",
    "                  alpha=0.1, training=False, reuse=False): # NL/BN\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # CNN\n",
    "        # NHWC format\n",
    "        print(inputs.shape)\n",
    "        inputs_cnn = tf.reshape(inputs, [-1, *input_size]) # ?, 40, 8, 256, 56 -> ?, 8, 256, 56\n",
    "        print(inputs_cnn.shape)\n",
    "        h1 = tf.layers.conv2d(inputs=inputs_cnn, filters=56*2, kernel_size=[8, 3], strides=[1, 4], padding='same')\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)\n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        print(nl1.shape)\n",
    "        h2 = tf.layers.conv2d(inputs=nl1, filters=56*2*2, kernel_size=[8, 3], strides=[1, 4], padding='same')\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)\n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        print(nl2.shape)\n",
    "        h3 = tf.layers.conv2d(inputs=nl2, filters=56*2*2*2, kernel_size=[8, 3], strides=[1, 4], padding='same')\n",
    "        bn3 = tf.layers.batch_normalization(h3, training=training)\n",
    "        nl3 = tf.maximum(alpha * bn3, bn3)\n",
    "        print(nl3.shape)\n",
    "        h4 = tf.layers.conv2d(inputs=nl3, filters=56*2*2*2*2, kernel_size=[8, 3], strides=[1, 4], padding='same')\n",
    "        bn4 = tf.layers.batch_normalization(h4, training=training)\n",
    "        nl4 = tf.maximum(alpha * bn4, bn4)\n",
    "        print(nl4.shape)\n",
    "        h5 = tf.layers.conv2d(inputs=nl4, filters=56*2*2*2*2*2, kernel_size=[8, 3], strides=[8, 4], padding='same')        \n",
    "        bn5 = tf.layers.batch_normalization(h5, training=training)\n",
    "        nl5 = tf.maximum(alpha * bn5, bn5)\n",
    "        print(nl5.shape)\n",
    "\n",
    "        # RNN/MLP/Fully Connected\n",
    "        inputs_rnn = tf.reshape(tensor=nl5, shape=[-1, batch_size, 56*2*2*2*2*2]) # ?, 40, VEC\n",
    "        print(inputs_rnn.shape, initial_state)\n",
    "        # uni-directional/bi-directional\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cells, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, 56*2*2*2*2*2]) # ?x40, VEC\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=output_size)\n",
    "        print(logits.shape)\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(inputs, indices, input_size, output_size, hidden_size, batch_size, cells, initial_state, training):\n",
    "    logits, final_state = discriminator(inputs=inputs, input_size=input_size, output_size=output_size, \n",
    "                                        cells=cells, initial_state=initial_state, batch_size=batch_size, \n",
    "                                        training=training)\n",
    "    labels = tf.one_hot(indices=indices, depth=output_size, dtype=logits.dtype)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "    # If argmax label and argmax logits are equal: true/false\n",
    "    if_equal_bool = tf.equal(x=tf.argmax(axis=1, input=logits), y=tf.argmax(axis=1, input=labels))\n",
    "    accuracy = tf.reduce_mean(axis=0, input_tensor=tf.cast(dtype=tf.float32, x=if_equal_bool))\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    tvars = tf.trainable_variables()\n",
    "    gvars = [var for var in tvars if var.name.startswith('discriminator')]\n",
    "    # gvars = []\n",
    "    # for var in tvars: \n",
    "    #     if var.name.startswith('discriminator'):\n",
    "    #         gvars.append(var)\n",
    "\n",
    "    # Optimize MLP/CNN\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        #opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars) # CNN\n",
    "        #grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(loss, g_vars), clip_norm=5) # usually around 1-5\n",
    "        grads = tf.gradients(loss, gvars) # RNN\n",
    "        opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(grads, gvars))\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, input_size, output_size, hidden_size, batch_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.inputs, self.indices, cells, self.initial_state, self.training = model_input(input_size=input_size, \n",
    "                                                                                         batch_size=batch_size, \n",
    "                                                                                         hidden_size=hidden_size)\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.loss, self.acc = model_loss(inputs=self.inputs, indices=self.indices, batch_size=batch_size, \n",
    "                                         input_size=input_size, output_size=output_size, hidden_size=hidden_size,\n",
    "                                         cells=cells, initial_state=self.initial_state, training=self.training)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_batches(X, Y, batch_size):\n",
    "#     \"\"\" Return a generator for batches \"\"\"\n",
    "#     n_batches = len(X) // batch_size\n",
    "#     X, Y = X[:n_batches*batch_size], Y[:n_batches*batch_size]\n",
    "\n",
    "#     # Loop over batches and yield\n",
    "#     for b in range(0, len(X), batch_size):\n",
    "#         yield X[b:b+batch_size], Y[b:b+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Network Hyper-parameters\n",
    "\n",
    "# # Number of classes Output layer or target labels\n",
    "# assert Ytrain.max()==Ytest.max()==Yvalid.max(), 'Number of output classes is the same in training, validation and testing data.'\n",
    "\n",
    "# # Hidden layer\n",
    "# input_size = [Xvalid.shape[1], Xvalid.shape[2]]\n",
    "# hidden_size = Xvalid.shape[1]* Xvalid.shape[2]\n",
    "# output_size = Yvalid.max()\n",
    "\n",
    "# # learning parameters\n",
    "# batch_size = Xvalid.shape[0]//1  # experience mini-batch size\n",
    "# train_epochs = 1000              # max number of training episodes/epochs\n",
    "# learning_rate = 0.001            # learning rate for training/optimization/adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Yvalid.max(), Yvalid.min():', Yvalid.max(), Yvalid.min())\n",
    "# print('Yvalid.shape:', Yvalid.shape)\n",
    "# print('Xvalid.shape:', Xvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 8, 256, 56)\n",
      "(?, 8, 256, 56)\n",
      "(?, 8, 64, 112)\n",
      "(?, 8, 16, 224)\n",
      "(?, 8, 4, 448)\n",
      "(?, 8, 1, 896)\n",
      "(?, 1, 1, 1792)\n",
      "(?, 40, 1792) (<tf.Tensor 'MultiRNNCellZeroState/GRUCellZeroState/zeros:0' shape=(40, 3584) dtype=float32>,)\n",
      "(40, 40, 3584) (<tf.Tensor 'discriminator/rnn/while/Exit_3:0' shape=(40, 3584) dtype=float32>,)\n",
      "(3200, 1792)\n",
      "(3200, 2)\n"
     ]
    }
   ],
   "source": [
    "graph = tf.reset_default_graph()\n",
    "\n",
    "model = Model(input_size=input_size, hidden_size=hidden_size, output_size=output_size, \n",
    "              learning_rate=learning_rate, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (5, 81920, 56) for Tensor 'inputs:0', which has shape '(?, 8, 256, 56)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-228-7eca12ab5919>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#for Xinputs, Yindices in get_batches(X=Xtrain, Y=Ytrain, batch_size=batch_size):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         loss, acc, _ = sess.run(fetches=[model.loss, model.acc, model.opt], \n\u001b[0;32m---> 21\u001b[0;31m                                 feed_dict={model.inputs: Xtrain, model.indices: Ytrain})\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1114\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1116\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1117\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (5, 81920, 56) for Tensor 'inputs:0', which has shape '(?, 8, 256, 56)'"
     ]
    }
   ],
   "source": [
    "# We should save the after training and validation\n",
    "saver = tf.train.Saver() \n",
    "\n",
    "# Loss and accuracy of the model for training and validation\n",
    "train_loss_mean, train_acc_mean = [], []\n",
    "\n",
    "# now that we can calculate loss and optimize, we can start a session for calculating the error.\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    \n",
    "    # Initialize all the model parameters/variables\n",
    "    sess.run(fetches=tf.global_variables_initializer())\n",
    "    #saver.restore(sess,'checkpoints/model.ckpt')\n",
    "    \n",
    "    # Epoch/episode\n",
    "    for epoch in range(1):\n",
    "        train_loss, valid_acc = [], []\n",
    "        \n",
    "        # Training\n",
    "        #for Xinputs, Yindices in get_batches(X=Xtrain, Y=Ytrain, batch_size=batch_size):\n",
    "        loss, acc, _ = sess.run(fetches=[model.loss, model.acc, model.opt], \n",
    "                                feed_dict={model.inputs: Xtrain, model.indices: Ytrain})\n",
    "        train_loss.append(loss)\n",
    "        train_acc.append(acc)\n",
    "        \n",
    "    # Saving the losses for plotting\n",
    "    train_loss_mean.append(np.mean(train_loss))\n",
    "    train_acc_mean.append(np.mean(train_acc))\n",
    "    print('train_loss_mean:{}'.format(train_loss_mean))\n",
    "    print('train_acc_mean:{}'.format(train_acc_mean))\n",
    "    \n",
    "    # Saving the trained and validated model\n",
    "    saver.save(sess,'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
