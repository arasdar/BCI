{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "import numpy as np\n",
    "from utilities import *\n",
    "\n",
    "# test and train read\n",
    "X_train_valid, Y_train_valid, _ = read_data(data_path=\"../../datasets/har/har-data/\", split=\"train\")\n",
    "X_test, Y_test, _ = read_data(data_path=\"../../datasets/har/har-data/\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing/standardizing the input data features\n",
    "X_train_valid_norm, X_test_norm = standardize(test=X_test, train=X_train_valid)\n",
    "Y_train_valid_onehot = one_hot(labels=Y_train_valid.reshape(-1), n_class=6) \n",
    "Y_test_onehot = one_hot(labels=Y_test.reshape(-1), n_class=6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and valid split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_norm, X_valid_norm, Y_train_onehot, Y_valid_onehot = train_test_split(X_train_valid_norm, \n",
    "                                                                              Y_train_valid_onehot,\n",
    "                                                                              test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N, W, C 66 128 9\n",
      "batch_size, seq_len, n_channels 66 128 9\n",
      "n_classes 6\n"
     ]
    }
   ],
   "source": [
    "## Hyperparameters\n",
    "# Input data\n",
    "# NWC for signal: N is batch size, W is the width/sequence length, and C is the number of channels\n",
    "# NHWC for images: This is the same as signals and H stands for height\n",
    "N, W, C = X_train_norm.shape[0]//100, X_train_norm.shape[1], X_train_norm.shape[2]\n",
    "print('N, W, C', N, W, C)\n",
    "batch_size, seq_len, n_channels = N, W, C\n",
    "print('batch_size, seq_len, n_channels', batch_size, seq_len, n_channels)\n",
    "\n",
    "# Output labels\n",
    "n_classes = Y_train_valid.max(axis=0)\n",
    "assert Y_train_valid.max(axis=0) == Y_test.max(axis=0)\n",
    "print('n_classes', n_classes)\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = 0.001 #1e-3\n",
    "epochs = 1000 # num iterations for updating model\n",
    "keep_prob = 0.90 # 90% neurons are kept and 10% are dropped out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output and hyperpaprameters NP tensors to feed into TF tensors: NP2TF tensors for computation\n",
    "# Input and output and hyperpaprameters tensors to feed into the tensor flow framwork\n",
    "Xin = tf.placeholder(dtype=tf.float32, shape=[None, seq_len, n_channels], name=None)\n",
    "keep_prob_= tf.placeholder(dtype=tf.float32, name=None, shape=None)\n",
    "learning_rate_ = tf.placeholder(dtype=tf.float32, name=None, shape=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Successfully implemented!\n",
    "# # Simple example of running it;\n",
    "# # This is a simple example of using placeholder to define a tensor and feed it into the session\n",
    "# # Also doing this, we can see how it might work using the session\n",
    "# # tensor, varilable, and operation for generating an output for high performance computation\n",
    "# Xexample = tf.placeholder(dtype=tf.float32, shape=(1024, 1024), name=None)\n",
    "# Yexample = tf.matmul(a=Xexample, b=Xexample, name=None)\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "# #   print(sess.run(y))  # ERROR: will fail because x was not fed.\n",
    "\n",
    "# #     rand_array = np.random.rand(1024, 1024)\n",
    "# #     Xin = tf.random_normal(dtype=tf.float32, mean=0., name=None, shape=Xexample.shape, stddev=1.)    \n",
    "#     Xin = np.random.normal(loc=0., scale=1., size=Xexample.shape)\n",
    "#     print(Xexample.shape)\n",
    "#     print(Xin.shape, Xin.dtype)\n",
    "#     Yout = sess.run(fetches=Yexample, feed_dict={Xexample: Xin})\n",
    "#     print(Yout.shape, Yout.dtype, Yexample.shape, Yexample.dtype)  # Will succeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Successfull!\n",
    "# print(inputs_.shape, inputs_.dtype, inputs_.shape[0], inputs_.shape[1], inputs_.shape[2])\n",
    "# # NWC=?, 128, 9\n",
    "# width, in_ch, out_ch = inputs_.shape[1]//4, inputs_.shape[2], inputs_.shape[2]*2\n",
    "# # print(width, in_ch, out_ch)\n",
    "# filter_shape=[width.value, in_ch.value, out_ch.value]\n",
    "# print(filter_shape)\n",
    "# init_val = tf.random_normal(dtype=tf.float32, mean=0., name=None, stddev=1., shape=filter_shape)\n",
    "# # print(init_val.shape, init_val.dtype)\n",
    "# # filters = tf.get_variable(shape=[width, in_ch, out_ch], name='conv1d1', dtype=tf.float32, trainable=True)\n",
    "# filter1 = tf.Variable(initial_value=init_val, dtype=tf.float32, name=None, trainable=True)\n",
    "# conv1d = tf.nn.conv1d(data_format='NWC', filters=filter1, name=None, padding='SAME', stride=2,\n",
    "#                       use_cudnn_on_gpu=True, value=inputs_)\n",
    "# print(conv1d.shape, conv1d.dtype)\n",
    "# filterT = tf.Variable(dtype=tf.float32, initial_value=init_val, name=None, trainable=True)\n",
    "# print(filterT.shape, filterT.dtype)\n",
    "# output_shape = [batch_size, inputs_.shape[1].value, inputs_.shape[2].value]\n",
    "# print(output_shape)\n",
    "# conv1dT = tf.contrib.nn.conv1d_transpose(data_format='NWC', filter=filterT, name=None, output_shape=output_shape, \n",
    "#                                          padding='SAME', stride=2, value=conv1d)\n",
    "# print(conv1dT.shape, conv1d.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Successfull!\n",
    "# # This is explaining a single operation on input tensors and varilables to generate the output tensor\n",
    "# # The pull request is open as of this moment, so the API and behavior can and probably will change. \n",
    "# # Some feature that one might expect from conv1d_transpose aren't supported:\n",
    "\n",
    "# # output_shape requires batch size to be known statically, can't pass -1;\n",
    "# # on the other hand, output shape is dynamic (this explains None dimension).\n",
    "# # Also, the kernel_width=7 expects in_width=255, not 256. \n",
    "# # Should make kernel_width less than 4 to match in_width=256. \n",
    "# # The result is this demo code:\n",
    "# Xexample = tf.placeholder(shape=[None, 256, 16], dtype=tf.float32, name=None)\n",
    "# print(Xexample.shape, Xexample.dtype)\n",
    "# [kernel_width, output_depth, input_depth] = [Xexample.shape[1].value//4, Xexample.shape[2].value//2, \n",
    "#                                              Xexample.shape[2].value]\n",
    "# shape = [kernel_width, output_depth, input_depth] \n",
    "# print(shape)\n",
    "# init_val = tf.random_normal(shape=shape, dtype=tf.float32, mean=0., name=None, stddev=1.)\n",
    "# Wexample = tf.Variable(initial_value=init_val, dtype=tf.float32, name=None, trainable=True)\n",
    "# print(Wexample.shape, Wexample.dtype)\n",
    "# # output shape is based on NWC: batch size, width, and Channels\n",
    "# stride = 4\n",
    "# shape = [1, Xexample.shape[1].value*stride, Wexample.shape[1].value] \n",
    "# print(shape)\n",
    "# Yexample = tf.contrib.nn.conv1d_transpose(value=Xexample, filter=Wexample, output_shape=shape, data_format='NWC',\n",
    "#                                      name=None, stride=4, padding='SAME')\n",
    "# # both paddings 'SAME' or 'VALID' work # any difference?\n",
    "# print(Yexample.shape, Yexample.dtype)\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(fetches=tf.global_variables_initializer())\n",
    "#     print(Xexample.shape)\n",
    "#     shape = [Yexample.shape[0].value, Xexample.shape[1].value, Xexample.shape[2].value]\n",
    "#     print(shape)\n",
    "#     Xin = np.random.normal(loc=0., scale=1., size=shape) \n",
    "#     Yout = sess.run(fetches=Yexample, feed_dict={Xexample: Xin})\n",
    "#     print(Yout.shape, Yout.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 128, 9) <dtype: 'float32'>\n",
      "(32, 9, 18) <dtype: 'float32_ref'>\n",
      "(?, 64, 18) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(Xin.shape, Xin.dtype)\n",
    "# shape = [kernel_width, input_depth, output_depth] \n",
    "W, Cin, Cout = Xin.shape[1].value//4, Xin.shape[2].value, Xin.shape[2].value*2\n",
    "shape = [W, Cin, Cout]\n",
    "init_val = tf.random_normal(dtype=tf.float32, mean=0., name=None, shape=shape, stddev=1.0)\n",
    "W1 = tf.Variable(dtype=tf.float32, initial_value=init_val, name=None, trainable=True)\n",
    "print(W1.shape, W1.dtype)\n",
    "Xconv1 = tf.nn.conv1d(data_format='NWC', filters=W1, name=None, padding='SAME', stride=2, use_cudnn_on_gpu=True, \n",
    "                     value=Xin)\n",
    "Xconv1 = tf.nn.relu(features=Xconv1, name=None)\n",
    "print(Xconv1.shape, Xconv1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 64, 18) <dtype: 'float32'>\n",
      "(16, 18, 36) <dtype: 'float32_ref'>\n",
      "(?, 32, 36) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(Xconv1.shape, Xconv1.dtype)\n",
    "W, Cin, Cout = Xconv1.shape[1].value//4, Xconv1.shape[2].value, Xconv1.shape[2].value*2\n",
    "shape = [W, Cin, Cout]\n",
    "init_val = tf.random_normal(dtype=tf.float32, mean=0.0, name=None, shape=shape, stddev=1.0)\n",
    "W2 = tf.Variable(dtype=tf.float32, initial_value=init_val, name=None, trainable=True)\n",
    "print(W2.shape, W2.dtype)\n",
    "Xconv2 = tf.nn.conv1d(data_format='NWC', filters=W2, name=None, padding='SAME', stride=2, use_cudnn_on_gpu=True, \n",
    "                     value=Xconv1)\n",
    "Xconv2 = tf.nn.relu(features=Xconv2, name=None)\n",
    "print(Xconv2.shape, Xconv2.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 36) <dtype: 'float32'>\n",
      "(8, 36, 72) <dtype: 'float32_ref'>\n",
      "(?, 16, 72) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(Xconv2.shape, Xconv2.dtype)\n",
    "W, Cin, Cout = Xconv2.shape[1].value//4, Xconv2.shape[2].value, Xconv2.shape[2].value*2\n",
    "shape = [W, Cin, Cout]\n",
    "init_val = tf.random_normal(dtype=tf.float32, mean=0.0, name=None, shape=shape, stddev=1.0)\n",
    "W3 = tf.Variable(dtype=tf.float32, initial_value=init_val, name=None, trainable=True)\n",
    "print(W3.shape, W3.dtype)\n",
    "Xconv3 = tf.nn.conv1d(data_format='NWC', filters=W3, name=None, padding='SAME', stride=2, use_cudnn_on_gpu=True, \n",
    "                     value=Xconv2)\n",
    "Xconv3 = tf.nn.relu(features=Xconv3, name=None)\n",
    "print(Xconv3.shape, Xconv3.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 72) <dtype: 'float32'>\n",
      "(8, 36, 72) <dtype: 'float32_ref'>\n",
      "[66, 32, 36]\n",
      "WARNING:tensorflow:From /home/arasdar/anaconda3/envs/arasdar-DL-env/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "(66, 32, 36) <dtype: 'float32'>\n",
      "(?, 32, 36) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(Xconv3.shape, Xconv3.dtype)\n",
    "W, Cin, Cout = Xconv2.shape[1].value//4, Xconv2.shape[2].value, Xconv2.shape[2].value*2\n",
    "shape = [W, Cin, Cout]\n",
    "init_val = tf.random_normal(dtype=tf.float32, mean=0., name=None, shape=shape, stddev=1.0)\n",
    "W3T = tf.Variable(dtype=tf.float32, initial_value=init_val, name=None, trainable=True)\n",
    "print(W3T.shape, W3T.dtype)\n",
    "# output shape should be NWC equal to Xconv2\n",
    "out_shape = [batch_size, Xconv2.shape[1].value, Xconv2.shape[2].value] # NWC, only N should be set (experimental)\n",
    "print(out_shape)\n",
    "Xconv2_ = tf.contrib.nn.conv1d_transpose(data_format='NWC', filter=W3T, name=None, padding='SAME', stride=2, \n",
    "                                         value=Xconv3, output_shape=out_shape)\n",
    "Xconv2_ = tf.nn.relu(features=Xconv2_, name=None)\n",
    "print(Xconv2_.shape, Xconv2_.dtype)\n",
    "print(Xconv2.shape, Xconv2.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 32, 36) <dtype: 'float32'>\n",
      "(16, 18, 36) <dtype: 'float32_ref'>\n",
      "[66, 64, 18]\n",
      "(66, 64, 18) <dtype: 'float32'>\n",
      "(?, 64, 18) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(Xconv2_.shape, Xconv2_.dtype)\n",
    "W, Cin, Cout = Xconv1.shape[1].value//4, Xconv1.shape[2].value, Xconv1.shape[2].value*2\n",
    "shape = [W, Cin, Cout]\n",
    "init_val = tf.random_normal(dtype=tf.float32, mean=0., name=None, shape=shape, stddev=1.0)\n",
    "W2T = tf.Variable(dtype=tf.float32, initial_value=init_val, name=None, trainable=True)\n",
    "print(W2T.shape, W2T.dtype)\n",
    "# output shape should be NWC equal to Xconv1\n",
    "out_shape = [batch_size, Xconv1.shape[1].value, Xconv1.shape[2].value] # NWC, only N should be set (experimental)\n",
    "print(out_shape)\n",
    "Xconv1_ = tf.contrib.nn.conv1d_transpose(data_format='NWC', filter=W2T, name=None, padding='SAME', stride=2, \n",
    "                                         value=Xconv2_, output_shape=out_shape)\n",
    "Xconv1_ = tf.nn.relu(features=Xconv1_, name=None)\n",
    "print(Xconv1_.shape, Xconv1_.dtype)\n",
    "print(Xconv1.shape, Xconv1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 64, 18) <dtype: 'float32'>\n",
      "(32, 9, 18) <dtype: 'float32_ref'>\n",
      "[66, 128, 9]\n",
      "(66, 128, 9) <dtype: 'float32'>\n",
      "(?, 128, 9) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(Xconv1_.shape, Xconv1_.dtype)\n",
    "W, Cin, Cout = Xin.shape[1].value//4, Xin.shape[2].value, Xin.shape[2].value*2 \n",
    "shape = [W, Cin, Cout]\n",
    "init_val = tf.random_normal(dtype=tf.float32, mean=0., name=None, shape=shape, stddev=1.0)\n",
    "W1T = tf.Variable(dtype=tf.float32, initial_value=init_val, name=None, trainable=True)\n",
    "print(W1T.shape, W1T.dtype)\n",
    "# output shape should be NWC equal to Xin\n",
    "out_shape = [batch_size, Xin.shape[1].value, Xin.shape[2].value] # NWC, only N should be set (experimental)\n",
    "print(out_shape)\n",
    "Xout = tf.contrib.nn.conv1d_transpose(data_format='NWC', filter=W1T, name=None, padding='SAME', stride=2, \n",
    "                                         value=Xconv1_, output_shape=out_shape)\n",
    "# Xin_ = tf.nn.relu(features=Xin_, name=None)\n",
    "print(Xout.shape, Xout.dtype)\n",
    "print(Xin.shape, Xin.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 128, 9) <dtype: 'float32'>\n",
      "() <dtype: 'float32'>\n",
      "optimizer name: \"Adam\"\n",
      "op: \"NoOp\"\n",
      "input: \"^Adam/update_Variable/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_1/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_2/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_3/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_4/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_5/ApplyAdam\"\n",
      "input: \"^Adam/Assign\"\n",
      "input: \"^Adam/Assign_1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Backward pass: error backpropagation\n",
    "# Cost function\n",
    "# cost_tensor = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_)\n",
    "cost_tensor = tf.nn.sigmoid_cross_entropy_with_logits(labels=Xin, logits=Xout, name=None)\n",
    "print(cost_tensor.shape, cost_tensor.dtype)\n",
    "cost = tf.reduce_mean(input_tensor=cost_tensor)\n",
    "# The cost has to be positive since it is a distance between two vectors in the hyperspace\n",
    "# Eucleadian dist or length, angle or entropy are all these kinds of distance\n",
    "cost = tf.abs(name=None, x=cost)\n",
    "print(cost.shape, cost.dtype)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_).minimize(cost)\n",
    "print('optimizer', optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1000 Train loss: 254997.765625 Valid loss: 236853.359375\n",
      "Epoch: 2/1000 Train loss: 136337.218750 Valid loss: 128185.992188\n",
      "Epoch: 3/1000 Train loss: 95171.593750 Valid loss: 90721.414062\n",
      "Epoch: 4/1000 Train loss: 73604.640625 Valid loss: 71663.710938\n",
      "Epoch: 5/1000 Train loss: 60237.023438 Valid loss: 59800.718750\n",
      "Epoch: 6/1000 Train loss: 51195.515625 Valid loss: 51780.421875\n",
      "Epoch: 7/1000 Train loss: 44695.332031 Valid loss: 46043.269531\n",
      "Epoch: 8/1000 Train loss: 39599.718750 Valid loss: 41641.636719\n",
      "Epoch: 9/1000 Train loss: 35745.710938 Valid loss: 38194.042969\n",
      "Epoch: 10/1000 Train loss: 32728.105469 Valid loss: 35423.152344\n",
      "Epoch: 11/1000 Train loss: 30215.347656 Valid loss: 33090.449219\n",
      "Epoch: 12/1000 Train loss: 28118.414062 Valid loss: 31082.166016\n",
      "Epoch: 13/1000 Train loss: 26337.416016 Valid loss: 29285.667969\n",
      "Epoch: 14/1000 Train loss: 24666.220703 Valid loss: 27703.857422\n",
      "Epoch: 15/1000 Train loss: 23219.117188 Valid loss: 26311.326172\n",
      "Epoch: 16/1000 Train loss: 21953.525391 Valid loss: 25085.679688\n",
      "Epoch: 17/1000 Train loss: 20860.203125 Valid loss: 24025.035156\n",
      "Epoch: 18/1000 Train loss: 19829.408203 Valid loss: 23037.650391\n",
      "Epoch: 19/1000 Train loss: 18991.248047 Valid loss: 22193.962891\n",
      "Epoch: 20/1000 Train loss: 18191.513672 Valid loss: 21396.859375\n",
      "Epoch: 21/1000 Train loss: 17432.152344 Valid loss: 20663.130859\n",
      "Epoch: 22/1000 Train loss: 16736.933594 Valid loss: 19998.042969\n",
      "Epoch: 23/1000 Train loss: 16162.536133 Valid loss: 19400.380859\n",
      "Epoch: 24/1000 Train loss: 15623.667969 Valid loss: 18850.220703\n",
      "Epoch: 25/1000 Train loss: 15104.686523 Valid loss: 18307.138672\n",
      "Epoch: 26/1000 Train loss: 14616.599609 Valid loss: 17801.496094\n",
      "Epoch: 27/1000 Train loss: 14152.710938 Valid loss: 17337.570312\n",
      "Epoch: 28/1000 Train loss: 13726.267578 Valid loss: 16904.580078\n",
      "Epoch: 29/1000 Train loss: 13366.197266 Valid loss: 16499.982422\n",
      "Epoch: 30/1000 Train loss: 13019.369141 Valid loss: 16113.657227\n",
      "Epoch: 31/1000 Train loss: 12658.583008 Valid loss: 15721.212891\n",
      "Epoch: 32/1000 Train loss: 12313.787109 Valid loss: 15359.563477\n",
      "Epoch: 33/1000 Train loss: 11972.809570 Valid loss: 15001.576172\n",
      "Epoch: 34/1000 Train loss: 11688.545898 Valid loss: 14671.245117\n",
      "Epoch: 35/1000 Train loss: 11388.001953 Valid loss: 14350.122070\n",
      "Epoch: 36/1000 Train loss: 11135.193359 Valid loss: 14051.903320\n",
      "Epoch: 37/1000 Train loss: 10867.111328 Valid loss: 13761.371094\n",
      "Epoch: 38/1000 Train loss: 10619.781250 Valid loss: 13483.270508\n",
      "Epoch: 39/1000 Train loss: 10380.764648 Valid loss: 13220.723633\n",
      "Epoch: 40/1000 Train loss: 10157.823242 Valid loss: 12971.333984\n",
      "Epoch: 41/1000 Train loss: 9946.248047 Valid loss: 12733.472656\n",
      "Epoch: 42/1000 Train loss: 9745.126953 Valid loss: 12505.634766\n",
      "Epoch: 43/1000 Train loss: 9547.608398 Valid loss: 12288.325195\n",
      "Epoch: 44/1000 Train loss: 9372.913086 Valid loss: 12081.427734\n",
      "Epoch: 45/1000 Train loss: 9200.074219 Valid loss: 11876.992188\n",
      "Epoch: 46/1000 Train loss: 9026.367188 Valid loss: 11680.147461\n",
      "Epoch: 47/1000 Train loss: 8874.444336 Valid loss: 11491.876953\n",
      "Epoch: 48/1000 Train loss: 8720.876953 Valid loss: 11306.428711\n",
      "Epoch: 49/1000 Train loss: 8560.104492 Valid loss: 11125.252930\n",
      "Epoch: 50/1000 Train loss: 8412.732422 Valid loss: 10949.879883\n",
      "Epoch: 51/1000 Train loss: 8266.392578 Valid loss: 10781.141602\n",
      "Epoch: 52/1000 Train loss: 8131.024414 Valid loss: 10619.125000\n",
      "Epoch: 53/1000 Train loss: 7994.445801 Valid loss: 10460.108398\n",
      "Epoch: 54/1000 Train loss: 7867.303711 Valid loss: 10309.004883\n",
      "Epoch: 55/1000 Train loss: 7743.133301 Valid loss: 10160.537109\n",
      "Epoch: 56/1000 Train loss: 7621.589355 Valid loss: 10015.434570\n",
      "Epoch: 57/1000 Train loss: 7509.351074 Valid loss: 9875.920898\n",
      "Epoch: 58/1000 Train loss: 7397.152344 Valid loss: 9738.097656\n",
      "Epoch: 59/1000 Train loss: 7284.801270 Valid loss: 9604.213867\n",
      "Epoch: 60/1000 Train loss: 7175.713867 Valid loss: 9473.648438\n",
      "Epoch: 61/1000 Train loss: 7068.783691 Valid loss: 9346.601562\n",
      "Epoch: 62/1000 Train loss: 6964.680176 Valid loss: 9223.736328\n",
      "Epoch: 63/1000 Train loss: 6864.014160 Valid loss: 9104.064453\n",
      "Epoch: 64/1000 Train loss: 6764.131348 Valid loss: 8988.287109\n",
      "Epoch: 65/1000 Train loss: 6668.477539 Valid loss: 8875.743164\n",
      "Epoch: 66/1000 Train loss: 6574.576172 Valid loss: 8765.506836\n",
      "Epoch: 67/1000 Train loss: 6485.950684 Valid loss: 8659.853516\n",
      "Epoch: 68/1000 Train loss: 6405.285156 Valid loss: 8557.215820\n",
      "Epoch: 69/1000 Train loss: 6323.368164 Valid loss: 8456.168945\n",
      "Epoch: 70/1000 Train loss: 6244.561523 Valid loss: 8357.935547\n",
      "Epoch: 71/1000 Train loss: 6162.497559 Valid loss: 8260.385742\n",
      "Epoch: 72/1000 Train loss: 6082.993652 Valid loss: 8165.353516\n",
      "Epoch: 73/1000 Train loss: 6005.276367 Valid loss: 8072.300293\n",
      "Epoch: 74/1000 Train loss: 5931.160645 Valid loss: 7982.472656\n",
      "Epoch: 75/1000 Train loss: 5857.492188 Valid loss: 7893.888672\n",
      "Epoch: 76/1000 Train loss: 5786.374023 Valid loss: 7807.134277\n",
      "Epoch: 77/1000 Train loss: 5719.247559 Valid loss: 7722.232910\n",
      "Epoch: 78/1000 Train loss: 5651.874023 Valid loss: 7638.563965\n",
      "Epoch: 79/1000 Train loss: 5587.394531 Valid loss: 7557.158691\n",
      "Epoch: 80/1000 Train loss: 5523.957520 Valid loss: 7476.881836\n",
      "Epoch: 81/1000 Train loss: 5459.330078 Valid loss: 7398.977539\n",
      "Epoch: 82/1000 Train loss: 5397.791992 Valid loss: 7323.254883\n",
      "Epoch: 83/1000 Train loss: 5337.130859 Valid loss: 7249.181641\n",
      "Epoch: 84/1000 Train loss: 5279.089844 Valid loss: 7177.117188\n",
      "Epoch: 85/1000 Train loss: 5221.914551 Valid loss: 7105.345215\n",
      "Epoch: 86/1000 Train loss: 5165.108398 Valid loss: 7034.364258\n",
      "Epoch: 87/1000 Train loss: 5110.784668 Valid loss: 6964.896973\n",
      "Epoch: 88/1000 Train loss: 5057.459961 Valid loss: 6896.588867\n",
      "Epoch: 89/1000 Train loss: 5003.936523 Valid loss: 6830.054199\n",
      "Epoch: 90/1000 Train loss: 4951.307617 Valid loss: 6765.395508\n",
      "Epoch: 91/1000 Train loss: 4900.748535 Valid loss: 6702.380859\n",
      "Epoch: 92/1000 Train loss: 4851.060059 Valid loss: 6639.795410\n",
      "Epoch: 93/1000 Train loss: 4802.701172 Valid loss: 6577.918945\n",
      "Epoch: 94/1000 Train loss: 4754.938477 Valid loss: 6517.081055\n",
      "Epoch: 95/1000 Train loss: 4708.545898 Valid loss: 6457.259766\n",
      "Epoch: 96/1000 Train loss: 4663.191406 Valid loss: 6398.597168\n",
      "Epoch: 97/1000 Train loss: 4617.728516 Valid loss: 6340.771484\n",
      "Epoch: 98/1000 Train loss: 4572.817383 Valid loss: 6284.591309\n",
      "Epoch: 99/1000 Train loss: 4528.763672 Valid loss: 6229.604492\n",
      "Epoch: 100/1000 Train loss: 4486.217773 Valid loss: 6175.141602\n",
      "Epoch: 101/1000 Train loss: 4444.727539 Valid loss: 6120.877930\n",
      "Epoch: 102/1000 Train loss: 4403.413574 Valid loss: 6067.572266\n",
      "Epoch: 103/1000 Train loss: 4363.458984 Valid loss: 6015.650391\n",
      "Epoch: 104/1000 Train loss: 4323.796387 Valid loss: 5964.560547\n",
      "Epoch: 105/1000 Train loss: 4284.537109 Valid loss: 5914.426758\n",
      "Epoch: 106/1000 Train loss: 4245.946289 Valid loss: 5864.623047\n",
      "Epoch: 107/1000 Train loss: 4208.253418 Valid loss: 5815.532227\n",
      "Epoch: 108/1000 Train loss: 4171.609863 Valid loss: 5766.905273\n",
      "Epoch: 109/1000 Train loss: 4134.828125 Valid loss: 5719.213379\n",
      "Epoch: 110/1000 Train loss: 4098.856934 Valid loss: 5672.660156\n",
      "Epoch: 111/1000 Train loss: 4063.613770 Valid loss: 5626.668945\n",
      "Epoch: 112/1000 Train loss: 4028.546387 Valid loss: 5581.640137\n",
      "Epoch: 113/1000 Train loss: 3994.376221 Valid loss: 5537.189941\n",
      "Epoch: 114/1000 Train loss: 3961.095459 Valid loss: 5493.575684\n",
      "Epoch: 115/1000 Train loss: 3927.946777 Valid loss: 5450.669922\n",
      "Epoch: 116/1000 Train loss: 3895.777832 Valid loss: 5408.313965\n",
      "Epoch: 117/1000 Train loss: 3863.784912 Valid loss: 5366.716797\n",
      "Epoch: 118/1000 Train loss: 3832.289062 Valid loss: 5325.590332\n",
      "Epoch: 119/1000 Train loss: 3801.226807 Valid loss: 5285.153809\n",
      "Epoch: 120/1000 Train loss: 3770.787354 Valid loss: 5245.508789\n",
      "Epoch: 121/1000 Train loss: 3740.487305 Valid loss: 5206.572754\n",
      "Epoch: 122/1000 Train loss: 3710.875000 Valid loss: 5168.277832\n",
      "Epoch: 123/1000 Train loss: 3681.951172 Valid loss: 5130.683594\n",
      "Epoch: 124/1000 Train loss: 3653.443115 Valid loss: 5093.359863\n",
      "Epoch: 125/1000 Train loss: 3625.510498 Valid loss: 5056.494141\n",
      "Epoch: 126/1000 Train loss: 3597.784668 Valid loss: 5020.204102\n",
      "Epoch: 127/1000 Train loss: 3570.502441 Valid loss: 4984.476074\n",
      "Epoch: 128/1000 Train loss: 3543.683350 Valid loss: 4949.258789\n",
      "Epoch: 129/1000 Train loss: 3517.284424 Valid loss: 4914.325195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 130/1000 Train loss: 3491.475098 Valid loss: 4879.806152\n",
      "Epoch: 131/1000 Train loss: 3465.543213 Valid loss: 4845.694824\n",
      "Epoch: 132/1000 Train loss: 3440.084473 Valid loss: 4812.318359\n",
      "Epoch: 133/1000 Train loss: 3415.044189 Valid loss: 4779.451660\n",
      "Epoch: 134/1000 Train loss: 3390.321777 Valid loss: 4746.937988\n",
      "Epoch: 135/1000 Train loss: 3366.157227 Valid loss: 4714.564941\n",
      "Epoch: 136/1000 Train loss: 3342.442871 Valid loss: 4682.648926\n",
      "Epoch: 137/1000 Train loss: 3318.668945 Valid loss: 4651.188965\n",
      "Epoch: 138/1000 Train loss: 3295.465576 Valid loss: 4620.434570\n",
      "Epoch: 139/1000 Train loss: 3272.359131 Valid loss: 4589.966797\n",
      "Epoch: 140/1000 Train loss: 3249.611328 Valid loss: 4559.757812\n",
      "Epoch: 141/1000 Train loss: 3227.364014 Valid loss: 4529.690430\n",
      "Epoch: 142/1000 Train loss: 3205.518799 Valid loss: 4499.996582\n",
      "Epoch: 143/1000 Train loss: 3183.674316 Valid loss: 4470.767090\n",
      "Epoch: 144/1000 Train loss: 3162.217041 Valid loss: 4441.980957\n",
      "Epoch: 145/1000 Train loss: 3141.049561 Valid loss: 4413.576660\n",
      "Epoch: 146/1000 Train loss: 3120.264648 Valid loss: 4385.357910\n",
      "Epoch: 147/1000 Train loss: 3099.661865 Valid loss: 4357.446289\n",
      "Epoch: 148/1000 Train loss: 3079.172607 Valid loss: 4329.827148\n",
      "Epoch: 149/1000 Train loss: 3059.167725 Valid loss: 4302.679688\n",
      "Epoch: 150/1000 Train loss: 3039.304199 Valid loss: 4275.841309\n",
      "Epoch: 151/1000 Train loss: 3019.652100 Valid loss: 4249.226562\n",
      "Epoch: 152/1000 Train loss: 3000.168213 Valid loss: 4222.928223\n",
      "Epoch: 153/1000 Train loss: 2980.969482 Valid loss: 4196.965820\n",
      "Epoch: 154/1000 Train loss: 2962.065918 Valid loss: 4171.348633\n",
      "Epoch: 155/1000 Train loss: 2943.431641 Valid loss: 4146.011230\n",
      "Epoch: 156/1000 Train loss: 2924.935059 Valid loss: 4120.891602\n",
      "Epoch: 157/1000 Train loss: 2906.647949 Valid loss: 4096.089844\n",
      "Epoch: 158/1000 Train loss: 2888.621582 Valid loss: 4071.588379\n",
      "Epoch: 159/1000 Train loss: 2870.765381 Valid loss: 4047.407959\n",
      "Epoch: 160/1000 Train loss: 2853.121582 Valid loss: 4023.435303\n",
      "Epoch: 161/1000 Train loss: 2835.700195 Valid loss: 3999.704102\n",
      "Epoch: 162/1000 Train loss: 2818.472412 Valid loss: 3976.259766\n",
      "Epoch: 163/1000 Train loss: 2801.526367 Valid loss: 3953.031494\n",
      "Epoch: 164/1000 Train loss: 2784.726318 Valid loss: 3930.055908\n",
      "Epoch: 165/1000 Train loss: 2768.167725 Valid loss: 3907.313477\n",
      "Epoch: 166/1000 Train loss: 2751.739990 Valid loss: 3884.760254\n",
      "Epoch: 167/1000 Train loss: 2735.508301 Valid loss: 3862.548096\n",
      "Epoch: 168/1000 Train loss: 2719.471924 Valid loss: 3840.578369\n",
      "Epoch: 169/1000 Train loss: 2703.688965 Valid loss: 3818.837158\n",
      "Epoch: 170/1000 Train loss: 2688.027588 Valid loss: 3797.342041\n",
      "Epoch: 171/1000 Train loss: 2672.581055 Valid loss: 3776.090820\n",
      "Epoch: 172/1000 Train loss: 2657.218994 Valid loss: 3755.000732\n",
      "Epoch: 173/1000 Train loss: 2642.074707 Valid loss: 3734.160400\n",
      "Epoch: 174/1000 Train loss: 2627.117676 Valid loss: 3713.527344\n",
      "Epoch: 175/1000 Train loss: 2612.352783 Valid loss: 3693.151123\n",
      "Epoch: 176/1000 Train loss: 2597.705566 Valid loss: 3673.051025\n",
      "Epoch: 177/1000 Train loss: 2583.239258 Valid loss: 3653.169678\n",
      "Epoch: 178/1000 Train loss: 2568.952881 Valid loss: 3633.493164\n",
      "Epoch: 179/1000 Train loss: 2554.811523 Valid loss: 3613.972168\n",
      "Epoch: 180/1000 Train loss: 2540.800781 Valid loss: 3594.634277\n",
      "Epoch: 181/1000 Train loss: 2526.959229 Valid loss: 3575.553711\n",
      "Epoch: 182/1000 Train loss: 2513.256592 Valid loss: 3556.686768\n",
      "Epoch: 183/1000 Train loss: 2499.718262 Valid loss: 3538.050049\n",
      "Epoch: 184/1000 Train loss: 2486.368164 Valid loss: 3519.592285\n",
      "Epoch: 185/1000 Train loss: 2473.129395 Valid loss: 3501.310059\n",
      "Epoch: 186/1000 Train loss: 2459.996582 Valid loss: 3483.191895\n",
      "Epoch: 187/1000 Train loss: 2446.982910 Valid loss: 3465.271973\n",
      "Epoch: 188/1000 Train loss: 2434.101807 Valid loss: 3447.520752\n",
      "Epoch: 189/1000 Train loss: 2421.394531 Valid loss: 3429.930420\n",
      "Epoch: 190/1000 Train loss: 2408.788574 Valid loss: 3412.494385\n",
      "Epoch: 191/1000 Train loss: 2396.316895 Valid loss: 3395.220215\n",
      "Epoch: 192/1000 Train loss: 2383.950684 Valid loss: 3378.146973\n",
      "Epoch: 193/1000 Train loss: 2371.775146 Valid loss: 3361.222168\n",
      "Epoch: 194/1000 Train loss: 2359.660400 Valid loss: 3344.454102\n",
      "Epoch: 195/1000 Train loss: 2347.686279 Valid loss: 3327.866943\n",
      "Epoch: 196/1000 Train loss: 2335.815674 Valid loss: 3311.444092\n",
      "Epoch: 197/1000 Train loss: 2324.062988 Valid loss: 3295.183105\n",
      "Epoch: 198/1000 Train loss: 2312.424072 Valid loss: 3279.105225\n",
      "Epoch: 199/1000 Train loss: 2300.903564 Valid loss: 3263.161377\n",
      "Epoch: 200/1000 Train loss: 2289.504883 Valid loss: 3247.351562\n",
      "Epoch: 201/1000 Train loss: 2278.202637 Valid loss: 3231.685791\n",
      "Epoch: 202/1000 Train loss: 2267.016357 Valid loss: 3216.206787\n",
      "Epoch: 203/1000 Train loss: 2255.946289 Valid loss: 3200.870361\n",
      "Epoch: 204/1000 Train loss: 2244.985596 Valid loss: 3185.674316\n",
      "Epoch: 205/1000 Train loss: 2234.121582 Valid loss: 3170.607910\n",
      "Epoch: 206/1000 Train loss: 2223.366211 Valid loss: 3155.685791\n",
      "Epoch: 207/1000 Train loss: 2212.720459 Valid loss: 3140.867676\n",
      "Epoch: 208/1000 Train loss: 2202.176758 Valid loss: 3126.176758\n",
      "Epoch: 209/1000 Train loss: 2191.721436 Valid loss: 3111.596436\n",
      "Epoch: 210/1000 Train loss: 2181.364990 Valid loss: 3097.149170\n",
      "Epoch: 211/1000 Train loss: 2171.107178 Valid loss: 3082.859619\n",
      "Epoch: 212/1000 Train loss: 2160.966064 Valid loss: 3068.721680\n",
      "Epoch: 213/1000 Train loss: 2150.908691 Valid loss: 3054.689209\n",
      "Epoch: 214/1000 Train loss: 2140.923584 Valid loss: 3040.756348\n",
      "Epoch: 215/1000 Train loss: 2131.029785 Valid loss: 3026.951660\n",
      "Epoch: 216/1000 Train loss: 2121.240479 Valid loss: 3013.277100\n",
      "Epoch: 217/1000 Train loss: 2111.525391 Valid loss: 2999.717285\n",
      "Epoch: 218/1000 Train loss: 2101.909180 Valid loss: 2986.274658\n",
      "Epoch: 219/1000 Train loss: 2092.380371 Valid loss: 2972.951660\n",
      "Epoch: 220/1000 Train loss: 2082.923340 Valid loss: 2959.751709\n",
      "Epoch: 221/1000 Train loss: 2073.566650 Valid loss: 2946.649170\n",
      "Epoch: 222/1000 Train loss: 2064.280518 Valid loss: 2933.661377\n",
      "Epoch: 223/1000 Train loss: 2055.087646 Valid loss: 2920.780518\n",
      "Epoch: 224/1000 Train loss: 2045.971924 Valid loss: 2908.002441\n",
      "Epoch: 225/1000 Train loss: 2036.934937 Valid loss: 2895.308350\n",
      "Epoch: 226/1000 Train loss: 2027.991699 Valid loss: 2882.728516\n",
      "Epoch: 227/1000 Train loss: 2019.109985 Valid loss: 2870.251953\n",
      "Epoch: 228/1000 Train loss: 2010.304565 Valid loss: 2857.881592\n",
      "Epoch: 229/1000 Train loss: 2001.566772 Valid loss: 2845.604736\n",
      "Epoch: 230/1000 Train loss: 1992.912354 Valid loss: 2833.425781\n",
      "Epoch: 231/1000 Train loss: 1984.325195 Valid loss: 2821.336426\n",
      "Epoch: 232/1000 Train loss: 1975.816772 Valid loss: 2809.349854\n",
      "Epoch: 233/1000 Train loss: 1967.375977 Valid loss: 2797.467529\n",
      "Epoch: 234/1000 Train loss: 1959.003906 Valid loss: 2785.684326\n",
      "Epoch: 235/1000 Train loss: 1950.705322 Valid loss: 2774.007568\n",
      "Epoch: 236/1000 Train loss: 1942.485229 Valid loss: 2762.440186\n",
      "Epoch: 237/1000 Train loss: 1934.328125 Valid loss: 2750.957764\n",
      "Epoch: 238/1000 Train loss: 1926.247681 Valid loss: 2739.561523\n",
      "Epoch: 239/1000 Train loss: 1918.227295 Valid loss: 2728.242432\n",
      "Epoch: 240/1000 Train loss: 1910.281616 Valid loss: 2717.007812\n",
      "Epoch: 241/1000 Train loss: 1902.392090 Valid loss: 2705.855713\n",
      "Epoch: 242/1000 Train loss: 1894.563965 Valid loss: 2694.796631\n",
      "Epoch: 243/1000 Train loss: 1886.791992 Valid loss: 2683.839600\n",
      "Epoch: 244/1000 Train loss: 1879.093750 Valid loss: 2672.964355\n",
      "Epoch: 245/1000 Train loss: 1871.446899 Valid loss: 2662.165527\n",
      "Epoch: 246/1000 Train loss: 1863.866211 Valid loss: 2651.453857\n",
      "Epoch: 247/1000 Train loss: 1856.344482 Valid loss: 2640.823242\n",
      "Epoch: 248/1000 Train loss: 1848.890503 Valid loss: 2630.268799\n",
      "Epoch: 249/1000 Train loss: 1841.486084 Valid loss: 2619.788574\n",
      "Epoch: 250/1000 Train loss: 1834.140991 Valid loss: 2609.396240\n",
      "Epoch: 251/1000 Train loss: 1826.856934 Valid loss: 2599.088867\n",
      "Epoch: 252/1000 Train loss: 1819.627808 Valid loss: 2588.863770\n",
      "Epoch: 253/1000 Train loss: 1812.458618 Valid loss: 2578.726562\n",
      "Epoch: 254/1000 Train loss: 1805.348022 Valid loss: 2568.665771\n",
      "Epoch: 255/1000 Train loss: 1798.285278 Valid loss: 2558.672363\n",
      "Epoch: 256/1000 Train loss: 1791.277344 Valid loss: 2548.755859\n",
      "Epoch: 257/1000 Train loss: 1784.328125 Valid loss: 2538.915771\n",
      "Epoch: 258/1000 Train loss: 1777.429321 Valid loss: 2529.151367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 259/1000 Train loss: 1770.584229 Valid loss: 2519.466553\n",
      "Epoch: 260/1000 Train loss: 1763.790283 Valid loss: 2509.854980\n",
      "Epoch: 261/1000 Train loss: 1757.048218 Valid loss: 2500.308838\n",
      "Epoch: 262/1000 Train loss: 1750.359375 Valid loss: 2490.839355\n",
      "Epoch: 263/1000 Train loss: 1743.719849 Valid loss: 2481.446289\n",
      "Epoch: 264/1000 Train loss: 1737.130859 Valid loss: 2472.117920\n",
      "Epoch: 265/1000 Train loss: 1730.590332 Valid loss: 2462.856934\n",
      "Epoch: 266/1000 Train loss: 1724.098022 Valid loss: 2453.664307\n",
      "Epoch: 267/1000 Train loss: 1717.651978 Valid loss: 2444.540771\n",
      "Epoch: 268/1000 Train loss: 1711.258667 Valid loss: 2435.487305\n",
      "Epoch: 269/1000 Train loss: 1704.912842 Valid loss: 2426.500000\n",
      "Epoch: 270/1000 Train loss: 1698.612061 Valid loss: 2417.569580\n",
      "Epoch: 271/1000 Train loss: 1692.356201 Valid loss: 2408.699219\n",
      "Epoch: 272/1000 Train loss: 1686.146118 Valid loss: 2399.890625\n",
      "Epoch: 273/1000 Train loss: 1679.981567 Valid loss: 2391.144531\n",
      "Epoch: 274/1000 Train loss: 1673.860596 Valid loss: 2382.460938\n",
      "Epoch: 275/1000 Train loss: 1667.785645 Valid loss: 2373.838867\n",
      "Epoch: 276/1000 Train loss: 1661.752930 Valid loss: 2365.278809\n",
      "Epoch: 277/1000 Train loss: 1655.764648 Valid loss: 2356.782471\n",
      "Epoch: 278/1000 Train loss: 1649.818726 Valid loss: 2348.346436\n",
      "Epoch: 279/1000 Train loss: 1643.916870 Valid loss: 2339.971680\n",
      "Epoch: 280/1000 Train loss: 1638.055176 Valid loss: 2331.655518\n",
      "Epoch: 281/1000 Train loss: 1632.236572 Valid loss: 2323.394531\n",
      "Epoch: 282/1000 Train loss: 1626.457397 Valid loss: 2315.190186\n",
      "Epoch: 283/1000 Train loss: 1620.717896 Valid loss: 2307.043457\n",
      "Epoch: 284/1000 Train loss: 1615.019409 Valid loss: 2298.948975\n",
      "Epoch: 285/1000 Train loss: 1609.360596 Valid loss: 2290.912354\n",
      "Epoch: 286/1000 Train loss: 1603.741089 Valid loss: 2282.930908\n",
      "Epoch: 287/1000 Train loss: 1598.160278 Valid loss: 2274.999512\n",
      "Epoch: 288/1000 Train loss: 1592.616333 Valid loss: 2267.120361\n",
      "Epoch: 289/1000 Train loss: 1587.111450 Valid loss: 2259.297607\n",
      "Epoch: 290/1000 Train loss: 1581.645020 Valid loss: 2251.528076\n",
      "Epoch: 291/1000 Train loss: 1576.215942 Valid loss: 2243.810303\n",
      "Epoch: 292/1000 Train loss: 1570.823242 Valid loss: 2236.144043\n",
      "Epoch: 293/1000 Train loss: 1565.466797 Valid loss: 2228.529053\n",
      "Epoch: 294/1000 Train loss: 1560.146606 Valid loss: 2220.963867\n",
      "Epoch: 295/1000 Train loss: 1554.862183 Valid loss: 2213.448486\n",
      "Epoch: 296/1000 Train loss: 1549.614014 Valid loss: 2205.987061\n",
      "Epoch: 297/1000 Train loss: 1544.400391 Valid loss: 2198.573975\n",
      "Epoch: 298/1000 Train loss: 1539.221558 Valid loss: 2191.209229\n",
      "Epoch: 299/1000 Train loss: 1534.077759 Valid loss: 2183.891602\n",
      "Epoch: 300/1000 Train loss: 1528.967407 Valid loss: 2176.621582\n",
      "Epoch: 301/1000 Train loss: 1523.890991 Valid loss: 2169.401611\n",
      "Epoch: 302/1000 Train loss: 1518.848267 Valid loss: 2162.228760\n",
      "Epoch: 303/1000 Train loss: 1513.838867 Valid loss: 2155.103516\n",
      "Epoch: 304/1000 Train loss: 1508.862061 Valid loss: 2148.025635\n",
      "Epoch: 305/1000 Train loss: 1503.917603 Valid loss: 2140.993164\n",
      "Epoch: 306/1000 Train loss: 1499.005737 Valid loss: 2134.006104\n",
      "Epoch: 307/1000 Train loss: 1494.125488 Valid loss: 2127.063965\n",
      "Epoch: 308/1000 Train loss: 1489.277100 Valid loss: 2120.166016\n",
      "Epoch: 309/1000 Train loss: 1484.459839 Valid loss: 2113.312012\n",
      "Epoch: 310/1000 Train loss: 1479.673584 Valid loss: 2106.502686\n",
      "Epoch: 311/1000 Train loss: 1474.917725 Valid loss: 2099.737549\n",
      "Epoch: 312/1000 Train loss: 1470.193115 Valid loss: 2093.016113\n",
      "Epoch: 313/1000 Train loss: 1465.498291 Valid loss: 2086.337158\n",
      "Epoch: 314/1000 Train loss: 1460.833252 Valid loss: 2079.700928\n",
      "Epoch: 315/1000 Train loss: 1456.197632 Valid loss: 2073.105713\n",
      "Epoch: 316/1000 Train loss: 1451.591187 Valid loss: 2066.552246\n",
      "Epoch: 317/1000 Train loss: 1447.013794 Valid loss: 2060.038818\n",
      "Epoch: 318/1000 Train loss: 1442.464722 Valid loss: 2053.565918\n",
      "Epoch: 319/1000 Train loss: 1437.944580 Valid loss: 2047.133667\n",
      "Epoch: 320/1000 Train loss: 1433.452393 Valid loss: 2040.741455\n",
      "Epoch: 321/1000 Train loss: 1428.988159 Valid loss: 2034.388916\n",
      "Epoch: 322/1000 Train loss: 1424.551636 Valid loss: 2028.075073\n",
      "Epoch: 323/1000 Train loss: 1420.142822 Valid loss: 2021.800293\n",
      "Epoch: 324/1000 Train loss: 1415.760742 Valid loss: 2015.564331\n",
      "Epoch: 325/1000 Train loss: 1411.405762 Valid loss: 2009.366577\n",
      "Epoch: 326/1000 Train loss: 1407.077393 Valid loss: 2003.206421\n",
      "Epoch: 327/1000 Train loss: 1402.775391 Valid loss: 1997.083740\n",
      "Epoch: 328/1000 Train loss: 1398.499634 Valid loss: 1990.998901\n",
      "Epoch: 329/1000 Train loss: 1394.250122 Valid loss: 1984.950684\n",
      "Epoch: 330/1000 Train loss: 1390.026001 Valid loss: 1978.939331\n",
      "Epoch: 331/1000 Train loss: 1385.827515 Valid loss: 1972.963745\n",
      "Epoch: 332/1000 Train loss: 1381.654297 Valid loss: 1967.024658\n",
      "Epoch: 333/1000 Train loss: 1377.506348 Valid loss: 1961.121094\n",
      "Epoch: 334/1000 Train loss: 1373.382935 Valid loss: 1955.252319\n",
      "Epoch: 335/1000 Train loss: 1369.284058 Valid loss: 1949.418701\n",
      "Epoch: 336/1000 Train loss: 1365.209717 Valid loss: 1943.619629\n",
      "Epoch: 337/1000 Train loss: 1361.159424 Valid loss: 1937.854614\n",
      "Epoch: 338/1000 Train loss: 1357.133057 Valid loss: 1932.123779\n",
      "Epoch: 339/1000 Train loss: 1353.130371 Valid loss: 1926.426636\n",
      "Epoch: 340/1000 Train loss: 1349.151123 Valid loss: 1920.763062\n",
      "Epoch: 341/1000 Train loss: 1345.195435 Valid loss: 1915.132690\n",
      "Epoch: 342/1000 Train loss: 1341.262695 Valid loss: 1909.534912\n",
      "Epoch: 343/1000 Train loss: 1337.353027 Valid loss: 1903.969727\n",
      "Epoch: 344/1000 Train loss: 1333.466064 Valid loss: 1898.436890\n",
      "Epoch: 345/1000 Train loss: 1329.601685 Valid loss: 1892.936035\n",
      "Epoch: 346/1000 Train loss: 1325.759521 Valid loss: 1887.466797\n",
      "Epoch: 347/1000 Train loss: 1321.939453 Valid loss: 1882.029175\n",
      "Epoch: 348/1000 Train loss: 1318.141479 Valid loss: 1876.623047\n",
      "Epoch: 349/1000 Train loss: 1314.365112 Valid loss: 1871.247681\n",
      "Epoch: 350/1000 Train loss: 1310.610352 Valid loss: 1865.902954\n",
      "Epoch: 351/1000 Train loss: 1306.877075 Valid loss: 1860.588501\n",
      "Epoch: 352/1000 Train loss: 1303.164917 Valid loss: 1855.304565\n",
      "Epoch: 353/1000 Train loss: 1299.473755 Valid loss: 1850.050415\n",
      "Epoch: 354/1000 Train loss: 1295.803589 Valid loss: 1844.825806\n",
      "Epoch: 355/1000 Train loss: 1292.153931 Valid loss: 1839.630493\n",
      "Epoch: 356/1000 Train loss: 1288.524780 Valid loss: 1834.464355\n",
      "Epoch: 357/1000 Train loss: 1284.916016 Valid loss: 1829.327026\n",
      "Epoch: 358/1000 Train loss: 1281.327271 Valid loss: 1824.218384\n",
      "Epoch: 359/1000 Train loss: 1277.758545 Valid loss: 1819.138184\n",
      "Epoch: 360/1000 Train loss: 1274.209717 Valid loss: 1814.086304\n",
      "Epoch: 361/1000 Train loss: 1270.680542 Valid loss: 1809.062378\n",
      "Epoch: 362/1000 Train loss: 1267.170898 Valid loss: 1804.066284\n",
      "Epoch: 363/1000 Train loss: 1263.680664 Valid loss: 1799.097656\n",
      "Epoch: 364/1000 Train loss: 1260.209351 Valid loss: 1794.156250\n",
      "Epoch: 365/1000 Train loss: 1256.757202 Valid loss: 1789.241943\n",
      "Epoch: 366/1000 Train loss: 1253.323853 Valid loss: 1784.354370\n",
      "Epoch: 367/1000 Train loss: 1249.909058 Valid loss: 1779.493652\n",
      "Epoch: 368/1000 Train loss: 1246.513184 Valid loss: 1774.659180\n",
      "Epoch: 369/1000 Train loss: 1243.135498 Valid loss: 1769.851074\n",
      "Epoch: 370/1000 Train loss: 1239.776001 Valid loss: 1765.068848\n",
      "Epoch: 371/1000 Train loss: 1236.434692 Valid loss: 1760.312378\n",
      "Epoch: 372/1000 Train loss: 1233.111450 Valid loss: 1755.581299\n",
      "Epoch: 373/1000 Train loss: 1229.805908 Valid loss: 1750.875610\n",
      "Epoch: 374/1000 Train loss: 1226.518066 Valid loss: 1746.195190\n",
      "Epoch: 375/1000 Train loss: 1223.247803 Valid loss: 1741.539673\n",
      "Epoch: 376/1000 Train loss: 1219.994751 Valid loss: 1736.908936\n",
      "Epoch: 377/1000 Train loss: 1216.759155 Valid loss: 1732.302979\n",
      "Epoch: 378/1000 Train loss: 1213.540649 Valid loss: 1727.721191\n",
      "Epoch: 379/1000 Train loss: 1210.338989 Valid loss: 1723.163696\n",
      "Epoch: 380/1000 Train loss: 1207.154297 Valid loss: 1718.630249\n",
      "Epoch: 381/1000 Train loss: 1203.986206 Valid loss: 1714.120605\n",
      "Epoch: 382/1000 Train loss: 1200.834839 Valid loss: 1709.634521\n",
      "Epoch: 383/1000 Train loss: 1197.699951 Valid loss: 1705.171875\n",
      "Epoch: 384/1000 Train loss: 1194.581299 Valid loss: 1700.732422\n",
      "Epoch: 385/1000 Train loss: 1191.479004 Valid loss: 1696.316040\n",
      "Epoch: 386/1000 Train loss: 1188.392700 Valid loss: 1691.922729\n",
      "Epoch: 387/1000 Train loss: 1185.322388 Valid loss: 1687.551880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 388/1000 Train loss: 1182.267822 Valid loss: 1683.203491\n",
      "Epoch: 389/1000 Train loss: 1179.229004 Valid loss: 1678.877563\n",
      "Epoch: 390/1000 Train loss: 1176.205688 Valid loss: 1674.573853\n",
      "Epoch: 391/1000 Train loss: 1173.197876 Valid loss: 1670.292114\n",
      "Epoch: 392/1000 Train loss: 1170.205322 Valid loss: 1666.032227\n",
      "Epoch: 393/1000 Train loss: 1167.228027 Valid loss: 1661.793823\n",
      "Epoch: 394/1000 Train loss: 1164.265747 Valid loss: 1657.576904\n",
      "Epoch: 395/1000 Train loss: 1161.318604 Valid loss: 1653.381470\n",
      "Epoch: 396/1000 Train loss: 1158.386353 Valid loss: 1649.207153\n",
      "Epoch: 397/1000 Train loss: 1155.468872 Valid loss: 1645.053955\n",
      "Epoch: 398/1000 Train loss: 1152.565918 Valid loss: 1640.921631\n",
      "Epoch: 399/1000 Train loss: 1149.677612 Valid loss: 1636.809937\n",
      "Epoch: 400/1000 Train loss: 1146.803711 Valid loss: 1632.718872\n",
      "Epoch: 401/1000 Train loss: 1143.944092 Valid loss: 1628.648193\n",
      "Epoch: 402/1000 Train loss: 1141.098877 Valid loss: 1624.597656\n",
      "Epoch: 403/1000 Train loss: 1138.267700 Valid loss: 1620.567383\n",
      "Epoch: 404/1000 Train loss: 1135.450439 Valid loss: 1616.557129\n",
      "Epoch: 405/1000 Train loss: 1132.647339 Valid loss: 1612.566772\n",
      "Epoch: 406/1000 Train loss: 1129.857910 Valid loss: 1608.595947\n",
      "Epoch: 407/1000 Train loss: 1127.082031 Valid loss: 1604.644775\n",
      "Epoch: 408/1000 Train loss: 1124.319946 Valid loss: 1600.712891\n",
      "Epoch: 409/1000 Train loss: 1121.571289 Valid loss: 1596.800171\n",
      "Epoch: 410/1000 Train loss: 1118.836060 Valid loss: 1592.906616\n",
      "Epoch: 411/1000 Train loss: 1116.114014 Valid loss: 1589.031982\n",
      "Epoch: 412/1000 Train loss: 1113.405396 Valid loss: 1585.176270\n",
      "Epoch: 413/1000 Train loss: 1110.709961 Valid loss: 1581.339111\n",
      "Epoch: 414/1000 Train loss: 1108.027466 Valid loss: 1577.520630\n",
      "Epoch: 415/1000 Train loss: 1105.357910 Valid loss: 1573.720581\n",
      "Epoch: 416/1000 Train loss: 1102.701050 Valid loss: 1569.938599\n",
      "Epoch: 417/1000 Train loss: 1100.057129 Valid loss: 1566.174805\n",
      "Epoch: 418/1000 Train loss: 1097.425781 Valid loss: 1562.428955\n",
      "Epoch: 419/1000 Train loss: 1094.806885 Valid loss: 1558.700928\n",
      "Epoch: 420/1000 Train loss: 1092.200439 Valid loss: 1554.990601\n",
      "Epoch: 421/1000 Train loss: 1089.606445 Valid loss: 1551.297974\n",
      "Epoch: 422/1000 Train loss: 1087.024780 Valid loss: 1547.622803\n",
      "Epoch: 423/1000 Train loss: 1084.455078 Valid loss: 1543.964966\n",
      "Epoch: 424/1000 Train loss: 1081.897705 Valid loss: 1540.324341\n",
      "Epoch: 425/1000 Train loss: 1079.352295 Valid loss: 1536.700928\n",
      "Epoch: 426/1000 Train loss: 1076.818848 Valid loss: 1533.094604\n",
      "Epoch: 427/1000 Train loss: 1074.297363 Valid loss: 1529.505249\n",
      "Epoch: 428/1000 Train loss: 1071.787598 Valid loss: 1525.932495\n",
      "Epoch: 429/1000 Train loss: 1069.289551 Valid loss: 1522.376343\n",
      "Epoch: 430/1000 Train loss: 1066.803101 Valid loss: 1518.836792\n",
      "Epoch: 431/1000 Train loss: 1064.328125 Valid loss: 1515.313843\n",
      "Epoch: 432/1000 Train loss: 1061.864624 Valid loss: 1511.806885\n",
      "Epoch: 433/1000 Train loss: 1059.412598 Valid loss: 1508.316284\n",
      "Epoch: 434/1000 Train loss: 1056.971680 Valid loss: 1504.841797\n",
      "Epoch: 435/1000 Train loss: 1054.542114 Valid loss: 1501.383301\n",
      "Epoch: 436/1000 Train loss: 1052.123657 Valid loss: 1497.940674\n",
      "Epoch: 437/1000 Train loss: 1049.716309 Valid loss: 1494.513916\n",
      "Epoch: 438/1000 Train loss: 1047.319946 Valid loss: 1491.102661\n",
      "Epoch: 439/1000 Train loss: 1044.934570 Valid loss: 1487.706787\n",
      "Epoch: 440/1000 Train loss: 1042.560059 Valid loss: 1484.326416\n",
      "Epoch: 441/1000 Train loss: 1040.196167 Valid loss: 1480.961304\n",
      "Epoch: 442/1000 Train loss: 1037.843018 Valid loss: 1477.611572\n",
      "Epoch: 443/1000 Train loss: 1035.500610 Valid loss: 1474.276978\n",
      "Epoch: 444/1000 Train loss: 1033.168701 Valid loss: 1470.957153\n",
      "Epoch: 445/1000 Train loss: 1030.847168 Valid loss: 1467.652466\n",
      "Epoch: 446/1000 Train loss: 1028.536011 Valid loss: 1464.362671\n",
      "Epoch: 447/1000 Train loss: 1026.235229 Valid loss: 1461.087524\n",
      "Epoch: 448/1000 Train loss: 1023.944824 Valid loss: 1457.826904\n",
      "Epoch: 449/1000 Train loss: 1021.664612 Valid loss: 1454.581055\n",
      "Epoch: 450/1000 Train loss: 1019.394409 Valid loss: 1451.349731\n",
      "Epoch: 451/1000 Train loss: 1017.134338 Valid loss: 1448.132568\n",
      "Epoch: 452/1000 Train loss: 1014.884338 Valid loss: 1444.929688\n",
      "Epoch: 453/1000 Train loss: 1012.644226 Valid loss: 1441.740845\n",
      "Epoch: 454/1000 Train loss: 1010.413940 Valid loss: 1438.566040\n",
      "Epoch: 455/1000 Train loss: 1008.193481 Valid loss: 1435.405151\n",
      "Epoch: 456/1000 Train loss: 1005.982727 Valid loss: 1432.258179\n",
      "Epoch: 457/1000 Train loss: 1003.781677 Valid loss: 1429.125000\n",
      "Epoch: 458/1000 Train loss: 1001.590210 Valid loss: 1426.005737\n",
      "Epoch: 459/1000 Train loss: 999.408386 Valid loss: 1422.899780\n",
      "Epoch: 460/1000 Train loss: 997.235901 Valid loss: 1419.807617\n",
      "Epoch: 461/1000 Train loss: 995.072998 Valid loss: 1416.728516\n",
      "Epoch: 462/1000 Train loss: 992.919312 Valid loss: 1413.662842\n",
      "Epoch: 463/1000 Train loss: 990.775024 Valid loss: 1410.610596\n",
      "Epoch: 464/1000 Train loss: 988.640076 Valid loss: 1407.571411\n",
      "Epoch: 465/1000 Train loss: 986.514221 Valid loss: 1404.545288\n",
      "Epoch: 466/1000 Train loss: 984.397522 Valid loss: 1401.532227\n",
      "Epoch: 467/1000 Train loss: 982.289856 Valid loss: 1398.532104\n",
      "Epoch: 468/1000 Train loss: 980.191223 Valid loss: 1395.544800\n",
      "Epoch: 469/1000 Train loss: 978.101562 Valid loss: 1392.570068\n",
      "Epoch: 470/1000 Train loss: 976.020752 Valid loss: 1389.608398\n",
      "Epoch: 471/1000 Train loss: 973.948792 Valid loss: 1386.659058\n",
      "Epoch: 472/1000 Train loss: 971.885620 Valid loss: 1383.722168\n",
      "Epoch: 473/1000 Train loss: 969.831116 Valid loss: 1380.797852\n",
      "Epoch: 474/1000 Train loss: 967.785217 Valid loss: 1377.885620\n",
      "Epoch: 475/1000 Train loss: 965.747986 Valid loss: 1374.985840\n",
      "Epoch: 476/1000 Train loss: 963.719299 Valid loss: 1372.098267\n",
      "Epoch: 477/1000 Train loss: 961.699097 Valid loss: 1369.222778\n",
      "Epoch: 478/1000 Train loss: 959.687378 Valid loss: 1366.359131\n",
      "Epoch: 479/1000 Train loss: 957.684021 Valid loss: 1363.507690\n",
      "Epoch: 480/1000 Train loss: 955.689026 Valid loss: 1360.667969\n",
      "Epoch: 481/1000 Train loss: 953.702393 Valid loss: 1357.840088\n",
      "Epoch: 482/1000 Train loss: 951.723999 Valid loss: 1355.023926\n",
      "Epoch: 483/1000 Train loss: 949.753784 Valid loss: 1352.219360\n",
      "Epoch: 484/1000 Train loss: 947.791748 Valid loss: 1349.426392\n",
      "Epoch: 485/1000 Train loss: 945.837769 Valid loss: 1346.645142\n",
      "Epoch: 486/1000 Train loss: 943.891785 Valid loss: 1343.875122\n",
      "Epoch: 487/1000 Train loss: 941.953857 Valid loss: 1341.116577\n",
      "Epoch: 488/1000 Train loss: 940.023865 Valid loss: 1338.369385\n",
      "Epoch: 489/1000 Train loss: 938.101685 Valid loss: 1335.633423\n",
      "Epoch: 490/1000 Train loss: 936.187439 Valid loss: 1332.908447\n",
      "Epoch: 491/1000 Train loss: 934.280884 Valid loss: 1330.194824\n",
      "Epoch: 492/1000 Train loss: 932.382141 Valid loss: 1327.492065\n",
      "Epoch: 493/1000 Train loss: 930.491089 Valid loss: 1324.800293\n",
      "Epoch: 494/1000 Train loss: 928.607666 Valid loss: 1322.119507\n",
      "Epoch: 495/1000 Train loss: 926.731934 Valid loss: 1319.449585\n",
      "Epoch: 496/1000 Train loss: 924.863770 Valid loss: 1316.790405\n",
      "Epoch: 497/1000 Train loss: 923.003052 Valid loss: 1314.141968\n",
      "Epoch: 498/1000 Train loss: 921.149902 Valid loss: 1311.504150\n",
      "Epoch: 499/1000 Train loss: 919.304138 Valid loss: 1308.877075\n",
      "Epoch: 500/1000 Train loss: 917.465759 Valid loss: 1306.260376\n",
      "Epoch: 501/1000 Train loss: 915.634644 Valid loss: 1303.654053\n",
      "Epoch: 502/1000 Train loss: 913.810852 Valid loss: 1301.058105\n",
      "Epoch: 503/1000 Train loss: 911.994263 Valid loss: 1298.472412\n",
      "Epoch: 504/1000 Train loss: 910.184998 Valid loss: 1295.897095\n",
      "Epoch: 505/1000 Train loss: 908.382812 Valid loss: 1293.331909\n",
      "Epoch: 506/1000 Train loss: 906.587891 Valid loss: 1290.776978\n",
      "Epoch: 507/1000 Train loss: 904.799927 Valid loss: 1288.231934\n",
      "Epoch: 508/1000 Train loss: 903.018982 Valid loss: 1285.696899\n",
      "Epoch: 509/1000 Train loss: 901.245117 Valid loss: 1283.171875\n",
      "Epoch: 510/1000 Train loss: 899.478088 Valid loss: 1280.656738\n",
      "Epoch: 511/1000 Train loss: 897.718018 Valid loss: 1278.151489\n",
      "Epoch: 512/1000 Train loss: 895.964905 Valid loss: 1275.656006\n",
      "Epoch: 513/1000 Train loss: 894.218567 Valid loss: 1273.170166\n",
      "Epoch: 514/1000 Train loss: 892.479004 Valid loss: 1270.693970\n",
      "Epoch: 515/1000 Train loss: 890.746155 Valid loss: 1268.227417\n",
      "Epoch: 516/1000 Train loss: 889.020081 Valid loss: 1265.770508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 517/1000 Train loss: 887.300720 Valid loss: 1263.322998\n",
      "Epoch: 518/1000 Train loss: 885.587891 Valid loss: 1260.884888\n",
      "Epoch: 519/1000 Train loss: 883.881775 Valid loss: 1258.456299\n",
      "Epoch: 520/1000 Train loss: 882.182251 Valid loss: 1256.036865\n",
      "Epoch: 521/1000 Train loss: 880.489197 Valid loss: 1253.626831\n",
      "Epoch: 522/1000 Train loss: 878.802612 Valid loss: 1251.225952\n",
      "Epoch: 523/1000 Train loss: 877.122437 Valid loss: 1248.834351\n",
      "Epoch: 524/1000 Train loss: 875.448730 Valid loss: 1246.451782\n",
      "Epoch: 525/1000 Train loss: 873.781311 Valid loss: 1244.078247\n",
      "Epoch: 526/1000 Train loss: 872.120300 Valid loss: 1241.713867\n",
      "Epoch: 527/1000 Train loss: 870.465637 Valid loss: 1239.358276\n",
      "Epoch: 528/1000 Train loss: 868.817200 Valid loss: 1237.011719\n",
      "Epoch: 529/1000 Train loss: 867.175049 Valid loss: 1234.674072\n",
      "Epoch: 530/1000 Train loss: 865.539001 Valid loss: 1232.345215\n",
      "Epoch: 531/1000 Train loss: 863.909180 Valid loss: 1230.025146\n",
      "Epoch: 532/1000 Train loss: 862.285400 Valid loss: 1227.713745\n",
      "Epoch: 533/1000 Train loss: 860.667786 Valid loss: 1225.411011\n",
      "Epoch: 534/1000 Train loss: 859.056152 Valid loss: 1223.116943\n",
      "Epoch: 535/1000 Train loss: 857.450623 Valid loss: 1220.831421\n",
      "Epoch: 536/1000 Train loss: 855.851074 Valid loss: 1218.554443\n",
      "Epoch: 537/1000 Train loss: 854.257446 Valid loss: 1216.285767\n",
      "Epoch: 538/1000 Train loss: 852.669739 Valid loss: 1214.025635\n",
      "Epoch: 539/1000 Train loss: 851.087952 Valid loss: 1211.773926\n",
      "Epoch: 540/1000 Train loss: 849.511902 Valid loss: 1209.530396\n",
      "Epoch: 541/1000 Train loss: 847.941833 Valid loss: 1207.295288\n",
      "Epoch: 542/1000 Train loss: 846.377502 Valid loss: 1205.068359\n",
      "Epoch: 543/1000 Train loss: 844.818909 Valid loss: 1202.849731\n",
      "Epoch: 544/1000 Train loss: 843.266113 Valid loss: 1200.639160\n",
      "Epoch: 545/1000 Train loss: 841.718994 Valid loss: 1198.436890\n",
      "Epoch: 546/1000 Train loss: 840.177490 Valid loss: 1196.242432\n",
      "Epoch: 547/1000 Train loss: 838.641663 Valid loss: 1194.056030\n",
      "Epoch: 548/1000 Train loss: 837.111511 Valid loss: 1191.877686\n",
      "Epoch: 549/1000 Train loss: 835.586914 Valid loss: 1189.707275\n",
      "Epoch: 550/1000 Train loss: 834.067871 Valid loss: 1187.544922\n",
      "Epoch: 551/1000 Train loss: 832.554260 Valid loss: 1185.390259\n",
      "Epoch: 552/1000 Train loss: 831.046204 Valid loss: 1183.243286\n",
      "Epoch: 553/1000 Train loss: 829.543579 Valid loss: 1181.104248\n",
      "Epoch: 554/1000 Train loss: 828.046326 Valid loss: 1178.973022\n",
      "Epoch: 555/1000 Train loss: 826.554565 Valid loss: 1176.849487\n",
      "Epoch: 556/1000 Train loss: 825.068115 Valid loss: 1174.733521\n",
      "Epoch: 557/1000 Train loss: 823.587097 Valid loss: 1172.625244\n",
      "Epoch: 558/1000 Train loss: 822.111328 Valid loss: 1170.524536\n",
      "Epoch: 559/1000 Train loss: 820.640869 Valid loss: 1168.431274\n",
      "Epoch: 560/1000 Train loss: 819.175659 Valid loss: 1166.345459\n",
      "Epoch: 561/1000 Train loss: 817.715637 Valid loss: 1164.267212\n",
      "Epoch: 562/1000 Train loss: 816.260803 Valid loss: 1162.196289\n",
      "Epoch: 563/1000 Train loss: 814.811157 Valid loss: 1160.132690\n",
      "Epoch: 564/1000 Train loss: 813.366577 Valid loss: 1158.076416\n",
      "Epoch: 565/1000 Train loss: 811.927124 Valid loss: 1156.027466\n",
      "Epoch: 566/1000 Train loss: 810.492798 Valid loss: 1153.985718\n",
      "Epoch: 567/1000 Train loss: 809.063477 Valid loss: 1151.951172\n",
      "Epoch: 568/1000 Train loss: 807.639221 Valid loss: 1149.923706\n",
      "Epoch: 569/1000 Train loss: 806.219971 Valid loss: 1147.903320\n",
      "Epoch: 570/1000 Train loss: 804.805664 Valid loss: 1145.890137\n",
      "Epoch: 571/1000 Train loss: 803.396423 Valid loss: 1143.883911\n",
      "Epoch: 572/1000 Train loss: 801.991943 Valid loss: 1141.884766\n",
      "Epoch: 573/1000 Train loss: 800.592468 Valid loss: 1139.892578\n",
      "Epoch: 574/1000 Train loss: 799.197937 Valid loss: 1137.907349\n",
      "Epoch: 575/1000 Train loss: 797.808167 Valid loss: 1135.928955\n",
      "Epoch: 576/1000 Train loss: 796.423218 Valid loss: 1133.957520\n",
      "Epoch: 577/1000 Train loss: 795.043030 Valid loss: 1131.992798\n",
      "Epoch: 578/1000 Train loss: 793.667664 Valid loss: 1130.034912\n",
      "Epoch: 579/1000 Train loss: 792.297058 Valid loss: 1128.083984\n",
      "Epoch: 580/1000 Train loss: 790.931152 Valid loss: 1126.139526\n",
      "Epoch: 581/1000 Train loss: 789.570007 Valid loss: 1124.202026\n",
      "Epoch: 582/1000 Train loss: 788.213501 Valid loss: 1122.271118\n",
      "Epoch: 583/1000 Train loss: 786.861633 Valid loss: 1120.346924\n",
      "Epoch: 584/1000 Train loss: 785.514404 Valid loss: 1118.429199\n",
      "Epoch: 585/1000 Train loss: 784.171753 Valid loss: 1116.517944\n",
      "Epoch: 586/1000 Train loss: 782.833740 Valid loss: 1114.613403\n",
      "Epoch: 587/1000 Train loss: 781.500244 Valid loss: 1112.715210\n",
      "Epoch: 588/1000 Train loss: 780.171265 Valid loss: 1110.823364\n",
      "Epoch: 589/1000 Train loss: 778.846863 Valid loss: 1108.938110\n",
      "Epoch: 590/1000 Train loss: 777.526855 Valid loss: 1107.059204\n",
      "Epoch: 591/1000 Train loss: 776.211365 Valid loss: 1105.186646\n",
      "Epoch: 592/1000 Train loss: 774.900330 Valid loss: 1103.320312\n",
      "Epoch: 593/1000 Train loss: 773.593750 Valid loss: 1101.460327\n",
      "Epoch: 594/1000 Train loss: 772.291504 Valid loss: 1099.606689\n",
      "Epoch: 595/1000 Train loss: 770.993652 Valid loss: 1097.759155\n",
      "Epoch: 596/1000 Train loss: 769.700134 Valid loss: 1095.917969\n",
      "Epoch: 597/1000 Train loss: 768.411011 Valid loss: 1094.082764\n",
      "Epoch: 598/1000 Train loss: 767.126099 Valid loss: 1092.253906\n",
      "Epoch: 599/1000 Train loss: 765.845581 Valid loss: 1090.431030\n",
      "Epoch: 600/1000 Train loss: 764.569275 Valid loss: 1088.614258\n",
      "Epoch: 601/1000 Train loss: 763.297241 Valid loss: 1086.803467\n",
      "Epoch: 602/1000 Train loss: 762.029419 Valid loss: 1084.998779\n",
      "Epoch: 603/1000 Train loss: 760.765808 Valid loss: 1083.199951\n",
      "Epoch: 604/1000 Train loss: 759.506409 Valid loss: 1081.407104\n",
      "Epoch: 605/1000 Train loss: 758.251221 Valid loss: 1079.620239\n",
      "Epoch: 606/1000 Train loss: 757.000122 Valid loss: 1077.839233\n",
      "Epoch: 607/1000 Train loss: 755.753174 Valid loss: 1076.064209\n",
      "Epoch: 608/1000 Train loss: 754.510315 Valid loss: 1074.295044\n",
      "Epoch: 609/1000 Train loss: 753.271545 Valid loss: 1072.531494\n",
      "Epoch: 610/1000 Train loss: 752.036865 Valid loss: 1070.773926\n",
      "Epoch: 611/1000 Train loss: 750.806152 Valid loss: 1069.021973\n",
      "Epoch: 612/1000 Train loss: 749.579529 Valid loss: 1067.275879\n",
      "Epoch: 613/1000 Train loss: 748.356873 Valid loss: 1065.535400\n",
      "Epoch: 614/1000 Train loss: 747.138184 Valid loss: 1063.800537\n",
      "Epoch: 615/1000 Train loss: 745.923401 Valid loss: 1062.071533\n",
      "Epoch: 616/1000 Train loss: 744.712585 Valid loss: 1060.348145\n",
      "Epoch: 617/1000 Train loss: 743.505737 Valid loss: 1058.630249\n",
      "Epoch: 618/1000 Train loss: 742.302795 Valid loss: 1056.917847\n",
      "Epoch: 619/1000 Train loss: 741.103638 Valid loss: 1055.210938\n",
      "Epoch: 620/1000 Train loss: 739.908447 Valid loss: 1053.509644\n",
      "Epoch: 621/1000 Train loss: 738.717102 Valid loss: 1051.813843\n",
      "Epoch: 622/1000 Train loss: 737.529602 Valid loss: 1050.123291\n",
      "Epoch: 623/1000 Train loss: 736.345886 Valid loss: 1048.438232\n",
      "Epoch: 624/1000 Train loss: 735.165955 Valid loss: 1046.758545\n",
      "Epoch: 625/1000 Train loss: 733.989868 Valid loss: 1045.084229\n",
      "Epoch: 626/1000 Train loss: 732.817444 Valid loss: 1043.415283\n",
      "Epoch: 627/1000 Train loss: 731.648804 Valid loss: 1041.751587\n",
      "Epoch: 628/1000 Train loss: 730.483887 Valid loss: 1040.093262\n",
      "Epoch: 629/1000 Train loss: 729.322693 Valid loss: 1038.440186\n",
      "Epoch: 630/1000 Train loss: 728.165161 Valid loss: 1036.792358\n",
      "Epoch: 631/1000 Train loss: 727.011292 Valid loss: 1035.149780\n",
      "Epoch: 632/1000 Train loss: 725.861145 Valid loss: 1033.512451\n",
      "Epoch: 633/1000 Train loss: 724.714600 Valid loss: 1031.880249\n",
      "Epoch: 634/1000 Train loss: 723.571716 Valid loss: 1030.253296\n",
      "Epoch: 635/1000 Train loss: 722.432434 Valid loss: 1028.631348\n",
      "Epoch: 636/1000 Train loss: 721.296692 Valid loss: 1027.014648\n",
      "Epoch: 637/1000 Train loss: 720.164429 Valid loss: 1025.402832\n",
      "Epoch: 638/1000 Train loss: 719.035767 Valid loss: 1023.796326\n",
      "Epoch: 639/1000 Train loss: 717.910583 Valid loss: 1022.194702\n",
      "Epoch: 640/1000 Train loss: 716.789001 Valid loss: 1020.598083\n",
      "Epoch: 641/1000 Train loss: 715.670898 Valid loss: 1019.006409\n",
      "Epoch: 642/1000 Train loss: 714.556213 Valid loss: 1017.419739\n",
      "Epoch: 643/1000 Train loss: 713.445068 Valid loss: 1015.837952\n",
      "Epoch: 644/1000 Train loss: 712.337341 Valid loss: 1014.261108\n",
      "Epoch: 645/1000 Train loss: 711.232971 Valid loss: 1012.689087\n",
      "Epoch: 646/1000 Train loss: 710.132141 Valid loss: 1011.122070\n",
      "Epoch: 647/1000 Train loss: 709.034607 Valid loss: 1009.559753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 648/1000 Train loss: 707.940552 Valid loss: 1008.002319\n",
      "Epoch: 649/1000 Train loss: 706.849792 Valid loss: 1006.449646\n",
      "Epoch: 650/1000 Train loss: 705.762390 Valid loss: 1004.901794\n",
      "Epoch: 651/1000 Train loss: 704.678406 Valid loss: 1003.358643\n",
      "Epoch: 652/1000 Train loss: 703.597656 Valid loss: 1001.820312\n",
      "Epoch: 653/1000 Train loss: 702.520325 Valid loss: 1000.286621\n",
      "Epoch: 654/1000 Train loss: 701.446167 Valid loss: 998.757629\n",
      "Epoch: 655/1000 Train loss: 700.375366 Valid loss: 997.233276\n",
      "Epoch: 656/1000 Train loss: 699.307800 Valid loss: 995.713684\n",
      "Epoch: 657/1000 Train loss: 698.243530 Valid loss: 994.198608\n",
      "Epoch: 658/1000 Train loss: 697.182434 Valid loss: 992.688171\n",
      "Epoch: 659/1000 Train loss: 696.124634 Valid loss: 991.182251\n",
      "Epoch: 660/1000 Train loss: 695.069946 Valid loss: 989.680969\n",
      "Epoch: 661/1000 Train loss: 694.018494 Valid loss: 988.184204\n",
      "Epoch: 662/1000 Train loss: 692.970215 Valid loss: 986.691956\n",
      "Epoch: 663/1000 Train loss: 691.925110 Valid loss: 985.204224\n",
      "Epoch: 664/1000 Train loss: 690.883179 Valid loss: 983.720947\n",
      "Epoch: 665/1000 Train loss: 689.844360 Valid loss: 982.242188\n",
      "Epoch: 666/1000 Train loss: 688.808655 Valid loss: 980.767822\n",
      "Epoch: 667/1000 Train loss: 687.776062 Valid loss: 979.297913\n",
      "Epoch: 668/1000 Train loss: 686.746582 Valid loss: 977.832397\n",
      "Epoch: 669/1000 Train loss: 685.720093 Valid loss: 976.371155\n",
      "Epoch: 670/1000 Train loss: 684.696777 Valid loss: 974.914368\n",
      "Epoch: 671/1000 Train loss: 683.676453 Valid loss: 973.461853\n",
      "Epoch: 672/1000 Train loss: 682.659180 Valid loss: 972.013672\n",
      "Epoch: 673/1000 Train loss: 681.644897 Valid loss: 970.569885\n",
      "Epoch: 674/1000 Train loss: 680.633667 Valid loss: 969.130249\n",
      "Epoch: 675/1000 Train loss: 679.625366 Valid loss: 967.694946\n",
      "Epoch: 676/1000 Train loss: 678.620117 Valid loss: 966.263916\n",
      "Epoch: 677/1000 Train loss: 677.617859 Valid loss: 964.837097\n",
      "Epoch: 678/1000 Train loss: 676.618530 Valid loss: 963.414429\n",
      "Epoch: 679/1000 Train loss: 675.622070 Valid loss: 961.996033\n",
      "Epoch: 680/1000 Train loss: 674.628662 Valid loss: 960.581787\n",
      "Epoch: 681/1000 Train loss: 673.638062 Valid loss: 959.171631\n",
      "Epoch: 682/1000 Train loss: 672.650391 Valid loss: 957.765686\n",
      "Epoch: 683/1000 Train loss: 671.665649 Valid loss: 956.363892\n",
      "Epoch: 684/1000 Train loss: 670.683716 Valid loss: 954.966064\n",
      "Epoch: 685/1000 Train loss: 669.704773 Valid loss: 953.572388\n",
      "Epoch: 686/1000 Train loss: 668.728577 Valid loss: 952.182739\n",
      "Epoch: 687/1000 Train loss: 667.755310 Valid loss: 950.797119\n",
      "Epoch: 688/1000 Train loss: 666.784851 Valid loss: 949.415588\n",
      "Epoch: 689/1000 Train loss: 665.817200 Valid loss: 948.038025\n",
      "Epoch: 690/1000 Train loss: 664.852295 Valid loss: 946.664490\n",
      "Epoch: 691/1000 Train loss: 663.890198 Valid loss: 945.294861\n",
      "Epoch: 692/1000 Train loss: 662.930908 Valid loss: 943.929199\n",
      "Epoch: 693/1000 Train loss: 661.974426 Valid loss: 942.567505\n",
      "Epoch: 694/1000 Train loss: 661.020630 Valid loss: 941.209656\n",
      "Epoch: 695/1000 Train loss: 660.069580 Valid loss: 939.855774\n",
      "Epoch: 696/1000 Train loss: 659.121338 Valid loss: 938.505737\n",
      "Epoch: 697/1000 Train loss: 658.175720 Valid loss: 937.159668\n",
      "Epoch: 698/1000 Train loss: 657.232910 Valid loss: 935.817444\n",
      "Epoch: 699/1000 Train loss: 656.292725 Valid loss: 934.479065\n",
      "Epoch: 700/1000 Train loss: 655.355286 Valid loss: 933.144592\n",
      "Epoch: 701/1000 Train loss: 654.420532 Valid loss: 931.813843\n",
      "Epoch: 702/1000 Train loss: 653.488403 Valid loss: 930.486877\n",
      "Epoch: 703/1000 Train loss: 652.558899 Valid loss: 929.163757\n",
      "Epoch: 704/1000 Train loss: 651.632080 Valid loss: 927.844299\n",
      "Epoch: 705/1000 Train loss: 650.707886 Valid loss: 926.528564\n",
      "Epoch: 706/1000 Train loss: 649.786316 Valid loss: 925.216614\n",
      "Epoch: 707/1000 Train loss: 648.867310 Valid loss: 923.908325\n",
      "Epoch: 708/1000 Train loss: 647.950928 Valid loss: 922.603760\n",
      "Epoch: 709/1000 Train loss: 647.037109 Valid loss: 921.302917\n",
      "Epoch: 710/1000 Train loss: 646.125854 Valid loss: 920.005615\n",
      "Epoch: 711/1000 Train loss: 645.217224 Valid loss: 918.712097\n",
      "Epoch: 712/1000 Train loss: 644.311096 Valid loss: 917.422119\n",
      "Epoch: 713/1000 Train loss: 643.407471 Valid loss: 916.135742\n",
      "Epoch: 714/1000 Train loss: 642.506470 Valid loss: 914.853027\n",
      "Epoch: 715/1000 Train loss: 641.607910 Valid loss: 913.573853\n",
      "Epoch: 716/1000 Train loss: 640.711853 Valid loss: 912.298340\n",
      "Epoch: 717/1000 Train loss: 639.818359 Valid loss: 911.026306\n",
      "Epoch: 718/1000 Train loss: 638.927368 Valid loss: 909.757874\n",
      "Epoch: 719/1000 Train loss: 638.038757 Valid loss: 908.492920\n",
      "Epoch: 720/1000 Train loss: 637.152710 Valid loss: 907.231506\n",
      "Epoch: 721/1000 Train loss: 636.269043 Valid loss: 905.973572\n",
      "Epoch: 722/1000 Train loss: 635.387878 Valid loss: 904.719177\n",
      "Epoch: 723/1000 Train loss: 634.509155 Valid loss: 903.468201\n",
      "Epoch: 724/1000 Train loss: 633.632812 Valid loss: 902.220703\n",
      "Epoch: 725/1000 Train loss: 632.758911 Valid loss: 900.976685\n",
      "Epoch: 726/1000 Train loss: 631.887451 Valid loss: 899.736023\n",
      "Epoch: 727/1000 Train loss: 631.018311 Valid loss: 898.498840\n",
      "Epoch: 728/1000 Train loss: 630.151672 Valid loss: 897.265015\n",
      "Epoch: 729/1000 Train loss: 629.287354 Valid loss: 896.034546\n",
      "Epoch: 730/1000 Train loss: 628.425415 Valid loss: 894.807495\n",
      "Epoch: 731/1000 Train loss: 627.565857 Valid loss: 893.583862\n",
      "Epoch: 732/1000 Train loss: 626.708557 Valid loss: 892.363525\n",
      "Epoch: 733/1000 Train loss: 625.853699 Valid loss: 891.146545\n",
      "Epoch: 734/1000 Train loss: 625.001099 Valid loss: 889.932800\n",
      "Epoch: 735/1000 Train loss: 624.150818 Valid loss: 888.722412\n",
      "Epoch: 736/1000 Train loss: 623.302856 Valid loss: 887.515320\n",
      "Epoch: 737/1000 Train loss: 622.457214 Valid loss: 886.311462\n",
      "Epoch: 738/1000 Train loss: 621.613953 Valid loss: 885.110901\n",
      "Epoch: 739/1000 Train loss: 620.772827 Valid loss: 883.913574\n",
      "Epoch: 740/1000 Train loss: 619.934021 Valid loss: 882.719543\n",
      "Epoch: 741/1000 Train loss: 619.097412 Valid loss: 881.528625\n",
      "Epoch: 742/1000 Train loss: 618.263184 Valid loss: 880.340942\n",
      "Epoch: 743/1000 Train loss: 617.431152 Valid loss: 879.156494\n",
      "Epoch: 744/1000 Train loss: 616.601379 Valid loss: 877.975220\n",
      "Epoch: 745/1000 Train loss: 615.773804 Valid loss: 876.797058\n",
      "Epoch: 746/1000 Train loss: 614.948486 Valid loss: 875.622070\n",
      "Epoch: 747/1000 Train loss: 614.125305 Valid loss: 874.450256\n",
      "Epoch: 748/1000 Train loss: 613.304443 Valid loss: 873.281616\n",
      "Epoch: 749/1000 Train loss: 612.485718 Valid loss: 872.115967\n",
      "Epoch: 750/1000 Train loss: 611.669189 Valid loss: 870.953552\n",
      "Epoch: 751/1000 Train loss: 610.854797 Valid loss: 869.794189\n",
      "Epoch: 752/1000 Train loss: 610.042603 Valid loss: 868.637939\n",
      "Epoch: 753/1000 Train loss: 609.232544 Valid loss: 867.484741\n",
      "Epoch: 754/1000 Train loss: 608.424683 Valid loss: 866.334595\n",
      "Epoch: 755/1000 Train loss: 607.618896 Valid loss: 865.187561\n",
      "Epoch: 756/1000 Train loss: 606.815308 Valid loss: 864.043518\n",
      "Epoch: 757/1000 Train loss: 606.013794 Valid loss: 862.902527\n",
      "Epoch: 758/1000 Train loss: 605.214417 Valid loss: 861.764526\n",
      "Epoch: 759/1000 Train loss: 604.417114 Valid loss: 860.629578\n",
      "Epoch: 760/1000 Train loss: 603.622070 Valid loss: 859.497559\n",
      "Epoch: 761/1000 Train loss: 602.829041 Valid loss: 858.368591\n",
      "Epoch: 762/1000 Train loss: 602.038025 Valid loss: 857.242554\n",
      "Epoch: 763/1000 Train loss: 601.249084 Valid loss: 856.119446\n",
      "Epoch: 764/1000 Train loss: 600.462158 Valid loss: 854.999329\n",
      "Epoch: 765/1000 Train loss: 599.677307 Valid loss: 853.882080\n",
      "Epoch: 766/1000 Train loss: 598.894531 Valid loss: 852.767761\n",
      "Epoch: 767/1000 Train loss: 598.113770 Valid loss: 851.656311\n",
      "Epoch: 768/1000 Train loss: 597.335022 Valid loss: 850.547791\n",
      "Epoch: 769/1000 Train loss: 596.558411 Valid loss: 849.442078\n",
      "Epoch: 770/1000 Train loss: 595.783752 Valid loss: 848.339294\n",
      "Epoch: 771/1000 Train loss: 595.011047 Valid loss: 847.239380\n",
      "Epoch: 772/1000 Train loss: 594.240417 Valid loss: 846.142273\n",
      "Epoch: 773/1000 Train loss: 593.471741 Valid loss: 845.048096\n",
      "Epoch: 774/1000 Train loss: 592.705017 Valid loss: 843.956665\n",
      "Epoch: 775/1000 Train loss: 591.940308 Valid loss: 842.868042\n",
      "Epoch: 776/1000 Train loss: 591.177551 Valid loss: 841.782288\n",
      "Epoch: 777/1000 Train loss: 590.416809 Valid loss: 840.699280\n",
      "Epoch: 778/1000 Train loss: 589.658020 Valid loss: 839.619080\n",
      "Epoch: 779/1000 Train loss: 588.901184 Valid loss: 838.541687\n",
      "Epoch: 780/1000 Train loss: 588.146240 Valid loss: 837.467041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 781/1000 Train loss: 587.393250 Valid loss: 836.395081\n",
      "Epoch: 782/1000 Train loss: 586.642212 Valid loss: 835.325928\n",
      "Epoch: 783/1000 Train loss: 585.893005 Valid loss: 834.259521\n",
      "Epoch: 784/1000 Train loss: 585.145813 Valid loss: 833.195740\n",
      "Epoch: 785/1000 Train loss: 584.400513 Valid loss: 832.134766\n",
      "Epoch: 786/1000 Train loss: 583.657043 Valid loss: 831.076477\n",
      "Epoch: 787/1000 Train loss: 582.915527 Valid loss: 830.020935\n",
      "Epoch: 788/1000 Train loss: 582.175842 Valid loss: 828.967957\n",
      "Epoch: 789/1000 Train loss: 581.438049 Valid loss: 827.917725\n",
      "Epoch: 790/1000 Train loss: 580.702148 Valid loss: 826.870117\n",
      "Epoch: 791/1000 Train loss: 579.968079 Valid loss: 825.825195\n",
      "Epoch: 792/1000 Train loss: 579.235901 Valid loss: 824.782898\n",
      "Epoch: 793/1000 Train loss: 578.505554 Valid loss: 823.743164\n",
      "Epoch: 794/1000 Train loss: 577.777039 Valid loss: 822.706055\n",
      "Epoch: 795/1000 Train loss: 577.050354 Valid loss: 821.671570\n",
      "Epoch: 796/1000 Train loss: 576.325500 Valid loss: 820.639709\n",
      "Epoch: 797/1000 Train loss: 575.602539 Valid loss: 819.610413\n",
      "Epoch: 798/1000 Train loss: 574.881287 Valid loss: 818.583679\n",
      "Epoch: 799/1000 Train loss: 574.161926 Valid loss: 817.559509\n",
      "Epoch: 800/1000 Train loss: 573.444275 Valid loss: 816.537903\n",
      "Epoch: 801/1000 Train loss: 572.728455 Valid loss: 815.518860\n",
      "Epoch: 802/1000 Train loss: 572.014404 Valid loss: 814.502380\n",
      "Epoch: 803/1000 Train loss: 571.302185 Valid loss: 813.488403\n",
      "Epoch: 804/1000 Train loss: 570.591736 Valid loss: 812.476990\n",
      "Epoch: 805/1000 Train loss: 569.882996 Valid loss: 811.468079\n",
      "Epoch: 806/1000 Train loss: 569.176025 Valid loss: 810.461670\n",
      "Epoch: 807/1000 Train loss: 568.470825 Valid loss: 809.457764\n",
      "Epoch: 808/1000 Train loss: 567.767395 Valid loss: 808.456299\n",
      "Epoch: 809/1000 Train loss: 567.065674 Valid loss: 807.457275\n",
      "Epoch: 810/1000 Train loss: 566.365662 Valid loss: 806.460815\n",
      "Epoch: 811/1000 Train loss: 565.667419 Valid loss: 805.466858\n",
      "Epoch: 812/1000 Train loss: 564.970825 Valid loss: 804.475220\n",
      "Epoch: 813/1000 Train loss: 564.276001 Valid loss: 803.486084\n",
      "Epoch: 814/1000 Train loss: 563.582886 Valid loss: 802.499390\n",
      "Epoch: 815/1000 Train loss: 562.891418 Valid loss: 801.515015\n",
      "Epoch: 816/1000 Train loss: 562.201721 Valid loss: 800.533203\n",
      "Epoch: 817/1000 Train loss: 561.513611 Valid loss: 799.553711\n",
      "Epoch: 818/1000 Train loss: 560.827271 Valid loss: 798.576599\n",
      "Epoch: 819/1000 Train loss: 560.142578 Valid loss: 797.601868\n",
      "Epoch: 820/1000 Train loss: 559.459534 Valid loss: 796.629517\n",
      "Epoch: 821/1000 Train loss: 558.778137 Valid loss: 795.659485\n",
      "Epoch: 822/1000 Train loss: 558.098450 Valid loss: 794.691895\n",
      "Epoch: 823/1000 Train loss: 557.420349 Valid loss: 793.726685\n",
      "Epoch: 824/1000 Train loss: 556.743958 Valid loss: 792.763733\n",
      "Epoch: 825/1000 Train loss: 556.069153 Valid loss: 791.803162\n",
      "Epoch: 826/1000 Train loss: 555.396057 Valid loss: 790.844910\n",
      "Epoch: 827/1000 Train loss: 554.724487 Valid loss: 789.888977\n",
      "Epoch: 828/1000 Train loss: 554.054626 Valid loss: 788.935303\n",
      "Epoch: 829/1000 Train loss: 553.386414 Valid loss: 787.984009\n",
      "Epoch: 830/1000 Train loss: 552.719788 Valid loss: 787.034973\n",
      "Epoch: 831/1000 Train loss: 552.054749 Valid loss: 786.088257\n",
      "Epoch: 832/1000 Train loss: 551.391235 Valid loss: 785.143738\n",
      "Epoch: 833/1000 Train loss: 550.729431 Valid loss: 784.201477\n",
      "Epoch: 834/1000 Train loss: 550.069092 Valid loss: 783.261536\n",
      "Epoch: 835/1000 Train loss: 549.410461 Valid loss: 782.323792\n",
      "Epoch: 836/1000 Train loss: 548.753357 Valid loss: 781.388306\n",
      "Epoch: 837/1000 Train loss: 548.097778 Valid loss: 780.455078\n",
      "Epoch: 838/1000 Train loss: 547.443848 Valid loss: 779.523987\n",
      "Epoch: 839/1000 Train loss: 546.791382 Valid loss: 778.595215\n",
      "Epoch: 840/1000 Train loss: 546.140503 Valid loss: 777.668579\n",
      "Epoch: 841/1000 Train loss: 545.491150 Valid loss: 776.744141\n",
      "Epoch: 842/1000 Train loss: 544.843445 Valid loss: 775.821960\n",
      "Epoch: 843/1000 Train loss: 544.197205 Valid loss: 774.901917\n",
      "Epoch: 844/1000 Train loss: 543.552490 Valid loss: 773.984070\n",
      "Epoch: 845/1000 Train loss: 542.909302 Valid loss: 773.068420\n",
      "Epoch: 846/1000 Train loss: 542.267639 Valid loss: 772.154968\n",
      "Epoch: 847/1000 Train loss: 541.627502 Valid loss: 771.243591\n",
      "Epoch: 848/1000 Train loss: 540.988892 Valid loss: 770.334412\n",
      "Epoch: 849/1000 Train loss: 540.351746 Valid loss: 769.427368\n",
      "Epoch: 850/1000 Train loss: 539.716125 Valid loss: 768.522461\n",
      "Epoch: 851/1000 Train loss: 539.082031 Valid loss: 767.619690\n",
      "Epoch: 852/1000 Train loss: 538.449402 Valid loss: 766.718994\n",
      "Epoch: 853/1000 Train loss: 537.818237 Valid loss: 765.820435\n",
      "Epoch: 854/1000 Train loss: 537.188599 Valid loss: 764.924011\n",
      "Epoch: 855/1000 Train loss: 536.560364 Valid loss: 764.029724\n",
      "Epoch: 856/1000 Train loss: 535.933716 Valid loss: 763.137451\n",
      "Epoch: 857/1000 Train loss: 535.308350 Valid loss: 762.247314\n",
      "Epoch: 858/1000 Train loss: 534.684570 Valid loss: 761.359253\n",
      "Epoch: 859/1000 Train loss: 534.062195 Valid loss: 760.473206\n",
      "Epoch: 860/1000 Train loss: 533.441284 Valid loss: 759.589233\n",
      "Epoch: 861/1000 Train loss: 532.821777 Valid loss: 758.707336\n",
      "Epoch: 862/1000 Train loss: 532.203735 Valid loss: 757.827454\n",
      "Epoch: 863/1000 Train loss: 531.587097 Valid loss: 756.949585\n",
      "Epoch: 864/1000 Train loss: 530.971924 Valid loss: 756.073730\n",
      "Epoch: 865/1000 Train loss: 530.358093 Valid loss: 755.199890\n",
      "Epoch: 866/1000 Train loss: 529.745728 Valid loss: 754.328125\n",
      "Epoch: 867/1000 Train loss: 529.134827 Valid loss: 753.458374\n",
      "Epoch: 868/1000 Train loss: 528.525269 Valid loss: 752.590576\n",
      "Epoch: 869/1000 Train loss: 527.917175 Valid loss: 751.724792\n",
      "Epoch: 870/1000 Train loss: 527.310425 Valid loss: 750.861023\n",
      "Epoch: 871/1000 Train loss: 526.705078 Valid loss: 749.999146\n",
      "Epoch: 872/1000 Train loss: 526.101135 Valid loss: 749.139343\n",
      "Epoch: 873/1000 Train loss: 525.498596 Valid loss: 748.281494\n",
      "Epoch: 874/1000 Train loss: 524.897400 Valid loss: 747.425659\n",
      "Epoch: 875/1000 Train loss: 524.297607 Valid loss: 746.571777\n",
      "Epoch: 876/1000 Train loss: 523.699158 Valid loss: 745.719788\n",
      "Epoch: 877/1000 Train loss: 523.102112 Valid loss: 744.869812\n",
      "Epoch: 878/1000 Train loss: 522.506470 Valid loss: 744.021667\n",
      "Epoch: 879/1000 Train loss: 521.912109 Valid loss: 743.175537\n",
      "Epoch: 880/1000 Train loss: 521.319092 Valid loss: 742.331299\n",
      "Epoch: 881/1000 Train loss: 520.727417 Valid loss: 741.489014\n",
      "Epoch: 882/1000 Train loss: 520.137085 Valid loss: 740.648560\n",
      "Epoch: 883/1000 Train loss: 519.548157 Valid loss: 739.810120\n",
      "Epoch: 884/1000 Train loss: 518.960449 Valid loss: 738.973511\n",
      "Epoch: 885/1000 Train loss: 518.374146 Valid loss: 738.138733\n",
      "Epoch: 886/1000 Train loss: 517.789185 Valid loss: 737.305969\n",
      "Epoch: 887/1000 Train loss: 517.205505 Valid loss: 736.474976\n",
      "Epoch: 888/1000 Train loss: 516.623169 Valid loss: 735.645874\n",
      "Epoch: 889/1000 Train loss: 516.042114 Valid loss: 734.818665\n",
      "Epoch: 890/1000 Train loss: 515.462402 Valid loss: 733.993286\n",
      "Epoch: 891/1000 Train loss: 514.883972 Valid loss: 733.169800\n",
      "Epoch: 892/1000 Train loss: 514.306824 Valid loss: 732.348083\n",
      "Epoch: 893/1000 Train loss: 513.730957 Valid loss: 731.528259\n",
      "Epoch: 894/1000 Train loss: 513.156433 Valid loss: 730.710205\n",
      "Epoch: 895/1000 Train loss: 512.583130 Valid loss: 729.893982\n",
      "Epoch: 896/1000 Train loss: 512.011108 Valid loss: 729.079651\n",
      "Epoch: 897/1000 Train loss: 511.440399 Valid loss: 728.267090\n",
      "Epoch: 898/1000 Train loss: 510.870911 Valid loss: 727.456360\n",
      "Epoch: 899/1000 Train loss: 510.302704 Valid loss: 726.647461\n",
      "Epoch: 900/1000 Train loss: 509.735779 Valid loss: 725.840332\n",
      "Epoch: 901/1000 Train loss: 509.170074 Valid loss: 725.034973\n",
      "Epoch: 902/1000 Train loss: 508.605682 Valid loss: 724.231445\n",
      "Epoch: 903/1000 Train loss: 508.042511 Valid loss: 723.429749\n",
      "Epoch: 904/1000 Train loss: 507.480560 Valid loss: 722.629761\n",
      "Epoch: 905/1000 Train loss: 506.919922 Valid loss: 721.831543\n",
      "Epoch: 906/1000 Train loss: 506.360474 Valid loss: 721.035095\n",
      "Epoch: 907/1000 Train loss: 505.802246 Valid loss: 720.240417\n",
      "Epoch: 908/1000 Train loss: 505.245239 Valid loss: 719.447510\n",
      "Epoch: 909/1000 Train loss: 504.689514 Valid loss: 718.656311\n",
      "Epoch: 910/1000 Train loss: 504.134949 Valid loss: 717.866882\n",
      "Epoch: 911/1000 Train loss: 503.581635 Valid loss: 717.079163\n",
      "Epoch: 912/1000 Train loss: 503.029572 Valid loss: 716.293213\n",
      "Epoch: 913/1000 Train loss: 502.478699 Valid loss: 715.508911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 914/1000 Train loss: 501.928986 Valid loss: 714.726318\n",
      "Epoch: 915/1000 Train loss: 501.380493 Valid loss: 713.945557\n",
      "Epoch: 916/1000 Train loss: 500.833191 Valid loss: 713.166382\n",
      "Epoch: 917/1000 Train loss: 500.287109 Valid loss: 712.388916\n",
      "Epoch: 918/1000 Train loss: 499.742218 Valid loss: 711.613098\n",
      "Epoch: 919/1000 Train loss: 499.198517 Valid loss: 710.839111\n",
      "Epoch: 920/1000 Train loss: 498.656006 Valid loss: 710.066711\n",
      "Epoch: 921/1000 Train loss: 498.114655 Valid loss: 709.295959\n",
      "Epoch: 922/1000 Train loss: 497.574432 Valid loss: 708.526917\n",
      "Epoch: 923/1000 Train loss: 497.035400 Valid loss: 707.759583\n",
      "Epoch: 924/1000 Train loss: 496.497589 Valid loss: 706.993896\n",
      "Epoch: 925/1000 Train loss: 495.960876 Valid loss: 706.229858\n",
      "Epoch: 926/1000 Train loss: 495.425323 Valid loss: 705.467407\n",
      "Epoch: 927/1000 Train loss: 494.890961 Valid loss: 704.706726\n",
      "Epoch: 928/1000 Train loss: 494.357758 Valid loss: 703.947571\n",
      "Epoch: 929/1000 Train loss: 493.825714 Valid loss: 703.190125\n",
      "Epoch: 930/1000 Train loss: 493.294739 Valid loss: 702.434265\n",
      "Epoch: 931/1000 Train loss: 492.764984 Valid loss: 701.680054\n",
      "Epoch: 932/1000 Train loss: 492.236298 Valid loss: 700.927490\n",
      "Epoch: 933/1000 Train loss: 491.708771 Valid loss: 700.176453\n",
      "Epoch: 934/1000 Train loss: 491.182404 Valid loss: 699.427063\n",
      "Epoch: 935/1000 Train loss: 490.657166 Valid loss: 698.679321\n",
      "Epoch: 936/1000 Train loss: 490.132996 Valid loss: 697.933105\n",
      "Epoch: 937/1000 Train loss: 489.609955 Valid loss: 697.188538\n",
      "Epoch: 938/1000 Train loss: 489.088074 Valid loss: 696.445496\n",
      "Epoch: 939/1000 Train loss: 488.567291 Valid loss: 695.704041\n",
      "Epoch: 940/1000 Train loss: 488.047577 Valid loss: 694.964233\n",
      "Epoch: 941/1000 Train loss: 487.528961 Valid loss: 694.225952\n",
      "Epoch: 942/1000 Train loss: 487.011505 Valid loss: 693.489197\n",
      "Epoch: 943/1000 Train loss: 486.495087 Valid loss: 692.754089\n",
      "Epoch: 944/1000 Train loss: 485.979797 Valid loss: 692.020447\n",
      "Epoch: 945/1000 Train loss: 485.465607 Valid loss: 691.288391\n",
      "Epoch: 946/1000 Train loss: 484.952515 Valid loss: 690.557861\n",
      "Epoch: 947/1000 Train loss: 484.440460 Valid loss: 689.828857\n",
      "Epoch: 948/1000 Train loss: 483.929535 Valid loss: 689.101379\n",
      "Epoch: 949/1000 Train loss: 483.419647 Valid loss: 688.375488\n",
      "Epoch: 950/1000 Train loss: 482.910858 Valid loss: 687.651123\n",
      "Epoch: 951/1000 Train loss: 482.403168 Valid loss: 686.928284\n",
      "Epoch: 952/1000 Train loss: 481.896545 Valid loss: 686.206970\n",
      "Epoch: 953/1000 Train loss: 481.390930 Valid loss: 685.487183\n",
      "Epoch: 954/1000 Train loss: 480.886444 Valid loss: 684.768860\n",
      "Epoch: 955/1000 Train loss: 480.382996 Valid loss: 684.052124\n",
      "Epoch: 956/1000 Train loss: 479.880554 Valid loss: 683.336792\n",
      "Epoch: 957/1000 Train loss: 479.379150 Valid loss: 682.622925\n",
      "Epoch: 958/1000 Train loss: 478.878845 Valid loss: 681.910645\n",
      "Epoch: 959/1000 Train loss: 478.379517 Valid loss: 681.199829\n",
      "Epoch: 960/1000 Train loss: 477.881256 Valid loss: 680.490479\n",
      "Epoch: 961/1000 Train loss: 477.384064 Valid loss: 679.782532\n",
      "Epoch: 962/1000 Train loss: 476.887848 Valid loss: 679.076172\n",
      "Epoch: 963/1000 Train loss: 476.392700 Valid loss: 678.371216\n",
      "Epoch: 964/1000 Train loss: 475.898560 Valid loss: 677.667786\n",
      "Epoch: 965/1000 Train loss: 475.405426 Valid loss: 676.965698\n",
      "Epoch: 966/1000 Train loss: 474.913330 Valid loss: 676.265198\n",
      "Epoch: 967/1000 Train loss: 474.422302 Valid loss: 675.566101\n",
      "Epoch: 968/1000 Train loss: 473.932281 Valid loss: 674.868408\n",
      "Epoch: 969/1000 Train loss: 473.443268 Valid loss: 674.172180\n",
      "Epoch: 970/1000 Train loss: 472.955261 Valid loss: 673.477417\n",
      "Epoch: 971/1000 Train loss: 472.468231 Valid loss: 672.784119\n",
      "Epoch: 972/1000 Train loss: 471.982208 Valid loss: 672.092224\n",
      "Epoch: 973/1000 Train loss: 471.497223 Valid loss: 671.401794\n",
      "Epoch: 974/1000 Train loss: 471.013184 Valid loss: 670.712708\n",
      "Epoch: 975/1000 Train loss: 470.530121 Valid loss: 670.025085\n",
      "Epoch: 976/1000 Train loss: 470.048065 Valid loss: 669.338867\n",
      "Epoch: 977/1000 Train loss: 469.566986 Valid loss: 668.653992\n",
      "Epoch: 978/1000 Train loss: 469.086945 Valid loss: 667.970581\n",
      "Epoch: 979/1000 Train loss: 468.607849 Valid loss: 667.288513\n",
      "Epoch: 980/1000 Train loss: 468.129700 Valid loss: 666.607849\n",
      "Epoch: 981/1000 Train loss: 467.652588 Valid loss: 665.928589\n",
      "Epoch: 982/1000 Train loss: 467.176422 Valid loss: 665.250671\n",
      "Epoch: 983/1000 Train loss: 466.701233 Valid loss: 664.574158\n",
      "Epoch: 984/1000 Train loss: 466.226990 Valid loss: 663.899048\n",
      "Epoch: 985/1000 Train loss: 465.753693 Valid loss: 663.225281\n",
      "Epoch: 986/1000 Train loss: 465.281433 Valid loss: 662.552795\n",
      "Epoch: 987/1000 Train loss: 464.810089 Valid loss: 661.881775\n",
      "Epoch: 988/1000 Train loss: 464.339722 Valid loss: 661.212097\n",
      "Epoch: 989/1000 Train loss: 463.870300 Valid loss: 660.543762\n",
      "Epoch: 990/1000 Train loss: 463.401764 Valid loss: 659.876831\n",
      "Epoch: 991/1000 Train loss: 462.934235 Valid loss: 659.211182\n",
      "Epoch: 992/1000 Train loss: 462.467651 Valid loss: 658.546936\n",
      "Epoch: 993/1000 Train loss: 462.002014 Valid loss: 657.883972\n",
      "Epoch: 994/1000 Train loss: 461.537292 Valid loss: 657.222351\n",
      "Epoch: 995/1000 Train loss: 461.073517 Valid loss: 656.562073\n",
      "Epoch: 996/1000 Train loss: 460.610687 Valid loss: 655.903137\n",
      "Epoch: 997/1000 Train loss: 460.148773 Valid loss: 655.245483\n",
      "Epoch: 998/1000 Train loss: 459.687744 Valid loss: 654.589233\n",
      "Epoch: 999/1000 Train loss: 459.227661 Valid loss: 653.934204\n",
      "Epoch: 1000/1000 Train loss: 458.768494 Valid loss: 653.280457\n"
     ]
    }
   ],
   "source": [
    "# Save the training result or trained and validated model params\n",
    "saver = tf.train.Saver()\n",
    "train_loss, valid_loss = [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # my assumption is the local variables are the parameters and hyperparameters\n",
    "    # the globala variables are the one needed/required by the seession/graph to run\n",
    "    sess.run(fetches=tf.global_variables_initializer())\n",
    "   \n",
    "    # Loop over epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Loop over batches\n",
    "        for X_train_norm_batch, _ in get_batches(X_train_norm, Y_train_onehot, batch_size):\n",
    "            \n",
    "            ######################## Training\n",
    "            # Feed dictionary\n",
    "            feed = {Xin : X_train_norm_batch, keep_prob_ : keep_prob, learning_rate_ : learning_rate}\n",
    "            \n",
    "            # Loss\n",
    "            loss, _ = sess.run(fetches=[cost, optimizer], feed_dict = feed)\n",
    "            train_loss.append(loss)\n",
    "\n",
    "            ################## Validation\n",
    "            loss_batch = []    \n",
    "            # Loop over batches\n",
    "            for X_valid_norm_batch, _ in get_batches(X_valid_norm, Y_valid_onehot, batch_size):\n",
    "\n",
    "                # Feed dictionary\n",
    "                feed = {Xin : X_valid_norm_batch, keep_prob_ : 1.0} \n",
    "                # no learning is needed therefore no learning rate is needed.\n",
    "\n",
    "                # Loss\n",
    "                loss = sess.run(fetches=[cost], feed_dict = feed)\n",
    "                # no learning is needed therefore no learning rate is needed.\n",
    "                # Therefore no optimization approach or backprop is needed either.\n",
    "                loss_batch.append(loss)\n",
    "\n",
    "            # Store\n",
    "            valid_loss.append(np.mean(loss_batch))\n",
    "            \n",
    "        # Print info for every iter/epoch\n",
    "        print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "              \"Train loss: {:6f}\".format(np.mean(train_loss)),\n",
    "              \"Valid loss: {:.6f}\".format(np.mean(valid_loss)))\n",
    "    \n",
    "    # At the end of training and validation\n",
    "    saver.save(sess,\"checkpoints/cnn-har-TEST.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as mplot\n",
    "%matplotlib inline\n",
    "\n",
    "mplot.plot(train_loss, label='har train_loss')\n",
    "mplot.plot(valid_loss, label='har valid_loss')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/cnn-har-TEST.ckpt\n",
      "Test loss: 0.605664\n"
     ]
    }
   ],
   "source": [
    "# initilize the saver which has already been initialized.\n",
    "test_loss = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore the validated model\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    ################## Test\n",
    "    loss_batch = []    \n",
    "    # Loop over batches\n",
    "    for X_test_norm_batch, _ in get_batches(X_test_norm, Y_test_onehot, batch_size):\n",
    "\n",
    "        # Feed dictionary\n",
    "        # No learning/training is needed at this step\n",
    "        feed = {Xin: X_test_norm_batch, keep_prob_ : 1.0}\n",
    "\n",
    "        # Loss\n",
    "        # Only the computation of the cost is needed\n",
    "        loss = sess.run(fetches=[cost], feed_dict = feed)\n",
    "        loss_batch.append(loss)\n",
    "\n",
    "    # Store\n",
    "    test_loss.append(np.mean(loss_batch))\n",
    "\n",
    "    # Print info for every iter/epoch\n",
    "    print(\"Test loss: {:6f}\".format(np.mean(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
