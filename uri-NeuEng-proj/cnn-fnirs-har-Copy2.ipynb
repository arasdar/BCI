{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAR (human activity recognition) using DNN/DL on fNIRS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mP11-4-17-2018\u001b[0m/  \u001b[01;34mP13-4-17-2018\u001b[0m/  \u001b[01;34mP15-4-18-2018\u001b[0m/  \u001b[01;34mP17-4-18-2018\u001b[0m/  \u001b[01;34mP19-4-19-2018\u001b[0m/\r\n",
      "\u001b[01;34mP12-4-17-2018\u001b[0m/  \u001b[01;34mP14-4-18-2018\u001b[0m/  \u001b[01;34mP16-4-18-2018\u001b[0m/  \u001b[01;34mP18-4-19-2018\u001b[0m/  \u001b[01;34mP20-4-19-2018\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "% ls /home/arasdar/datasets/fNIRs-data-10subjects/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m1. Right Hand\u001b[0m/  \u001b[01;34m2. Both Hands\u001b[0m/  \u001b[01;34m3. Left Hand\u001b[0m/  \u001b[01;34m4. Right Leg\u001b[0m/  \u001b[01;34m5. Left Leg\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "% ls /home/arasdar/datasets/fNIRs-data-10subjects/P12-4-17-2018/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m2018-04-17_006\u001b[0m/\r\n",
      "fNIR_data.txt\r\n",
      "head20180417-145130.txt\r\n",
      "NIRS-2018-04-17_006_deoxyhb_T141to2511_C1to20.txt\r\n",
      "NIRS-2018-04-17_006_oxyhb_T141to2511_C1to20.txt\r\n",
      "\u001b[01;34mProcessed\u001b[0m/\r\n",
      "r_hand20180417-145128.txt\r\n",
      "r_lower_arm20180417-145129.txt\r\n",
      "r_upper_arm20180417-145129.txt\r\n"
     ]
    }
   ],
   "source": [
    "% ls /home/arasdar/datasets/fNIRs-data-10subjects/P12-4-17-2018/1.\\ Right\\ Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# % find ../../datasets/fNIRs_data/ | grep fNIR_data # NOT WORKING!!\n",
    "def find_all(name, path):\n",
    "    result = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if name in files:\n",
    "            result.append(os.path.join(root, name))\n",
    "    return result\n",
    "\n",
    "allpaths = find_all(name='fNIR_data.txt', path='/home/arasdar/datasets/fNIRs-data-10subjects/')\n",
    "allpaths = sorted(allpaths, reverse=False)\n",
    "# print(allpaths, len(allpaths))\n",
    "# allpaths, len(allpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/pandas/io/parsers.py:678: UserWarning: Duplicate names specified. This will raise an error in the future.\n",
      "  return _read(filepath_or_buffer, kwds)\n"
     ]
    }
   ],
   "source": [
    "# df: data frame object\n",
    "df = []\n",
    "for each_idx in range(len(allpaths)):\n",
    "    file = pd.read_csv(filepath_or_buffer=allpaths[each_idx], names=['time', 'sample', \n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel'],\n",
    "                         header=None)\n",
    "    df.append(file)\n",
    "    \n",
    "for each in range(len(df)):\n",
    "#     print(df[each].shape, allpaths[each])\n",
    "    df[each]=df[each].drop(axis=1, columns=None, index=None, labels=['time', 'sample'])\n",
    "    df[each] = df[each].dropna()\n",
    "    df[each]['channel.39'] = df[each]['channel.39'].astype(str).str[1:-1].astype(float)\n",
    "# print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(48, 48)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, labels = [], []\n",
    "for each in range(0, len(df), 1):\n",
    "    dfmat = df[each].as_matrix()\n",
    "    label = allpaths[each][59:60]\n",
    "#     print(dfmat.dtype, dfmat.shape, label, allpaths[each])\n",
    "    data.append(dfmat)\n",
    "    labels.append(label)\n",
    "len(data), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is very much like a convolution for extracting the windows\n",
    "# size/width, stride/overlap, padding, dilation, num filters/out channel\n",
    "def minibatching(X, Y, stride, width):\n",
    "    Xmb, Ymb = [], []\n",
    "    print(len(X), len(Y))\n",
    "    # 1st and 1st\n",
    "    for eachX in range(len(X)):\n",
    "        num_mb = ((X[eachX].shape[0]-width)//stride)+1\n",
    "        for each in range(num_mb):\n",
    "            # The max is (num_mb-1)*stride+width==X[idx].shape[0]\n",
    "            # The last each is (num_mb-1)\n",
    "            # each = ((each-1)*stride)+width\n",
    "            each *= stride\n",
    "            Xmb.append(X[eachX][each:each+width])\n",
    "            # There is only one label for one image signal or signal window or temporal window\n",
    "            #Ymb.append(Y[eachX][each:each+1])\n",
    "            Ymb.append(Y[eachX])\n",
    "    return Xmb, Ymb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48\n",
      "69615 69615\n",
      "(250, 40) float64\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Width is based on the sampling rate which is roughly about 233 points per window\n",
    "# for 10sec rest and 20 sec activity\n",
    "width = 250\n",
    "Xmb, Ymb = minibatching(X=data, Y=labels, stride=1, width=width)\n",
    "# for eachX, eachY in zip(Xmb, Ymb):\n",
    "#     print(eachX.shape, eachY)\n",
    "print(len(Xmb), len(Ymb))\n",
    "print(Xmb[0].shape, Xmb[0].dtype)\n",
    "print(Ymb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69615, 250, 40) float64 (69615,) int64\n"
     ]
    }
   ],
   "source": [
    "# Conversion from python list to numpy array\n",
    "X, Y=np.array(object=Xmb, dtype=float), np.array(object=Ymb, dtype=int)\n",
    "print(X.shape, X.dtype, Y.shape, Y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48730, 250, 40) (20885, 250, 40) (48730,) (20885,)\n",
      "float64 float64 int64 int64\n"
     ]
    }
   ],
   "source": [
    "# Now I should devide the data into train and test\n",
    "# Train and valid split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 30% of the training data/ entire training data is assigned to validation.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.30)\n",
    "print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)\n",
    "print(Xtrain.dtype, Xtest.dtype, Ytrain.dtype, Ytest.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # standardizing/normalizing the train and test data\n",
    "# # def standardize(train, test):\n",
    "# # \"\"\" Standardize data \"\"\"\n",
    "# # # Standardize train and test\n",
    "# # X_train = (train - np.mean(train, axis=0)[None,:,:]) / np.std(train, axis=0)[None,:,:]\n",
    "# # X_test = (test - np.mean(test, axis=0)[None,:,:]) / np.std(test, axis=0)[None,:,:]\n",
    "# # return X_train, X_test\n",
    "\n",
    "# Xtrain = (Xtrain - Xtrain.mean(axis=0))/ Xtrain.std(axis=0)\n",
    "# Xtest = (Xtest - Xtest.mean(axis=0))/ Xtest.std(axis=0)\n",
    "# print(Xtrain.shape, Xtrain.dtype)\n",
    "# print(Xtest.shape, Xtest.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.23554403 0.20575593 0.18495657 ... 0.46111169 0.52762557 0.49465244]\n",
      " [0.23556481 0.20576707 0.184952   ... 0.46112577 0.52764148 0.49466631]\n",
      " [0.23560505 0.20574727 0.18494023 ... 0.46114088 0.52765266 0.49468089]\n",
      " ...\n",
      " [0.23552294 0.20630135 0.18543658 ... 0.46198475 0.52880979 0.49748372]\n",
      " [0.23552992 0.20628603 0.18538877 ... 0.46200798 0.52884831 0.49750334]\n",
      " [0.23552453 0.2063183  0.1854071  ... 0.46201101 0.52883827 0.49752433]] [[0.19285766 0.16627797 0.22238958 ... 0.42500892 0.32865339 0.39384347]\n",
      " [0.19285166 0.16628782 0.22234756 ... 0.42500321 0.32866523 0.3938663 ]\n",
      " [0.19287132 0.16628347 0.22240013 ... 0.42503077 0.32869325 0.39389741]\n",
      " ...\n",
      " [0.19349291 0.16647666 0.22302495 ... 0.42435421 0.33020757 0.39728052]\n",
      " [0.19350172 0.16645575 0.22301906 ... 0.42437603 0.33022773 0.39730335]\n",
      " [0.19351622 0.16648958 0.22311172 ... 0.42442223 0.33022996 0.39732096]]\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain.mean(axis=0), Xtrain.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.23765635 0.20597665 0.18336603 ... 0.45654725 0.52858353 0.49686509]\n",
      " [0.23760658 0.20595644 0.18338596 ... 0.45652995 0.52855008 0.49686592]\n",
      " [0.23750989 0.20600986 0.18342205 ... 0.45651175 0.52852138 0.49686913]\n",
      " ...\n",
      " [0.237542   0.20650866 0.18383809 ... 0.45759895 0.52984235 0.49954631]\n",
      " [0.23752772 0.20654956 0.1839596  ... 0.45754887 0.52976889 0.49950552]\n",
      " [0.23754784 0.20648215 0.18392217 ... 0.45755178 0.52980125 0.49946098]] [[0.19329449 0.16668512 0.21998131 ... 0.42141658 0.3307093  0.39560504]\n",
      " [0.19331477 0.16665267 0.22009097 ... 0.4214354  0.33068433 0.39561556]\n",
      " [0.1932717  0.16665732 0.21997587 ... 0.42136722 0.33061238 0.39560948]\n",
      " ...\n",
      " [0.19404805 0.16667976 0.22077867 ... 0.42120564 0.33241725 0.39893713]\n",
      " [0.19402672 0.16673311 0.2207919  ... 0.4211549  0.33240221 0.39889508]\n",
      " [0.19400099 0.16666572 0.2205828  ... 0.42105012 0.33242536 0.3988653 ]]\n"
     ]
    }
   ],
   "source": [
    "print(Xtest.mean(axis=0), Xtest.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34111, 250, 40) (14619, 250, 40) (20885, 250, 40) float64 float64 float64\n",
      "(34111,) (14619,) (20885,) int64 int64 int64\n"
     ]
    }
   ],
   "source": [
    "# Now separating train and validation set\n",
    "# 30% of the training data/ entire training data is assigned to validation.\n",
    "Xtrain, Xvalid, Ytrain, Yvalid = train_test_split(Xtrain, Ytrain, test_size=0.30)\n",
    "print(Xtrain.shape, Xvalid.shape, Xtest.shape, Xtrain.dtype, Xvalid.dtype, Xtest.dtype)\n",
    "print(Ytrain.shape, Yvalid.shape, Ytest.shape, Ytrain.dtype, Yvalid.dtype, Ytest.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.8.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(input_size, output_size):\n",
    "    #     N, W, Cin = Xvalid.shape[0], Xvalid.shape[1], Xvalid.shape[2]\n",
    "    Xinputs = tf.placeholder(dtype=tf.float32, shape=[None, *input_size], name='Xinputs')\n",
    "    \n",
    "    #     N, Cout = Yvalid.shape[0], Yvalid.shape[1]\n",
    "    Yindices = tf.placeholder(dtype=tf.int32, shape=[None], name='Yindices')\n",
    "    \n",
    "    # # Batchnorm mode: training and inference/testing/validation\n",
    "    # #is_bn_training = tf.placeholder(dtype=tf.bool, shape=[], name='is_bn_training')\n",
    "    # training = tf.placeholder(dtype=tf.bool, shape=[], name='training')\n",
    "\n",
    "    # returning input data/sequences, output labels/classes\n",
    "    return Xinputs, Yindices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.layers.conv2d(\n",
    "#     inputs,\n",
    "#     filters,\n",
    "#     kernel_size, kHxkW\n",
    "#     strides=(1, 1), sHxsW\n",
    "#     padding='valid',\n",
    "#     data_format='channels_last', NxHxWxC\n",
    "#     data_format='channels_first', NxCxHxW\n",
    "#     dilation_rate=(1, 1),\n",
    "#     activation=None,\n",
    "#     use_bias=True,\n",
    "#     kernel_initializer=None,\n",
    "#     bias_initializer=tf.zeros_initializer(),\n",
    "#     kernel_regularizer=None,\n",
    "#     bias_regularizer=None,\n",
    "#     activity_regularizer=None,\n",
    "#     kernel_constraint=None,\n",
    "#     bias_constraint=None,\n",
    "#     trainable=True,\n",
    "#     name=None,\n",
    "#     reuse=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.layers.conv2d(\n",
    "    inputs,\n",
    "    filters,\n",
    "    kernel_size=(0, 3), #kHxkW= [] list/tuple; can be a single integer too 3 which means 3x3\n",
    "    strides=(0, 2), #sHxsW; default=(1, 1); for reducing dim and getting rid of pooling\n",
    "    padding='valid', # or 'same'\n",
    "    data_format='channels_last', #NxHxWxC default\n",
    "#     data_format='channels_first', #NxCxHxW\n",
    "    dilation_rate=(0, 1), #dHxdW= (1, 1) default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/ classifier/ recognizer\n",
    "def discriminator(Xinputs, input_size, output_size, hidden_size, reuse=False, alpha=0.1, training=True):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        \n",
    "        # Flatten/Vectorize the input data tensor for FC/fully connected layer/Dense Layer\n",
    "        Xinputs_vec = tf.reshape(tensor=Xinputs, shape=[-1, input_size[0]*input_size[1]])\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=Xinputs_vec, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)\n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)\n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=output_size)   \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # return output logits for loss and accuracy\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Discriminator/ classifier/ recognizer\n",
    "# def discriminator(Xinputs, input_size, output_size, hidden_size, reuse=False, alpha=0.1, training=True):\n",
    "#     with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        \n",
    "#         # Flatten/Vectorize the input data tensor for FC/fully connected layer/Dense Layer\n",
    "#         Xinputs_vec = tf.reshape(tensor=Xinputs, shape=[-1, input_size[0]*input_size[1]])\n",
    "        \n",
    "#         # First fully connected layer\n",
    "#         h1 = tf.layers.dense(inputs=Xinputs_vec, units=hidden_size)\n",
    "#         nl1 = tf.maximum(alpha * h1, h1)\n",
    "        \n",
    "#         # Second fully connected layer\n",
    "#         h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "#         nl2 = tf.maximum(alpha * h2, h2)\n",
    "        \n",
    "#         # Output layer\n",
    "#         logits = tf.layers.dense(inputs=nl2, units=output_size)   \n",
    "#         #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "#         # return output logits for loss and accuracy\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the forward propagation of the model to calculate the loss.\n",
    "def model_loss(Xinputs, Yindices, input_size, output_size, hidden_size):\n",
    "    \n",
    "    # Creating logits and labels\n",
    "    Ylogits = discriminator(Xinputs=Xinputs, input_size=input_size, output_size=output_size, \n",
    "                            hidden_size=hidden_size)\n",
    "    Ylabels = tf.one_hot(indices=Yindices, depth=output_size, dtype=Ylogits.dtype)\n",
    "    \n",
    "    # Loss using logits and labels\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Ylogits, labels=Ylabels))\n",
    "    \n",
    "    # Accuracy using logits and labels\n",
    "    acc_tensor = tf.equal(x=tf.argmax(axis=1, input=Ylogits), y=tf.argmax(axis=1, input=Ylabels))\n",
    "    acc = tf.reduce_mean(axis=0, input_tensor=tf.cast(dtype=tf.float32, x=acc_tensor))\n",
    "\n",
    "    # returning loss and accuracy\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_opt(loss, learning_rate):\n",
    "#     \"\"\"\n",
    "#     Get optimization operations in order\n",
    "#     :param loss: Discriminator/classifier loss Tensor\n",
    "#     :param learning_rate: Learning Rate Placeholder\n",
    "#     :return: A tuple of (discriminator training)\n",
    "#     \"\"\"\n",
    "#     # Get weights and bias to update\n",
    "#     t_vars = tf.trainable_variables()\n",
    "#     # q_vars = [var for var in t_vars if var.name.startswith('qfunction')] # Q: action At/at\n",
    "#     # g_vars = [var for var in t_vars if var.name.startswith('generator')] # G: next state St/st\n",
    "#     # d_vars = [var for var in t_vars if var.name.startswith('discriminator')] # D: reward Rt/rt\n",
    "#     var_list = [var for var in t_vars if var.name.startswith('discriminator')] # D: reward Rt/rt\n",
    "    \n",
    "#     # Optimize\n",
    "#     with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "#         # q_opt = tf.train.AdamOptimizer(learning_rate).minimize(q_loss, var_list=q_vars)\n",
    "#         # g_opt = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "#         # d_opt = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "#         opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=var_list)\n",
    "\n",
    "#     return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, output_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.Xinputs, self.Yindices = model_input(input_size=input_size, output_size=output_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.loss, self.acc = model_loss(Xinputs=self.Xinputs, Yindices=self.Yindices,\n",
    "                                         input_size=input_size, output_size=output_size, hidden_size=hidden_size)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        #self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)\n",
    "        self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(X, Y, batch_size):\n",
    "    \"\"\" Return a generator for batches \"\"\"\n",
    "    n_batches = len(X) // batch_size\n",
    "    X, Y = X[:n_batches*batch_size], Y[:n_batches*batch_size]\n",
    "\n",
    "    # Loop over batches and yield\n",
    "    for b in range(0, len(X), batch_size):\n",
    "        yield X[b:b+batch_size], Y[b:b+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Hyper-parameters\n",
    "\n",
    "# # Input\n",
    "# Xwidth, Xchannels = Xvalid.shape[1], Xvalid.shape[2]\n",
    "\n",
    "# Output layer\n",
    "assert Ytrain.max()==Ytest.max()==Yvalid.max(), 'Output classes'\n",
    "# Ychannels = Yvalid.max()\n",
    "\n",
    "# Hidden layer\n",
    "input_size = [Xvalid.shape[1], Xvalid.shape[2]]\n",
    "hidden_size = Xvalid.shape[1]* Xvalid.shape[2]\n",
    "output_size = Yvalid.max()\n",
    "\n",
    "# learning parameters\n",
    "batch_size = Xvalid.shape[0]//1 # experience mini-batch size\n",
    "train_epochs = 1000              # max number of training episodes/epochs\n",
    "learning_rate = 0.001            # learning rate for training/optimization/adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yvalid.max(), Yvalid.min(): 5 1\n",
      "Yvalid.shape: (14619,)\n",
      "Xvalid.shape: (14619, 250, 40)\n"
     ]
    }
   ],
   "source": [
    "print('Yvalid.max(), Yvalid.min():', Yvalid.max(), Yvalid.min())\n",
    "print('Yvalid.shape:', Yvalid.shape)\n",
    "print('Xvalid.shape:', Xvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = MLP(input_size=input_size, hidden_size=hidden_size, output_size=output_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 train_loss: 6.7945423 train_acc: 0.20760652\n",
      "epoch: 1 valid_loss: 13.018539 valid_acc: 0.23531021\n",
      "epoch: 2 train_loss: 11.511367 train_acc: 0.27060676\n",
      "epoch: 2 valid_loss: 7.2873964 valid_acc: 0.31821603\n",
      "epoch: 3 train_loss: 6.666421 train_acc: 0.3046378\n",
      "epoch: 3 valid_loss: 6.0155897 valid_acc: 0.34167865\n",
      "epoch: 4 train_loss: 5.5748844 train_acc: 0.32813463\n",
      "epoch: 4 valid_loss: 4.47367 valid_acc: 0.41965935\n",
      "epoch: 5 train_loss: 4.048591 train_acc: 0.39106643\n",
      "epoch: 5 valid_loss: 3.5599895 valid_acc: 0.3676038\n",
      "epoch: 6 train_loss: 3.143035 train_acc: 0.4101854\n",
      "epoch: 6 valid_loss: 2.8266127 valid_acc: 0.3825843\n",
      "epoch: 7 train_loss: 2.523176 train_acc: 0.36681718\n",
      "epoch: 7 valid_loss: 1.9985411 valid_acc: 0.40652576\n",
      "epoch: 8 train_loss: 1.8668896 train_acc: 0.3531705\n",
      "epoch: 8 valid_loss: 1.8317511 valid_acc: 0.36527807\n",
      "epoch: 9 train_loss: 1.7085202 train_acc: 0.3826185\n",
      "epoch: 9 valid_loss: 1.4748303 valid_acc: 0.417402\n",
      "epoch: 10 train_loss: 1.3289514 train_acc: 0.46118066\n",
      "epoch: 10 valid_loss: 1.1567299 valid_acc: 0.47089404\n",
      "epoch: 11 train_loss: 1.0586903 train_acc: 0.5087899\n",
      "epoch: 11 valid_loss: 0.88990486 valid_acc: 0.53403103\n",
      "epoch: 12 train_loss: 0.8928902 train_acc: 0.53806686\n",
      "epoch: 12 valid_loss: 0.96397346 valid_acc: 0.5315685\n",
      "epoch: 13 train_loss: 0.9625436 train_acc: 0.52072644\n",
      "epoch: 13 valid_loss: 0.9853146 valid_acc: 0.5009919\n",
      "epoch: 14 train_loss: 0.9552516 train_acc: 0.4982215\n",
      "epoch: 14 valid_loss: 0.8449948 valid_acc: 0.58738625\n",
      "epoch: 15 train_loss: 0.82712513 train_acc: 0.599357\n",
      "epoch: 15 valid_loss: 0.8100676 valid_acc: 0.60394007\n",
      "epoch: 16 train_loss: 0.7833499 train_acc: 0.59508175\n",
      "epoch: 16 valid_loss: 0.7127161 valid_acc: 0.64689785\n",
      "epoch: 17 train_loss: 0.71414906 train_acc: 0.6273343\n",
      "epoch: 17 valid_loss: 0.7343306 valid_acc: 0.55058485\n",
      "epoch: 18 train_loss: 0.7069471 train_acc: 0.5488063\n",
      "epoch: 18 valid_loss: 0.69157207 valid_acc: 0.543334\n",
      "epoch: 19 train_loss: 0.6745201 train_acc: 0.56977224\n",
      "epoch: 19 valid_loss: 0.67881393 valid_acc: 0.5811615\n",
      "epoch: 20 train_loss: 0.6577003 train_acc: 0.5896778\n",
      "epoch: 20 valid_loss: 0.62207586 valid_acc: 0.6275395\n",
      "epoch: 21 train_loss: 0.6071033 train_acc: 0.6288392\n",
      "epoch: 21 valid_loss: 0.59391665 valid_acc: 0.61009645\n",
      "epoch: 22 train_loss: 0.57372713 train_acc: 0.63728714\n",
      "epoch: 22 valid_loss: 0.5711249 valid_acc: 0.6705657\n",
      "epoch: 23 train_loss: 0.55613303 train_acc: 0.67408854\n",
      "epoch: 23 valid_loss: 0.5433045 valid_acc: 0.6576373\n",
      "epoch: 24 train_loss: 0.5346371 train_acc: 0.6622204\n",
      "epoch: 24 valid_loss: 0.5480854 valid_acc: 0.6529858\n",
      "epoch: 25 train_loss: 0.5380626 train_acc: 0.66006565\n",
      "epoch: 25 valid_loss: 0.5319343 valid_acc: 0.6533279\n",
      "epoch: 26 train_loss: 0.5187261 train_acc: 0.67668784\n",
      "epoch: 26 valid_loss: 0.5145351 valid_acc: 0.65955263\n",
      "epoch: 27 train_loss: 0.4997432 train_acc: 0.6693344\n",
      "epoch: 27 valid_loss: 0.4894013 valid_acc: 0.66338325\n",
      "epoch: 28 train_loss: 0.47330767 train_acc: 0.67624325\n",
      "epoch: 28 valid_loss: 0.4743069 valid_acc: 0.6675559\n",
      "epoch: 29 train_loss: 0.46389896 train_acc: 0.6852042\n",
      "epoch: 29 valid_loss: 0.47345066 valid_acc: 0.6806895\n",
      "epoch: 30 train_loss: 0.46135637 train_acc: 0.6939941\n",
      "epoch: 30 valid_loss: 0.46496066 valid_acc: 0.67891103\n",
      "epoch: 31 train_loss: 0.4528038 train_acc: 0.6868801\n",
      "epoch: 31 valid_loss: 0.46466747 valid_acc: 0.6708393\n",
      "epoch: 32 train_loss: 0.44971865 train_acc: 0.67969763\n",
      "epoch: 32 valid_loss: 0.45645556 valid_acc: 0.6718654\n",
      "epoch: 33 train_loss: 0.44149962 train_acc: 0.68773514\n",
      "epoch: 33 valid_loss: 0.45063883 valid_acc: 0.68157876\n",
      "epoch: 34 train_loss: 0.43778887 train_acc: 0.7016896\n",
      "epoch: 34 valid_loss: 0.44126138 valid_acc: 0.6986114\n",
      "epoch: 35 train_loss: 0.4269064 train_acc: 0.7121896\n",
      "epoch: 35 valid_loss: 0.42989925 valid_acc: 0.6925234\n",
      "epoch: 36 train_loss: 0.41827825 train_acc: 0.6999453\n",
      "epoch: 36 valid_loss: 0.4272319 valid_acc: 0.6869827\n",
      "epoch: 37 train_loss: 0.4165516 train_acc: 0.6956358\n",
      "epoch: 37 valid_loss: 0.4310426 valid_acc: 0.69573843\n",
      "epoch: 38 train_loss: 0.42274088 train_acc: 0.70637524\n",
      "epoch: 38 valid_loss: 0.43673226 valid_acc: 0.7072303\n",
      "epoch: 39 train_loss: 0.42600167 train_acc: 0.7118476\n",
      "epoch: 39 valid_loss: 0.4308612 valid_acc: 0.70223683\n",
      "epoch: 40 train_loss: 0.41991073 train_acc: 0.7089746\n",
      "epoch: 40 valid_loss: 0.42309645 valid_acc: 0.70292085\n",
      "epoch: 41 train_loss: 0.41169193 train_acc: 0.70928246\n",
      "epoch: 41 valid_loss: 0.42169997 valid_acc: 0.7041521\n",
      "epoch: 42 train_loss: 0.41443086 train_acc: 0.7113688\n",
      "epoch: 42 valid_loss: 0.4232053 valid_acc: 0.7038785\n",
      "epoch: 43 train_loss: 0.41522485 train_acc: 0.71441275\n",
      "epoch: 43 valid_loss: 0.41153795 valid_acc: 0.7094877\n",
      "epoch: 44 train_loss: 0.39582062 train_acc: 0.72180045\n",
      "epoch: 44 valid_loss: 0.39687365 valid_acc: 0.70593065\n",
      "epoch: 45 train_loss: 0.38918173 train_acc: 0.71793556\n",
      "epoch: 45 valid_loss: 0.39921045 valid_acc: 0.7023052\n",
      "epoch: 46 train_loss: 0.3916396 train_acc: 0.72439975\n",
      "epoch: 46 valid_loss: 0.38812926 valid_acc: 0.7205007\n",
      "epoch: 47 train_loss: 0.3955762 train_acc: 0.7289828\n",
      "epoch: 47 valid_loss: 0.44172412 valid_acc: 0.69279706\n",
      "epoch: 48 train_loss: 0.40379226 train_acc: 0.7168411\n",
      "epoch: 48 valid_loss: 0.40379092 valid_acc: 0.72706753\n",
      "epoch: 49 train_loss: 0.39764822 train_acc: 0.7316506\n",
      "epoch: 49 valid_loss: 0.37820953 valid_acc: 0.7386962\n",
      "epoch: 50 train_loss: 0.37909317 train_acc: 0.7419112\n",
      "epoch: 50 valid_loss: 0.44050357 valid_acc: 0.6755592\n",
      "epoch: 51 train_loss: 0.42902523 train_acc: 0.7005267\n",
      "epoch: 51 valid_loss: 0.4070079 valid_acc: 0.7008003\n",
      "epoch: 52 train_loss: 0.3828627 train_acc: 0.7071619\n",
      "epoch: 52 valid_loss: 0.3691729 valid_acc: 0.7337027\n",
      "epoch: 53 train_loss: 0.38114372 train_acc: 0.7202271\n",
      "epoch: 53 valid_loss: 0.472698 valid_acc: 0.6731651\n",
      "epoch: 54 train_loss: 0.49114692 train_acc: 0.6771325\n",
      "epoch: 54 valid_loss: 0.35561153 valid_acc: 0.7166701\n",
      "epoch: 55 train_loss: 0.3875115 train_acc: 0.71923524\n",
      "epoch: 55 valid_loss: 0.5294942 valid_acc: 0.66126275\n",
      "epoch: 56 train_loss: 0.4863301 train_acc: 0.6731993\n",
      "epoch: 56 valid_loss: 0.5479574 valid_acc: 0.6281551\n",
      "epoch: 57 train_loss: 0.5041062 train_acc: 0.6517204\n",
      "epoch: 57 valid_loss: 0.4356799 valid_acc: 0.7003899\n",
      "epoch: 58 train_loss: 0.49120075 train_acc: 0.72087693\n",
      "epoch: 58 valid_loss: 0.46888262 valid_acc: 0.71119773\n",
      "epoch: 59 train_loss: 0.42854404 train_acc: 0.7076749\n",
      "epoch: 59 valid_loss: 0.46426168 valid_acc: 0.68137354\n",
      "epoch: 60 train_loss: 0.44164073 train_acc: 0.69156575\n",
      "epoch: 60 valid_loss: 0.54750496 valid_acc: 0.6494972\n",
      "epoch: 61 train_loss: 0.6231438 train_acc: 0.65777415\n",
      "epoch: 61 valid_loss: 0.40050858 valid_acc: 0.70572543\n",
      "epoch: 62 train_loss: 0.51673925 train_acc: 0.64758193\n",
      "epoch: 62 valid_loss: 0.7784843 valid_acc: 0.61830497\n",
      "epoch: 63 train_loss: 0.6437199 train_acc: 0.67969763\n",
      "epoch: 63 valid_loss: 0.7582304 valid_acc: 0.62528217\n",
      "epoch: 64 train_loss: 0.76805586 train_acc: 0.64607704\n",
      "epoch: 64 valid_loss: 0.70834833 valid_acc: 0.6780217\n",
      "epoch: 65 train_loss: 0.80222154 train_acc: 0.6511731\n",
      "epoch: 65 valid_loss: 0.4979865 valid_acc: 0.7040153\n",
      "epoch: 66 train_loss: 0.51506925 train_acc: 0.6786374\n",
      "epoch: 66 valid_loss: 0.6087523 valid_acc: 0.618989\n",
      "epoch: 67 train_loss: 0.6113715 train_acc: 0.6333538\n",
      "epoch: 67 valid_loss: 0.5254676 valid_acc: 0.65510637\n",
      "epoch: 68 train_loss: 0.61222506 train_acc: 0.6331829\n",
      "epoch: 68 valid_loss: 0.7311011 valid_acc: 0.6675559\n",
      "epoch: 69 train_loss: 0.7897919 train_acc: 0.64946306\n",
      "epoch: 69 valid_loss: 0.8639199 valid_acc: 0.62391406\n",
      "epoch: 70 train_loss: 0.6626594 train_acc: 0.6596894\n",
      "epoch: 70 valid_loss: 0.7882128 valid_acc: 0.70613587\n",
      "epoch: 71 train_loss: 0.77327365 train_acc: 0.6768931\n",
      "epoch: 71 valid_loss: 0.9023139 valid_acc: 0.6423832\n",
      "epoch: 72 train_loss: 0.91565746 train_acc: 0.6418018\n",
      "epoch: 72 valid_loss: 1.3752049 valid_acc: 0.6238457\n",
      "epoch: 73 train_loss: 1.3213 train_acc: 0.6124222\n",
      "epoch: 73 valid_loss: 1.1636024 valid_acc: 0.5808195\n",
      "epoch: 74 train_loss: 1.0725472 train_acc: 0.5981941\n",
      "epoch: 74 valid_loss: 0.75895065 valid_acc: 0.62877077\n",
      "epoch: 75 train_loss: 0.7478791 train_acc: 0.64819753\n",
      "epoch: 75 valid_loss: 0.6106789 valid_acc: 0.6840413\n",
      "epoch: 76 train_loss: 0.68998194 train_acc: 0.6588344\n",
      "epoch: 76 valid_loss: 0.5542261 valid_acc: 0.6923866\n",
      "epoch: 77 train_loss: 0.67138743 train_acc: 0.6908475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 77 valid_loss: 0.75442064 valid_acc: 0.69286543\n",
      "epoch: 78 train_loss: 0.9204589 train_acc: 0.6736781\n",
      "epoch: 78 valid_loss: 0.86517453 valid_acc: 0.62877077\n",
      "epoch: 79 train_loss: 0.7828111 train_acc: 0.635748\n",
      "epoch: 79 valid_loss: 1.2191778 valid_acc: 0.6061974\n",
      "epoch: 80 train_loss: 0.8372953 train_acc: 0.65137833\n",
      "epoch: 80 valid_loss: 0.5044117 valid_acc: 0.68575144\n",
      "epoch: 81 train_loss: 0.5100134 train_acc: 0.6741569\n",
      "epoch: 81 valid_loss: 0.7100329 valid_acc: 0.6439565\n",
      "epoch: 82 train_loss: 0.8458938 train_acc: 0.6257952\n",
      "epoch: 82 valid_loss: 0.9507564 valid_acc: 0.6851358\n",
      "epoch: 83 train_loss: 0.81695604 train_acc: 0.6787058\n",
      "epoch: 83 valid_loss: 0.872762 valid_acc: 0.62644506\n",
      "epoch: 84 train_loss: 0.7108462 train_acc: 0.64518774\n",
      "epoch: 84 valid_loss: 0.90447736 valid_acc: 0.632533\n",
      "epoch: 85 train_loss: 0.7749181 train_acc: 0.66040766\n",
      "epoch: 85 valid_loss: 1.056708 valid_acc: 0.628634\n",
      "epoch: 86 train_loss: 0.9066402 train_acc: 0.64857376\n",
      "epoch: 86 valid_loss: 0.6635743 valid_acc: 0.6445037\n",
      "epoch: 87 train_loss: 0.7212106 train_acc: 0.67610645\n",
      "epoch: 87 valid_loss: 0.979405 valid_acc: 0.6693344\n",
      "epoch: 88 train_loss: 0.74485713 train_acc: 0.68520415\n",
      "epoch: 88 valid_loss: 0.6844907 valid_acc: 0.6726862\n",
      "epoch: 89 train_loss: 0.8140902 train_acc: 0.6424858\n",
      "epoch: 89 valid_loss: 0.8343311 valid_acc: 0.6544907\n",
      "epoch: 90 train_loss: 0.80355096 train_acc: 0.6469321\n",
      "epoch: 90 valid_loss: 0.5103265 valid_acc: 0.72946167\n",
      "epoch: 91 train_loss: 0.58618593 train_acc: 0.6966277\n",
      "epoch: 91 valid_loss: 0.65060174 valid_acc: 0.7003899\n",
      "epoch: 92 train_loss: 0.64211 train_acc: 0.67853475\n",
      "epoch: 92 valid_loss: 0.44722104 valid_acc: 0.734934\n",
      "epoch: 93 train_loss: 0.49573928 train_acc: 0.7197825\n",
      "epoch: 93 valid_loss: 0.4184151 valid_acc: 0.72556263\n",
      "epoch: 94 train_loss: 0.43237156 train_acc: 0.7381148\n",
      "epoch: 94 valid_loss: 0.5479393 valid_acc: 0.6646829\n",
      "epoch: 95 train_loss: 0.48288658 train_acc: 0.6844859\n",
      "epoch: 95 valid_loss: 0.5221114 valid_acc: 0.69286543\n",
      "epoch: 96 train_loss: 0.51437485 train_acc: 0.6834941\n",
      "epoch: 96 valid_loss: 0.75946295 valid_acc: 0.6685136\n",
      "epoch: 97 train_loss: 0.66663146 train_acc: 0.69307065\n",
      "epoch: 97 valid_loss: 0.6189835 valid_acc: 0.6673507\n",
      "epoch: 98 train_loss: 0.56161124 train_acc: 0.6878377\n",
      "epoch: 98 valid_loss: 0.5265611 valid_acc: 0.70394695\n",
      "epoch: 99 train_loss: 0.4694619 train_acc: 0.7123606\n",
      "epoch: 99 valid_loss: 0.41454425 valid_acc: 0.74492097\n",
      "epoch: 100 train_loss: 0.4057561 train_acc: 0.75993574\n",
      "epoch: 100 valid_loss: 0.5258443 valid_acc: 0.7090088\n",
      "epoch: 101 train_loss: 0.63342655 train_acc: 0.6952596\n",
      "epoch: 101 valid_loss: 0.42466608 valid_acc: 0.7482044\n",
      "epoch: 102 train_loss: 0.45389283 train_acc: 0.7524797\n",
      "epoch: 102 valid_loss: 0.51529765 valid_acc: 0.7384226\n",
      "epoch: 103 train_loss: 0.50824404 train_acc: 0.73024833\n",
      "epoch: 103 valid_loss: 0.7863151 valid_acc: 0.79355633\n",
      "epoch: 104 train_loss: 1.0985515 train_acc: 0.72922224\n",
      "epoch: 104 valid_loss: 0.74961066 valid_acc: 0.7003215\n",
      "epoch: 105 train_loss: 0.6980823 train_acc: 0.69286543\n",
      "epoch: 105 valid_loss: 0.7226234 valid_acc: 0.71974826\n",
      "epoch: 106 train_loss: 0.5853639 train_acc: 0.70210004\n",
      "epoch: 106 valid_loss: 0.64859235 valid_acc: 0.6800055\n",
      "epoch: 107 train_loss: 0.6446954 train_acc: 0.67726934\n",
      "epoch: 107 valid_loss: 0.50636715 valid_acc: 0.6772009\n",
      "epoch: 108 train_loss: 0.5117611 train_acc: 0.69943225\n",
      "epoch: 108 valid_loss: 0.59836555 valid_acc: 0.73103493\n",
      "epoch: 109 train_loss: 0.50976694 train_acc: 0.7475203\n",
      "epoch: 109 valid_loss: 0.52705884 valid_acc: 0.71304464\n",
      "epoch: 110 train_loss: 0.60315716 train_acc: 0.69974005\n",
      "epoch: 110 valid_loss: 0.4942276 valid_acc: 0.70640945\n",
      "epoch: 111 train_loss: 0.5332792 train_acc: 0.6953964\n",
      "epoch: 111 valid_loss: 0.9312037 valid_acc: 0.64539295\n",
      "epoch: 112 train_loss: 0.8857273 train_acc: 0.6525412\n",
      "epoch: 112 valid_loss: 0.6420178 valid_acc: 0.70770913\n",
      "epoch: 113 train_loss: 0.66094863 train_acc: 0.72009027\n",
      "epoch: 113 valid_loss: 0.5450221 valid_acc: 0.752035\n",
      "epoch: 114 train_loss: 0.553269 train_acc: 0.7560024\n",
      "epoch: 114 valid_loss: 0.6152185 valid_acc: 0.6896505\n",
      "epoch: 115 train_loss: 0.61189383 train_acc: 0.7078118\n",
      "epoch: 115 valid_loss: 0.7075662 valid_acc: 0.64977086\n",
      "epoch: 116 train_loss: 0.6041552 train_acc: 0.68876123\n",
      "epoch: 116 valid_loss: 0.6345951 valid_acc: 0.7601751\n",
      "epoch: 117 train_loss: 0.6489333 train_acc: 0.72405773\n",
      "epoch: 117 valid_loss: 0.5139403 valid_acc: 0.7244682\n",
      "epoch: 118 train_loss: 0.4772193 train_acc: 0.70705926\n",
      "epoch: 118 valid_loss: 0.62380135 valid_acc: 0.71605444\n",
      "epoch: 119 train_loss: 0.51273566 train_acc: 0.7281278\n",
      "epoch: 119 valid_loss: 0.743298 valid_acc: 0.73308706\n",
      "epoch: 120 train_loss: 0.6250879 train_acc: 0.7368835\n",
      "epoch: 120 valid_loss: 0.84052044 valid_acc: 0.67781657\n"
     ]
    }
   ],
   "source": [
    "# We should save the after training and validation\n",
    "saver = tf.train.Saver() \n",
    "\n",
    "# Loss and accuracy of the model for training and validation\n",
    "train_loss_mean, valid_loss_mean = [], []\n",
    "train_acc_mean, valid_acc_mean = [], []\n",
    "\n",
    "# now that we can calculate loss and optimize, we can start a session for calculating the error.\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize all the model parameters/variables\n",
    "    sess.run(fetches=tf.global_variables_initializer())\n",
    "    \n",
    "    #     # Restoring/loading/uploading the trained and validated model\n",
    "    #     saver.restore(sess,'checkpoints/mlp-fnirs-har.ckpt')\n",
    "    \n",
    "    # for every epoch start feeding the arrays into the tensors in the model\n",
    "    for epoch in range(train_epochs):\n",
    "        train_loss, valid_loss = [], []\n",
    "        train_acc, valid_acc = [], []\n",
    "        \n",
    "        # Training\n",
    "        for Xinputs, Yindices in get_batches(X=Xtrain, Y=Ytrain, batch_size=batch_size):\n",
    "            feed_dict = {model.Xinputs: Xinputs, model.Yindices: Yindices}\n",
    "            loss, acc, _ = sess.run(fetches=[model.loss, model.acc, model.opt], feed_dict=feed_dict)\n",
    "            train_loss.append(loss)\n",
    "            train_acc.append(acc)\n",
    "            \n",
    "        # printing out train and validation loss\n",
    "        print('epoch:', epoch+1, 'train_loss:', np.mean(train_loss), 'train_acc:', np.mean(train_acc))\n",
    "        \n",
    "        # Saving the losses for plotting\n",
    "        train_loss_mean.append(np.mean(train_loss))\n",
    "        train_acc_mean.append(np.mean(train_acc))\n",
    "        \n",
    "        # Validation\n",
    "        for Xinputs, Yindices in get_batches(X=Xvalid, Y=Yvalid, batch_size=batch_size):\n",
    "            feed_dict = {model.Xinputs: Xinputs, model.Yindices: Yindices}\n",
    "            loss, acc = sess.run(fetches=[model.loss, model.acc], feed_dict=feed_dict)\n",
    "            valid_loss.append(loss)\n",
    "            valid_acc.append(acc)\n",
    "        \n",
    "        # printing out train and validation loss\n",
    "        print('epoch:', epoch+1, 'valid_loss:', np.mean(valid_loss), 'valid_acc:', np.mean(valid_acc))\n",
    "\n",
    "        # Saving the losses for plotting\n",
    "        valid_loss_mean.append(np.mean(valid_loss))\n",
    "        valid_acc_mean.append(np.mean(valid_acc))\n",
    "        \n",
    "        #         # printing out train and validation loss\n",
    "        #         print('epoch:', epoch+1, \n",
    "        #               'train_loss:', np.mean(train_loss), 'valid_loss:', np.mean(valid_loss),\n",
    "        #               'train_acc:', np.mean(train_acc), 'valid_acc:', np.mean(valid_acc))\n",
    "        \n",
    "    \n",
    "    # Saving the trained and validated model\n",
    "    saver.save(sess,'checkpoints/mlp-fnirs-har.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as mplot\n",
    "%matplotlib inline\n",
    "\n",
    "mplot.plot(train_loss_mean, label='train_loss_mean')\n",
    "mplot.plot(valid_loss_mean, label='valid_loss_mean')\n",
    "mplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mplot.plot(train_acc_mean, label='train_acc_mean')\n",
    "mplot.plot(valid_acc_mean, label='valid_acc_mean')\n",
    "mplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(fetches=tf.global_variables_initializer())\n",
    "    \n",
    "    # Restoring/loading/uploading the trained and validated model\n",
    "    saver.restore(sess,'checkpoints/mlp-fnirs-har.ckpt')\n",
    "    \n",
    "    # Saving the test loss for every batch/minibtch\n",
    "    test_loss, test_acc = [], []\n",
    "    \n",
    "    # Testing\n",
    "    for Xinputs, Yindices in get_batches(X=Xtest, Y=Ytest, batch_size=batch_size):\n",
    "        feed_dict = {model.Xinputs: Xinputs, model.Yindices: Yindices}\n",
    "        loss, acc = sess.run(fetches=[model.loss, model.acc], feed_dict=feed_dict)\n",
    "        test_loss.append(loss)\n",
    "        test_acc.append(acc)\n",
    "        \n",
    "    # Printing the test loss\n",
    "    print('test_loss:', np.mean(test_loss), 'test acc', np.mean(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest test_loss: 0.7203058 test acc 0.67275465"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
