{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fNIRS data for Human Activity Recognition (HAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mP12-4-17-2018\u001b[0m/  \u001b[01;34mP14-4-18-2018\u001b[0m/  \u001b[01;34mP16-4-18-2018\u001b[0m/\r\n",
      "\u001b[01;34mP13-4-17-2018\u001b[0m/  \u001b[01;34mP15-4-18-2018\u001b[0m/  \u001b[01;34mP17-4-18-2018\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "% ls ../../../datasets/fNIRs_data/\n",
    "# % find ../../datasets/fNIRs_data/ | grep fNIR_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m1. Right Hand\u001b[0m/  \u001b[01;34m2. Both Hands\u001b[0m/  \u001b[01;34m3. Left Hand\u001b[0m/  \u001b[01;34m4. Right Leg\u001b[0m/  \u001b[01;34m5. Left Leg\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "% ls ../../../datasets/fNIRs_data/P12-4-17-2018/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m2018-04-17_006\u001b[0m/\r\n",
      "fNIR_data.txt\r\n",
      "head20180417-145130.txt\r\n",
      "NIRS-2018-04-17_006_deoxyhb_T141to2511_C1to20.txt\r\n",
      "NIRS-2018-04-17_006_oxyhb_T141to2511_C1to20.txt\r\n",
      "\u001b[01;34mProcessed\u001b[0m/\r\n",
      "r_hand20180417-145128.txt\r\n",
      "r_lower_arm20180417-145129.txt\r\n",
      "r_upper_arm20180417-145129.txt\r\n"
     ]
    }
   ],
   "source": [
    "% ls ../../../datasets/fNIRs_data/P12-4-17-2018/1.\\ Right\\ Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all(name, path):\n",
    "    result = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if name in files:\n",
    "            result.append(os.path.join(root, name))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpaths = find_all(name='fNIR_data.txt', path='/home/arasdar/datasets/fNIRs_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/5. Left Leg/fNIR_data.txt'],\n",
       " 30)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7 folders and subjects\n",
    "# 5 folders \n",
    "# 7*5=35 but there are 34 so one is missing.\n",
    "# This is why I got rid of the first which was subject P11.\n",
    "# 6 subjects and 5 activities = 30 files\n",
    "# 1, 2, 3, 4, 5\n",
    "# 0, 1, 2, 3, 4 these are all the classes\n",
    "((sorted(allpaths, reverse=False)), len(allpaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       " '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       " '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/3. Left Hand/fNIR_data.txt')"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allpaths[0], allpaths[-1], allpaths[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/arasdar-DL-env/lib/python3.6/site-packages/pandas/io/parsers.py:709: UserWarning: Duplicate names specified. This will raise an error in the future.\n",
      "  return _read(filepath_or_buffer, kwds)\n"
     ]
    }
   ],
   "source": [
    "# df: data frame object\n",
    "df = []\n",
    "for each_idx in range(len(allpaths)):\n",
    "    df.append(pd.read_csv(filepath_or_buffer=allpaths[each_idx], names=['time', 'sample', \n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/arasdar-DL-env/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/arasdar/anaconda3/envs/arasdar-DL-env/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for each in range(len(df)):\n",
    "    df[each]['sample'][1:] = df[each]['sample'][1:].astype(str).str[2:]\n",
    "    df[each]['channel.39'][1:] = df[each]['channel.39'][1:].astype(str).str[1:-1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = []\n",
    "for each in range(len(df)):\n",
    "    matrices.append(df[each][1:].as_matrix().astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For every single data or txt file here we have a label\n",
    "# # In total we 'll have five labels: 1, 2, 3, 4, 5\n",
    "# # These labels converted to a number will be: 0, 1, 2, 3, 4\n",
    "# # this would be 5 dimensional array\n",
    "# # starting like this:\n",
    "# np.array([1, 0, 0, 0, 0])\n",
    "# np.array([0, 1, 0, 0, 0])\n",
    "# # 0 1 0 0 0 \n",
    "# # 0 0 1 0 0\n",
    "# # 0 0 0 1 0\n",
    "# # 0 0 0 0 1\n",
    "# # ----------server/datasets/Project-HAR/subject-date/activity-class/input-data\n",
    "# # (['/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/1. Right Hand/fNIR_data.txt',\n",
    "# #   '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/2. Both Hands/fNIR_data.txt',\n",
    "# #   '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/3. Left Hand/fNIR_data.txt',\n",
    "# #   '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/4. Right Leg/fNIR_data.txt',\n",
    "# #   '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/5. Left Leg/fNIR_data.txt',\n",
    "# # len(data)\n",
    "# # For every single row in the matrix, \n",
    "# # there is gonna be a correspondence/ label/ supervision.\n",
    "# # That s how we can label this fNIRs signal.\n",
    "# # This is very much like the txt file and the folder\n",
    "# # it is subject independent, gender independent.\n",
    "# # fNIRs data meaning data dependent\n",
    "# # labeled/ certain activity\n",
    "# # subject and date independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of data and labels\n",
    "data, labels = [], []\n",
    "for row in range(0, len(matrices), 1):\n",
    "    data.append(matrices[row][:, 2:])\n",
    "    mat = np.zeros([data[row].shape[0], 5])\n",
    "    # for each in range(mat.shape[0]):\n",
    "    mat[:, row%5] = 1\n",
    "    labels.append(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.0674256 , 0.39841753, 0.8935684 , 0.11055192, 0.10349698,\n",
       "        0.10859558, 0.57522839, 0.16352198, 0.62339008, 0.12172733,\n",
       "        0.08546248, 0.22477067, 0.16844426, 0.22758171, 0.05182552,\n",
       "        0.90778166, 0.52145141, 0.66276747, 0.3101424 , 0.15122432,\n",
       "        0.15121958, 1.13010859, 1.93900955, 0.27584505, 0.23173632,\n",
       "        0.19257355, 1.2382772 , 0.33233806, 0.94395137, 0.19119443,\n",
       "        0.15490299, 0.69608921, 0.40221077, 0.47195953, 0.14262679,\n",
       "        1.59111071, 1.22073483, 1.76342607, 0.65680718, 0.22037013]),\n",
       " array([0., 0., 0., 0., 1.]))"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape, labels[0].shape\n",
    "data[0][1], labels[0][1]\n",
    "len(data), len(labels)\n",
    "data[-1][1], labels[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before stacking them all up on top pf each other,\n",
    "# we should separate the initial 70% of the stack for training and the last/ramaining 30% for testing.\n",
    "# That being said, we want to separate the initial 70% of the data for training and the remaining 30% for testing.\n",
    "# Every stack should be treated like this so that we can have a realistic test and validation data.\n",
    "# Here I can separate the test and train before stacking them all up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (2504, 40) (1074, 40) (2504, 5) (1074, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (3364, 40) (1443, 40) (3364, 5) (1443, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (5025, 40) (2156, 40) (5025, 5) (2156, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (5862, 40) (2515, 40) (5862, 5) (2515, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (6703, 40) (2876, 40) (6703, 5) (2876, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (8385, 40) (3598, 40) (8385, 5) (3598, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (9221, 40) (3957, 40) (9221, 5) (3957, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (10886, 40) (4671, 40) (10886, 5) (4671, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (11754, 40) (5044, 40) (11754, 5) (5044, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (12614, 40) (5413, 40) (12614, 5) (5413, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (14284, 40) (6129, 40) (14284, 5) (6129, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (15140, 40) (6496, 40) (15140, 5) (6496, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (16804, 40) (7210, 40) (16804, 5) (7210, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (17662, 40) (7578, 40) (17662, 5) (7578, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (18502, 40) (7939, 40) (18502, 5) (7939, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (20161, 40) (8651, 40) (20161, 5) (8651, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (21007, 40) (9014, 40) (21007, 5) (9014, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (22670, 40) (9728, 40) (22670, 5) (9728, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (23524, 40) (10095, 40) (23524, 5) (10095, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (24360, 40) (10454, 40) (24360, 5) (10454, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (26020, 40) (11166, 40) (26020, 5) (11166, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (26860, 40) (11527, 40) (26860, 5) (11527, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (28529, 40) (12243, 40) (28529, 5) (12243, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (29388, 40) (12612, 40) (29388, 5) (12612, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (30243, 40) (12979, 40) (30243, 5) (12979, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (31903, 40) (13691, 40) (31903, 5) (13691, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (32756, 40) (14057, 40) (32756, 5) (14057, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (34415, 40) (14769, 40) (34415, 5) (14769, 5)\n",
      "Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape:  (35269, 40) (15136, 40) (35269, 5) (15136, 5)\n"
     ]
    }
   ],
   "source": [
    "# we have to save the training size for each batch as well so that we can deduct the same percentage of\n",
    "# validation data from the training data after normalization\n",
    "# this is the traininzing size of each file\n",
    "# each file training size for validation\n",
    "# we need to have the complete train-valid size total so that we can deduct the percentage of it\n",
    "# this is each training file data size/length\n",
    "size = []\n",
    "\n",
    "# 70% training-validation and the first 70%\n",
    "# 30 percent of the last of each batch for test\n",
    "# define train to test ratio which 7 to 3 for example\n",
    "# a percentage of train out of total or test out of total\n",
    "# train percentage is 70% output total\n",
    "ratio = 0.7\n",
    "data[0].shape[0]*ratio, labels[0].shape[0]*ratio,\n",
    "limit = int(data[0].shape[0]*ratio)\n",
    "# initialize them and then stack them all up vertically\n",
    "Xtrain, Xtest = data[0][:limit], data[0][limit:]\n",
    "Ytrain, Ytest = labels[0][:limit], labels[0][limit:]\n",
    "\n",
    "# This is the training size for file for train-validation\n",
    "each=0\n",
    "size.append(data[each][:limit].shape[0])\n",
    "\n",
    "for each in range(1, len(data), 1): # start, stop, step\n",
    "    # each=0\n",
    "    limit = int(data[each].shape[0]*ratio)\n",
    "#     print(Xtrain.shape, data[each][:limit].shape)\n",
    "    size.append(data[each][:limit].shape[0])\n",
    "    Xtrain, Xtest = np.vstack(tup=(Xtrain, data[each][:limit])), np.vstack(tup=(Xtest, data[each][limit:]))\n",
    "    Ytrain, Ytest = np.vstack(tup=(Ytrain, labels[each][:limit])), np.vstack(tup=(Ytest, labels[each][limit:]))\n",
    "    print('Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape: ', Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalizing(X):\n",
    "    Xnorm = (X - X.mean(axis=0))/ X.std(axis=0)\n",
    "#     print('Xnorm.shape, Xnorm.dtype, Xnorm.mean(axis=0), Xnorm.std(axis=0):', \n",
    "    print(Xnorm.shape, Xnorm.dtype, Xnorm.mean(axis=0), Xnorm.std(axis=0))\n",
    "    return Xnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35269, 40) float64 [ 6.44684214e-17 -1.86958422e-16 -1.03149474e-16  2.57873686e-17\n",
      "  1.03149474e-16 -1.03149474e-16  0.00000000e+00  2.57873686e-17\n",
      " -4.51278950e-17  5.15747371e-17  9.02557900e-17 -6.44684214e-17\n",
      "  2.57873686e-16  1.03149474e-16  2.57873686e-17  2.57873686e-17\n",
      " -2.57873686e-17  5.15747371e-17  6.44684214e-18 -6.44684214e-18\n",
      "  7.73621057e-17 -2.57873686e-17 -2.57873686e-17  6.44684214e-17\n",
      " -1.67617896e-16  3.86810528e-17 -5.15747371e-17 -5.15747371e-17\n",
      " -1.67617896e-16  1.03149474e-16 -7.73621057e-17 -1.30548553e-16\n",
      " -1.03149474e-16  1.28936843e-16  1.54724211e-16  5.15747371e-17\n",
      " -5.15747371e-17  1.03149474e-16 -4.35161844e-17 -5.15747371e-17] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "Xtrain_norm = Normalizing(X=Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15136, 40) float64 [-1.05154316e-16 -9.01322709e-17  6.00881806e-17  2.10308632e-16\n",
      " -1.20176361e-16  2.10308632e-16  4.50661355e-17 -3.00440903e-17\n",
      " -5.25771580e-17  3.00440903e-17  9.01322709e-17 -9.01322709e-17\n",
      "  1.80264542e-16 -1.50220452e-16  9.01322709e-17  1.20176361e-16\n",
      "  1.65242497e-16 -1.65242497e-16 -6.00881806e-17 -6.00881806e-17\n",
      " -6.00881806e-17 -7.51102258e-17  9.01322709e-17  3.00440903e-17\n",
      "  6.00881806e-17 -6.00881806e-17  6.00881806e-17  1.80264542e-16\n",
      "  4.50661355e-17  0.00000000e+00  7.51102258e-17  6.00881806e-17\n",
      "  2.40352722e-16 -6.00881806e-17 -6.00881806e-17  9.01322709e-17\n",
      " -1.20176361e-16  6.00881806e-17  0.00000000e+00 -6.00881806e-17] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "Xtest_norm = Normalizing(X=Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sr = 0.129 # 111933.573-111933.504 is the difference between each two samples or sampling rate\n",
    "# each_trial=30 # 30seconds=20sec+10sec\n",
    "# # num_samples_per_trial\n",
    "# # each trial or epoch based on BCI course\n",
    "# # height or window size\n",
    "# 30/0.129, np.floor(30/0.129), np.ceil(30/0.129)\n",
    "# # The size of each image for convnet will be 233x40x1 which is equal to the format NHWC, txn or NWC\n",
    "# # NWC is for signals since all the channels are projected the same as all channels in an image as well.\n",
    "# # In each convolutionn for image or signals the channels are all included for filtering\n",
    "# # Image or video might be part of this\n",
    "# # Image is NHWC or NCHW\n",
    "# # Signal is NWC or NCW\n",
    "# # N is the number of trials/epochs/windows\n",
    "# # C is the number of channels is 40 in this experiment: 20 for hemoglobin with o2 and 20 for hemoglobin with co2\n",
    "# # W is the window width size or signal window size which is 233\n",
    "# width = np.ceil(30/0.129)\n",
    "# # This is the number of minibatches per file\n",
    "# num_mb = mat.shape[0] - width +1\n",
    "# # num_mb # number of minibatches\n",
    "# # totla number of all the windows with the overlapping windows of 1 sample\n",
    "# num_mb, width\n",
    "# # len(allpaths), allpaths[44], num_mb*len(allpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtrain, Xvalid = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(size), size[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (585, 40) (251, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (585, 5) (251, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (1752, 40) (752, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (1752, 5) (752, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (2354, 40) (1010, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (2354, 5) (1010, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (3516, 40) (1509, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (3516, 5) (1509, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (4101, 40) (1761, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (4101, 5) (1761, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (4689, 40) (2014, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (4689, 5) (2014, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (5866, 40) (2519, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (5866, 5) (2519, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (6451, 40) (2770, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (6451, 5) (2770, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (7616, 40) (3270, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (7616, 5) (3270, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (8223, 40) (3531, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (8223, 5) (3531, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (8825, 40) (3789, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (8825, 5) (3789, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (9994, 40) (4290, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (9994, 5) (4290, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (10593, 40) (4547, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (10593, 5) (4547, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (11757, 40) (5047, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (11757, 5) (5047, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (12357, 40) (5305, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (12357, 5) (5305, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (12945, 40) (5557, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (12945, 5) (5557, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (14106, 40) (6055, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (14106, 5) (6055, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (14698, 40) (6309, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (14698, 5) (6309, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (15862, 40) (6808, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (15862, 5) (6808, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (16459, 40) (7065, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (16459, 5) (7065, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (17044, 40) (7316, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (17044, 5) (7316, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (18206, 40) (7814, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (18206, 5) (7814, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (18794, 40) (8066, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (18794, 5) (8066, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (19962, 40) (8567, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (19962, 5) (8567, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (20563, 40) (8825, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (20563, 5) (8825, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (21161, 40) (9082, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (21161, 5) (9082, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (22323, 40) (9580, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (22323, 5) (9580, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (22920, 40) (9836, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (22920, 5) (9836, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (24081, 40) (10334, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (24081, 5) (10334, 5)\n",
      "Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape:  (35269, 40) (24678, 40) (10591, 40)\n",
      "Ytrain.shape, Yntrain.shape, Ynvalid.shape: (35269, 5) (24678, 5) (10591, 5)\n"
     ]
    }
   ],
   "source": [
    "# ratio of train to validation\n",
    "# ratio = 0.7\n",
    "# # each = 0\n",
    "# # in limits there is somthing like high limit and low limit\n",
    "# limit = int(size[0]*ratio)\n",
    "# print(limit, size[0])\n",
    "# # initialize them and then stack them all up vertically\n",
    "# # This is train  and validation split after normalization\n",
    "# Xntrain, Xnvalid = Xtrain_norm[:limit], Xtrain_norm[limit:size[0]]\n",
    "# Yntrain, Ynvalid = Ytrain[:limit], Ytrain[limit:size[each]]\n",
    "# print(Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape)\n",
    "# print(Ytrain.shape, Yntrain.shape, Ynvalid.shape)\n",
    "\n",
    "ratio = 0.7\n",
    "sizeHigh = 0\n",
    "# for each in range(size):\n",
    "# print(len(size))\n",
    "each = 0\n",
    "# in limits there is somthing like high limit and low limit\n",
    "limit = int(size[each]*ratio)\n",
    "# print(limit, size[each])\n",
    "sizeLow = sizeHigh\n",
    "sizeHigh += size[each] \n",
    "Xntrain = Xtrain_norm[sizeLow      : sizeLow+limit] \n",
    "Xnvalid = Xtrain_norm[sizeLow+limit: sizeHigh     ]\n",
    "Yntrain = Ytrain[sizeLow      : sizeLow+limit] \n",
    "Ynvalid = Ytrain[sizeLow+limit: sizeHigh     ]\n",
    "print('Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape: ', \n",
    "     Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape)\n",
    "print('Ytrain.shape, Yntrain.shape, Ynvalid.shape:',\n",
    "     Ytrain.shape, Yntrain.shape, Ynvalid.shape)\n",
    "\n",
    "for each in range(1, len(size), 1):\n",
    "# each = 1\n",
    "# in limits there is somthing like high limit and low limit\n",
    "    limit = int(size[each]*ratio)\n",
    "#     print(limit, size[each])\n",
    "    sizeLow = sizeHigh\n",
    "    sizeHigh += size[each] \n",
    "    Xntrain = np.vstack(tup=(Xntrain, Xtrain_norm[sizeLow      : sizeLow+limit])) \n",
    "    Xnvalid = np.vstack(tup=(Xnvalid, Xtrain_norm[sizeLow+limit: sizeHigh     ])) \n",
    "    Yntrain = np.vstack(tup=(Yntrain, Ytrain[sizeLow      : sizeLow+limit])) \n",
    "    Ynvalid = np.vstack(tup=(Ynvalid, Ytrain[sizeLow+limit: sizeHigh     ])) \n",
    "    print('Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape: ', \n",
    "         Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape)\n",
    "    print('Ytrain.shape, Yntrain.shape, Ynvalid.shape:',\n",
    "         Ytrain.shape, Yntrain.shape, Ynvalid.shape)\n",
    "\n",
    "# each = 2\n",
    "# # in limits there is somthing like high limit and low limit\n",
    "# limit = int(size[each]*ratio)\n",
    "# print(limit, size[each])\n",
    "# sizeLow = sizeHigh\n",
    "# sizeHigh += size[each] \n",
    "# Xntrain = np.vstack(tup=(Xntrain, Xtrain_norm[sizeLow      : sizeLow+limit])) \n",
    "# Xnvalid = np.vstack(tup=(Xnvalid, Xtrain_norm[sizeLow+limit: sizeHigh     ])) \n",
    "# Yntrain = np.vstack(tup=(Yntrain, Ytrain[sizeLow      : sizeLow+limit])) \n",
    "# Ynvalid = np.vstack(tup=(Ynvalid, Ytrain[sizeLow+limit: sizeHigh     ])) \n",
    "# print(Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape)\n",
    "# print(Ytrain.shape, Yntrain.shape, Ynvalid.shape)\n",
    "\n",
    "\n",
    "# # each = 2\n",
    "# # # in limits there is somthing like high limit and low limit\n",
    "# # limit = int(size[each]*ratio)\n",
    "# # print(limit, size[each])\n",
    "# # sizeLow = sizeHigh\n",
    "# # sizeHigh += size[each] \n",
    "# # Xntrain = Xtrain_norm[sizeLow      : sizeLow+limit] \n",
    "# # Xnvalid = Xtrain_norm[sizeLow+limit: sizeHigh     ]\n",
    "# # # Yntrain, Ynvalid = Ytrain[size[0]:size[0]+limit], Ytrain[size[0]+limit:size[0]+size[1]]\n",
    "# # print(Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape)\n",
    "# # # print(Ytrain.shape, Yntrain.shape, Ynvalid.shape)\n",
    "\n",
    "# # each = 3\n",
    "# # # in limits there is somthing like high limit and low limit\n",
    "# # limit = int(size[each]*ratio)\n",
    "# # print(limit, size[each])\n",
    "# # sizeLow = sizeHigh\n",
    "# # sizeHigh += size[each] \n",
    "# # Xntrain = Xtrain_norm[sizeLow      : sizeLow+limit] \n",
    "# # Xnvalid = Xtrain_norm[sizeLow+limit: sizeHigh     ]\n",
    "# # # Yntrain, Ynvalid = Ytrain[size[0]:size[0]+limit], Ytrain[size[0]+limit:size[0]+size[1]]\n",
    "# # print(Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape)\n",
    "# # # print(Ytrain.shape, Yntrain.shape, Ynvalid.shape)\n",
    "\n",
    "# # each = 4\n",
    "# # # in limits there is somthing like high limit and low limit\n",
    "# # limit = int(size[each]*ratio)\n",
    "# # print(limit, size[each])\n",
    "# # sizeLow = sizeHigh\n",
    "# # sizeHigh += size[each] \n",
    "# # Xntrain = Xtrain_norm[sizeLow      : sizeLow+limit] \n",
    "# # Xnvalid = Xtrain_norm[sizeLow+limit: sizeHigh     ]\n",
    "# # # Yntrain, Ynvalid = Ytrain[size[0]:size[0]+limit], Ytrain[size[0]+limit:size[0]+size[1]]\n",
    "# # print(Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape)\n",
    "# # # print(Ytrain.shape, Yntrain.shape, Ynvalid.shape)\n",
    "\n",
    "# # each = 5\n",
    "# # # in limits there is somthing like high limit and low limit\n",
    "# # limit = int(size[each]*ratio)\n",
    "# # print(limit, size[each])\n",
    "# # sizeLow = sizeHigh\n",
    "# # sizeHigh += size[each] \n",
    "# # Xntrain = Xtrain_norm[sizeLow      : sizeLow+limit] \n",
    "# # Xnvalid = Xtrain_norm[sizeLow+limit: sizeHigh     ]\n",
    "# # # Yntrain, Ynvalid = Ytrain[size[0]:size[0]+limit], Ytrain[size[0]+limit:size[0]+size[1]]\n",
    "# # print(Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape)\n",
    "# # # print(Ytrain.shape, Yntrain.shape, Ynvalid.shape)\n",
    "\n",
    "# # each = 6\n",
    "# # # in limits there is somthing like high limit and low limit\n",
    "# # limit = int(size[each]*ratio)\n",
    "# # print(limit, size[each])\n",
    "# # sizeLow = sizeHigh\n",
    "# # sizeHigh += size[each] \n",
    "# # Xntrain = Xtrain_norm[sizeLow      : sizeLow+limit] \n",
    "# # Xnvalid = Xtrain_norm[sizeLow+limit: sizeHigh     ]\n",
    "# # # Yntrain, Ynvalid = Ytrain[size[0]:size[0]+limit], Ytrain[size[0]+limit:size[0]+size[1]]\n",
    "# # print(Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape)\n",
    "# # # print(Ytrain.shape, Yntrain.shape, Ynvalid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Xtrain and Xtest is 70% and 30%\n",
    "# # # This 0.7*length and 0.3* length every single matrix\n",
    "# # # Xtrain is then turned into train and validation\n",
    "# # # before that though it should normalized meaning zero-mean and 1-std\n",
    "# Xtrain, Xtest = [], []\n",
    "#  X.shape, Y.shape\n",
    "# # In this case, we 'll take the first \n",
    "# # # THe length of data and labels should be the same/equal\n",
    "# # # Ths is how we are going to devide this:\n",
    "# # # 70% first should be the training\n",
    "# # # for idx in range(0, len(data), 1): # start, stop, step\n",
    "# # #     l = h\n",
    "# # idx=0\n",
    "# # h = X[idx].shape[0] * 0.7\n",
    "# # X[:h].shape\n",
    "# # # #     print(idx, l, h, h-width, Xnorm.shape, data[idx].shape, Xnorm[l:h].shape, width)\n",
    "# # # Xnt.append(Xnorm[l:h-width])\n",
    "# # # Xnv.append(Xnorm[h-width:h])\n",
    "# # # print(Xnt[idx].shape, Xnv[idx].shape, data[idx].shape, width)    \n",
    "# # # print(Xnt[idx].dtype, Xnv[idx].dtype, data[idx].dtype, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(X, batch_size):\n",
    "    \"\"\" Return a generator for batches \"\"\"\n",
    "    n_batches = len(X) // batch_size\n",
    "    X = X[:n_batches*batch_size]\n",
    "\n",
    "    # Loop over batches and yield\n",
    "    for b in range(0, len(X), batch_size):\n",
    "        yield X[b:b+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n",
      "(250, 40)\n"
     ]
    }
   ],
   "source": [
    "Xntrain_bacthes = get_batches(X=Xntrain, batch_size=250)\n",
    "# len(Xntrain_bacthes)\n",
    "for each in Xntrain_bacthes:\n",
    "    print(each.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 250 is the rounded up version of 233 which is rest+action period for each activity\n",
    "# width, h, Xntrainmb, Xnvalidmb, Yntrainmb, Ynvalidmb = 250, 0, [], [], [], []\n",
    "# for idx in range(Xntrain.shape[0]): # start, stop, step\n",
    "#     #     This is very similar to convolution since this is a minibatching technique\n",
    "#     # window size kernel size or minibatch size\n",
    "#     # stride or overlapping window or percentage\n",
    "#     # dilation or or jump=0\n",
    "#     # padding=0\n",
    "#     num_minibatches = ((Xntrain.shape[0] - width)/stride) + 1\n",
    "#     Xntrainmb.append(Xntrain[idx:idx+width])\n",
    "#     print(Xnt[idx].dtype, Xnv[idx].dtype, data[idx].dtype, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111212\n"
     ]
    }
   ],
   "source": [
    "# This is scanning through the signal and extracting the minibatches\n",
    "# TO be accurate and having more samples, I want to scan it with the \n",
    "# window size=250, overlap/stride=1, no padding\n",
    "# Xnt-minibatches, only test one of them\n",
    "Xnt[0].shape, width, Xnt[0].shape[0]-width+1, Xnt[1].shape[0]-width+1\n",
    "# total number of minibatches extracted from each sample\n",
    "num_mb = 0\n",
    "for each in range(len(Xnt)):\n",
    "#     print(Xnt[each].shape[0]-width+1)\n",
    "    num_mb += Xnt[each].shape[0]-width+1\n",
    "print(num_mb)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to extract batches/minibatches from the first set of training data\n",
    "# The number of minibatches are equal to\n",
    "# This is the number of minibatches in the first set\n",
    "num_mb = Xnt[0].shape[0]-width+1\n",
    "# this the python list for holding all the minibatches inside\n",
    "mb=[]\n",
    "for each in range(num_mb):\n",
    "    #it has to start from 0+step:width+step/stride and sweeping the entire signal\n",
    "#     for stride in range()\n",
    "# The number of minibatches should be equal to the length of the list\n",
    "    mb.append(Xnt[0][each:each+width, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7888, 7888, (250, 40))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let s double check the minibatches list length and the estimated number of minibatches\n",
    "len(mb), num_mb, mb[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mb = Xnt[1].shape[0]-width+1\n",
    "for each in range(num_mb):\n",
    "    mb.append(Xnt[1][each:each+width, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(696, 8584, 8584)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_mb, len(mb), 7888+696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mb = Xnt[2].shape[0]-width+1\n",
    "for each in range(num_mb):\n",
    "    mb.append(Xnt[2][each:each+width, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1884, 10468, 10468)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_mb, len(mb), 7888+696+1884"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = []\n",
    "for eachXnt in range(len(Xnt)):\n",
    "    num_mb = Xnt[eachXnt].shape[0]-width+1\n",
    "    for each in range(num_mb):\n",
    "        mb.append(Xnt[eachXnt][each:each+width, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 111212, 44)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # len(mb)\n",
    "# for each in Xnt:\n",
    "#     print(each.shape, each.dtype)\n",
    "len(Xnt), len(mb), len(Xnv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n",
      "(250, 40) float64\n"
     ]
    }
   ],
   "source": [
    "# len(mb)\n",
    "for each in Xnv:\n",
    "    print(each.shape, each.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
