{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fNIRS data for Human Activity Recognition (HAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mP12-4-17-2018\u001b[0m/  \u001b[01;34mP14-4-18-2018\u001b[0m/  \u001b[01;34mP16-4-18-2018\u001b[0m/\r\n",
      "\u001b[01;34mP13-4-17-2018\u001b[0m/  \u001b[01;34mP15-4-18-2018\u001b[0m/  \u001b[01;34mP17-4-18-2018\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "% ls ../../../datasets/fNIRs_data/\n",
    "# % find ../../datasets/fNIRs_data/ | grep fNIR_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m1. Right Hand\u001b[0m/  \u001b[01;34m2. Both Hands\u001b[0m/  \u001b[01;34m3. Left Hand\u001b[0m/  \u001b[01;34m4. Right Leg\u001b[0m/  \u001b[01;34m5. Left Leg\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "% ls ../../../datasets/fNIRs_data/P12-4-17-2018/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m2018-04-17_006\u001b[0m/\r\n",
      "fNIR_data.txt\r\n",
      "head20180417-145130.txt\r\n",
      "NIRS-2018-04-17_006_deoxyhb_T141to2511_C1to20.txt\r\n",
      "NIRS-2018-04-17_006_oxyhb_T141to2511_C1to20.txt\r\n",
      "\u001b[01;34mProcessed\u001b[0m/\r\n",
      "r_hand20180417-145128.txt\r\n",
      "r_lower_arm20180417-145129.txt\r\n",
      "r_upper_arm20180417-145129.txt\r\n"
     ]
    }
   ],
   "source": [
    "% ls ../../../datasets/fNIRs_data/P12-4-17-2018/1.\\ Right\\ Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all(name, path):\n",
    "    result = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if name in files:\n",
    "            result.append(os.path.join(root, name))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpaths = find_all(name='fNIR_data.txt', path='/home/arasdar/datasets/fNIRs_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/5. Left Leg/fNIR_data.txt'],\n",
       " 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7 folders and subjects\n",
    "# 5 folders \n",
    "# 7*5=35 but there are 34 so one is missing.\n",
    "# This is why I got rid of the first which was subject P11.\n",
    "# 6 subjects and 5 activities = 30 files\n",
    "# 1, 2, 3, 4, 5\n",
    "# 0, 1, 2, 3, 4 these are all the classes\n",
    "((sorted(allpaths, reverse=False)), len(allpaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/arasdar-DL-env/lib/python3.6/site-packages/pandas/io/parsers.py:709: UserWarning: Duplicate names specified. This will raise an error in the future.\n",
      "  return _read(filepath_or_buffer, kwds)\n"
     ]
    }
   ],
   "source": [
    "# df: data frame object\n",
    "df = []\n",
    "for each_idx in range(len(allpaths)):\n",
    "    df.append(pd.read_csv(filepath_or_buffer=allpaths[each_idx], names=['time', 'sample', \n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/arasdar-DL-env/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/arasdar/anaconda3/envs/arasdar-DL-env/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for each in range(len(df)):\n",
    "    df[each]['sample'][1:] = df[each]['sample'][1:].astype(str).str[2:]\n",
    "    df[each]['channel.39'][1:] = df[each]['channel.39'][1:].astype(str).str[1:-1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = []\n",
    "for each in range(len(df)):\n",
    "    matrices.append(df[each][1:].as_matrix().astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1195, 40) (1195, 5)\n",
      "30 30\n"
     ]
    }
   ],
   "source": [
    "# List of data and labels\n",
    "data, labels = [], []\n",
    "for row in range(0, len(matrices), 1):\n",
    "    data.append(matrices[row][:, 2:])\n",
    "    mat = np.zeros([data[row].shape[0], 5])\n",
    "    # for each in range(mat.shape[0]):\n",
    "    mat[:, row%5] = 1\n",
    "    labels.append(mat)\n",
    "print(data[0].shape, labels[0].shape)\n",
    "# data[0][1], labels[0][1]\n",
    "print(len(data), len(labels))\n",
    "# data[-1][1], labels[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I should normalize this list which is going to be a big probelm\n",
    "# Concatenation and vertical stack or vstack was done for this reason.\n",
    "# Only input should be normalized.\n",
    "# np.array(data).shape\n",
    "# we have to divide the data in the begining to test and training data\n",
    "# after that we should normalize the data separately\n",
    "# after that we should separate the training data into train and validation\n",
    "# after that we should create the batches of data from test, train, and validation into NWC\n",
    "# These will be given into get_batches which will be fed into the ConvNet for classification.\n",
    "# After training this data, the train and validated model can tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.70 percent each data whihc fNIRstxt is train and 0.30 is test\n",
    "# 30% of train data is validation which 21% and ~50% of the data is training.\n",
    "# The total is 50% train, 20% valid, 30% test\n",
    "ratio = 0.7\n",
    "Ltrain, Ltest = [], []\n",
    "Ytrain, Ytest = [], []\n",
    "\n",
    "each=0\n",
    "limit = int(data[each].shape[0]*ratio)\n",
    "Xtrain, Xtest = data[each][:limit], data[each][limit:]\n",
    "Ytrain.append(labels[each][:limit])\n",
    "Ytest.append(labels[each][limit:])\n",
    "Ltrain.append(data[each][:limit].shape[0])\n",
    "Ltest.append(data[each][limit:].shape[0])\n",
    "# print(Xtrain.shape, Xtest.shape) \n",
    "# print(Ytrain[each].shape, Ytest[each].shape) \n",
    "# print(Ltrain[each], Ltest[each]) \n",
    "\n",
    "for each in range(1, len(data), 1): # start, stop, step\n",
    "    # each=0\n",
    "    limit = int(data[each].shape[0]*ratio)\n",
    "    Xtrain, Xtest = np.vstack(tup=(Xtrain, data[each][:limit])), np.vstack(tup=(Xtest, data[each][limit:]))\n",
    "    Ltrain.append(data[each][:limit].shape[0])\n",
    "    Ltest.append(data[each][limit:].shape[0])\n",
    "    Ytrain.append(labels[each][:limit])\n",
    "    Ytest.append(labels[each][limit:])\n",
    "#     print(Ytrain[each].shape, Ytest[each].shape) \n",
    "#     print(Xtrain.shape, Xtest.shape) \n",
    "#     print(Ltrain[each], Ltest[each]) \n",
    "#     print(len(Ltrain), len(Ltest)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalizing(X):\n",
    "    Xnorm = (X - X.mean(axis=0))/ X.std(axis=0)\n",
    "    print(Xnorm.shape, Xnorm.dtype, Xnorm.mean(axis=0), Xnorm.std(axis=0))\n",
    "    return Xnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35269, 40) float64 [ 6.44684214e-17 -1.86958422e-16 -1.03149474e-16  2.57873686e-17\n",
      "  1.03149474e-16 -1.03149474e-16  0.00000000e+00  2.57873686e-17\n",
      " -4.51278950e-17  5.15747371e-17  9.02557900e-17 -6.44684214e-17\n",
      "  2.57873686e-16  1.03149474e-16  2.57873686e-17  2.57873686e-17\n",
      " -2.57873686e-17  5.15747371e-17  6.44684214e-18 -6.44684214e-18\n",
      "  7.73621057e-17 -2.57873686e-17 -2.57873686e-17  6.44684214e-17\n",
      " -1.67617896e-16  3.86810528e-17 -5.15747371e-17 -5.15747371e-17\n",
      " -1.67617896e-16  1.03149474e-16 -7.73621057e-17 -1.30548553e-16\n",
      " -1.03149474e-16  1.28936843e-16  1.54724211e-16  5.15747371e-17\n",
      " -5.15747371e-17  1.03149474e-16 -4.35161844e-17 -5.15747371e-17] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "Xtrain_norm = Normalizing(X=Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15136, 40) float64 [-1.05154316e-16 -9.01322709e-17  6.00881806e-17  2.10308632e-16\n",
      " -1.20176361e-16  2.10308632e-16  4.50661355e-17 -3.00440903e-17\n",
      " -5.25771580e-17  3.00440903e-17  9.01322709e-17 -9.01322709e-17\n",
      "  1.80264542e-16 -1.50220452e-16  9.01322709e-17  1.20176361e-16\n",
      "  1.65242497e-16 -1.65242497e-16 -6.00881806e-17 -6.00881806e-17\n",
      " -6.00881806e-17 -7.51102258e-17  9.01322709e-17  3.00440903e-17\n",
      "  6.00881806e-17 -6.00881806e-17  6.00881806e-17  1.80264542e-16\n",
      "  4.50661355e-17  0.00000000e+00  7.51102258e-17  6.00881806e-17\n",
      "  2.40352722e-16 -6.00881806e-17 -6.00881806e-17  9.01322709e-17\n",
      " -1.20176361e-16  6.00881806e-17  0.00000000e+00 -6.00881806e-17] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "Xtest_norm = Normalizing(X=Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(854, 40) (367, 40)\n",
      "(854, 5) (367, 5)\n",
      "float64 float64\n",
      "30 30\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Xtest = [], []\n",
    "h_train, h_test = 0, 0\n",
    "# print(len(Ltrain), len(Ltest))\n",
    "\n",
    "for each in range(len(Ltrain)): # start, stop, step\n",
    "    # each = 0\n",
    "    l_train = h_train\n",
    "    h_train += Ltrain[each] \n",
    "    l_test = h_test\n",
    "    h_test += Ltest[each]\n",
    "    Xtrain.append(Xtrain_norm[l_train:h_train])\n",
    "    Xtest.append(Xtest_norm[l_test:h_test])\n",
    "print(Xtrain[each].shape, Xtest[each].shape)\n",
    "print(Ytrain[each].shape, Ytest[each].shape) \n",
    "print(Xtrain[each].dtype, Xtest[each].dtype)\n",
    "print(len(Xtrain), len(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sr = 0.129 # 111933.573-111933.504 is the difference between each two samples or sampling rate\n",
    "# each_trial=30 # 30seconds=20sec+10sec\n",
    "# # num_samples_per_trial\n",
    "# # each trial or epoch based on BCI course\n",
    "# # height or window size\n",
    "# 30/0.129, np.floor(30/0.129), np.ceil(30/0.129)\n",
    "# # The size of each image for convnet will be 233x40x1 which is equal to the format NHWC, txn or NWC\n",
    "# # NWC is for signals since all the channels are projected the same as all channels in an image as well.\n",
    "# # In each convolutionn for image or signals the channels are all included for filtering\n",
    "# # Image or video might be part of this\n",
    "# # Image is NHWC or NCHW\n",
    "# # Signal is NWC or NCW\n",
    "# # N is the number of trials/epochs/windows\n",
    "# # C is the number of channels is 40 in this experiment: 20 for hemoglobin with o2 and 20 for hemoglobin with co2\n",
    "# # W is the window width size or signal window size which is 233\n",
    "# width = np.ceil(30/0.129)\n",
    "# # This is the number of minibatches per file\n",
    "# num_mb = mat.shape[0] - width +1\n",
    "# # num_mb # number of minibatches\n",
    "# # totla number of all the windows with the overlapping windows of 1 sample\n",
    "# num_mb, width\n",
    "# # len(allpaths), allpaths[44], num_mb*len(allpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(836, 40) (585, 40) (251, 40)\n",
      "(836, 5) (585, 5) (251, 5)\n",
      "(1668, 40) (1167, 40) (501, 40)\n",
      "(1668, 5) (1167, 5) (501, 5)\n",
      "(860, 40) (602, 40) (258, 40)\n",
      "(860, 5) (602, 5) (258, 5)\n",
      "(1661, 40) (1162, 40) (499, 40)\n",
      "(1661, 5) (1162, 5) (499, 5)\n",
      "(837, 40) (585, 40) (252, 40)\n",
      "(837, 5) (585, 5) (252, 5)\n",
      "(841, 40) (588, 40) (253, 40)\n",
      "(841, 5) (588, 5) (253, 5)\n",
      "(1682, 40) (1177, 40) (505, 40)\n",
      "(1682, 5) (1177, 5) (505, 5)\n",
      "(836, 40) (585, 40) (251, 40)\n",
      "(836, 5) (585, 5) (251, 5)\n",
      "(1665, 40) (1165, 40) (500, 40)\n",
      "(1665, 5) (1165, 5) (500, 5)\n",
      "(868, 40) (607, 40) (261, 40)\n",
      "(868, 5) (607, 5) (261, 5)\n",
      "(860, 40) (602, 40) (258, 40)\n",
      "(860, 5) (602, 5) (258, 5)\n",
      "(1670, 40) (1169, 40) (501, 40)\n",
      "(1670, 5) (1169, 5) (501, 5)\n",
      "(856, 40) (599, 40) (257, 40)\n",
      "(856, 5) (599, 5) (257, 5)\n",
      "(1664, 40) (1164, 40) (500, 40)\n",
      "(1664, 5) (1164, 5) (500, 5)\n",
      "(858, 40) (600, 40) (258, 40)\n",
      "(858, 5) (600, 5) (258, 5)\n",
      "(840, 40) (588, 40) (252, 40)\n",
      "(840, 5) (588, 5) (252, 5)\n",
      "(1659, 40) (1161, 40) (498, 40)\n",
      "(1659, 5) (1161, 5) (498, 5)\n",
      "(846, 40) (592, 40) (254, 40)\n",
      "(846, 5) (592, 5) (254, 5)\n",
      "(1663, 40) (1164, 40) (499, 40)\n",
      "(1663, 5) (1164, 5) (499, 5)\n",
      "(854, 40) (597, 40) (257, 40)\n",
      "(854, 5) (597, 5) (257, 5)\n",
      "(836, 40) (585, 40) (251, 40)\n",
      "(836, 5) (585, 5) (251, 5)\n",
      "(1660, 40) (1162, 40) (498, 40)\n",
      "(1660, 5) (1162, 5) (498, 5)\n",
      "(840, 40) (588, 40) (252, 40)\n",
      "(840, 5) (588, 5) (252, 5)\n",
      "(1669, 40) (1168, 40) (501, 40)\n",
      "(1669, 5) (1168, 5) (501, 5)\n",
      "(859, 40) (601, 40) (258, 40)\n",
      "(859, 5) (601, 5) (258, 5)\n",
      "(855, 40) (598, 40) (257, 40)\n",
      "(855, 5) (598, 5) (257, 5)\n",
      "(1660, 40) (1162, 40) (498, 40)\n",
      "(1660, 5) (1162, 5) (498, 5)\n",
      "(853, 40) (597, 40) (256, 40)\n",
      "(853, 5) (597, 5) (256, 5)\n",
      "(1659, 40) (1161, 40) (498, 40)\n",
      "(1659, 5) (1161, 5) (498, 5)\n",
      "(854, 40) (597, 40) (257, 40)\n",
      "(854, 5) (597, 5) (257, 5)\n"
     ]
    }
   ],
   "source": [
    "# ratio of train to validation\n",
    "# ratio = 0.7\n",
    "# # each = 0\n",
    "# # in limits there is somthing like high limit and low limit\n",
    "# limit = int(size[0]*ratio)\n",
    "# print(limit, size[0])\n",
    "# # initialize them and then stack them all up vertically\n",
    "# # This is train  and validation split after normalization\n",
    "# Xntrain, Xnvalid = Xtrain_norm[:limit], Xtrain_norm[limit:size[0]]\n",
    "# Yntrain, Ynvalid = Ytrain[:limit], Ytrain[limit:size[each]]\n",
    "# print(Xtrain_norm.shape, Xntrain.shape, Xnvalid.shape)\n",
    "# print(Ytrain.shape, Yntrain.shape, Ynvalid.shape)\n",
    "\n",
    "# sizeHigh = 0\n",
    "# each = 0\n",
    "size = Ltrain\n",
    "Xntrain, Xnvalid = [], []\n",
    "Yntrain, Ynvalid = [], []\n",
    "for each in range(0, len(size), 1):\n",
    "    limit = int(size[each]*ratio)\n",
    "#     print(limit, size[each]-limit)\n",
    "    Xntrain.append(Xtrain[each][:limit])\n",
    "#     print(Xntrain[each].shape)\n",
    "    Xnvalid.append(Xtrain[each][limit:])\n",
    "#     print(Xnvalid[each].shape)\n",
    "    print(Xtrain[each].shape, Xntrain[each].shape, Xnvalid[each].shape)\n",
    "    Yntrain.append(Ytrain[each][:limit])\n",
    "#     print(Xntrain[each].shape)\n",
    "    Ynvalid.append(Ytrain[each][limit:])\n",
    "#     print(Xnvalid[each].shape)\n",
    "    print(Ytrain[each].shape, Yntrain[each].shape, Ynvalid[each].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Xtrain and Xtest is 70% and 30%\n",
    "# # # This 0.7*length and 0.3* length every single matrix\n",
    "# # # Xtrain is then turned into train and validation\n",
    "# # # before that though it should normalized meaning zero-mean and 1-std\n",
    "# Xtrain, Xtest = [], []\n",
    "#  X.shape, Y.shape\n",
    "# # In this case, we 'll take the first \n",
    "# # # THe length of data and labels should be the same/equal\n",
    "# # # Ths is how we are going to devide this:\n",
    "# # # 70% first should be the training\n",
    "# # # for idx in range(0, len(data), 1): # start, stop, step\n",
    "# # #     l = h\n",
    "# # idx=0\n",
    "# # h = X[idx].shape[0] * 0.7\n",
    "# # X[:h].shape\n",
    "# # # #     print(idx, l, h, h-width, Xnorm.shape, data[idx].shape, Xnorm[l:h].shape, width)\n",
    "# # # Xnt.append(Xnorm[l:h-width])\n",
    "# # # Xnv.append(Xnorm[h-width:h])\n",
    "# # # print(Xnt[idx].shape, Xnv[idx].shape, data[idx].shape, width)    \n",
    "# # # print(Xnt[idx].dtype, Xnv[idx].dtype, data[idx].dtype, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 250 is the rounded up version of 233 which is rest+action period for each activity\n",
    "# width, h, Xntrainmb, Xnvalidmb, Yntrainmb, Ynvalidmb = 250, 0, [], [], [], []\n",
    "# for idx in range(Xntrain.shape[0]): # start, stop, step\n",
    "#     #     This is very similar to convolution since this is a minibatching technique\n",
    "#     # window size kernel size or minibatch size\n",
    "#     # stride or overlapping window or percentage\n",
    "#     # dilation or or jump=0\n",
    "#     # padding=0\n",
    "#     num_minibatches = ((Xntrain.shape[0] - width)/stride) + 1\n",
    "#     Xntrainmb.append(Xntrain[idx:idx+width])\n",
    "#     print(Xnt[idx].dtype, Xnv[idx].dtype, data[idx].dtype, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is scanning through the signal and extracting the minibatches\n",
    "# # TO be accurate and having more samples, I want to scan it with the \n",
    "# # window size=250, overlap/stride=1, no padding\n",
    "# # Xnt-minibatches, only test one of them\n",
    "# Xnt[0].shape, width, Xnt[0].shape[0]-width+1, Xnt[1].shape[0]-width+1\n",
    "# # total number of minibatches extracted from each sample\n",
    "# num_mb = 0\n",
    "# for each in range(len(Xnt)):\n",
    "# #     print(Xnt[each].shape[0]-width+1)\n",
    "#     num_mb += Xnt[each].shape[0]-width+1\n",
    "# print(num_mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's try to extract batches/minibatches from the first set of training data\n",
    "# # The number of minibatches are equal to\n",
    "# # This is the number of minibatches in the first set\n",
    "# num_mb = Xnt[0].shape[0]-width+1\n",
    "# # this the python list for holding all the minibatches inside\n",
    "# mb=[]\n",
    "# for each in range(num_mb):\n",
    "#     #it has to start from 0+step:width+step/stride and sweeping the entire signal\n",
    "# #     for stride in range()\n",
    "# # The number of minibatches should be equal to the length of the list\n",
    "#     mb.append(Xnt[0][each:each+width, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let s double check the minibatches list length and the estimated number of minibatches\n",
    "# len(mb), num_mb, mb[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_mb = Xnt[1].shape[0]-width+1\n",
    "# for each in range(num_mb):\n",
    "#     mb.append(Xnt[1][each:each+width, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_mb, len(mb), 7888+696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_mb = Xnt[2].shape[0]-width+1\n",
    "# for each in range(num_mb):\n",
    "#     mb.append(Xnt[2][each:each+width, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_mb, len(mb), 7888+696+1884"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is very much like a convolution for extracting the windows\n",
    "# size/width, stride/overlap, padding, dilation, num filters/out channel\n",
    "def minibatching(X, Y, stride, width):\n",
    "    Xmb, Ymb = [], []\n",
    "#     print(len(X), len(Y))\n",
    "    for eachX in range(len(X)):\n",
    "        num_mb = ((X[eachX].shape[0]-width)//stride)+1\n",
    "        for each in range(num_mb):\n",
    "            # The max is (num_mb-1)*stride+width==X[idx].shape[0]\n",
    "            # The last each is (num_mb-1)\n",
    "            # each = ((each-1)*stride)+width\n",
    "            each *= stride\n",
    "            Xmb.append(X[eachX][each:each+width])\n",
    "            # There is only one label for one image signal or signal window or temporal window\n",
    "            Ymb.append(Y[eachX][each:each+1])\n",
    "    return Xmb, Ymb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride, width = 1, 250\n",
    "Xmb, Ymb = minibatching(X=Xntrain, Y=Yntrain, stride=width, width=width)\n",
    "# Xnvalid_mb = minibatching(X=Xnvalid, stride=stride, width=width)\n",
    "# Xntest_mb = minibatching(X=Xtest, stride=stride, width=width)\n",
    "# for eachX, eachY in zip(Xmb, Ymb):\n",
    "#     print(eachX.shape, eachY.shape)\n",
    "# len(Xmb), len(Ymb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(X, Y, batch_size):\n",
    "    \"\"\" Return a generator for batches \"\"\"\n",
    "    n_batches = len(X) // batch_size\n",
    "    X = X[:n_batches*batch_size]\n",
    "    Y = Y[:n_batches*batch_size]\n",
    "\n",
    "#     To understand what yield does, let's implement the equivalent of it;\n",
    "#     # Loop over batches and yield\n",
    "#     for b in range(0, len(X), batch_size):\n",
    "#         yield X[b:b+batch_size], Y[b:b+batch_size]\n",
    "    Xbatch, Ybatch = [], []\n",
    "    for b in range(0, len(X), batch_size):\n",
    "        Xbatch.append(X[b:b+batch_size]) \n",
    "        Ybatch.append(Y[b:b+batch_size])\n",
    "    return Xbatch, Ybatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_batches(X=Xmb, Y=Ymb, batch_size=10)\n",
    "\n",
    "for eachX, eachY in zip(X, Y):\n",
    "    print(len(eachX), len(eachY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_batches(X, y, batch_size = 100):\n",
    "# \t\"\"\" Return a generator for batches \"\"\"\n",
    "# \tn_batches = len(X) // batch_size\n",
    "# \tX, y = X[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "\n",
    "# \t# Loop over batches and yield\n",
    "# \tfor b in range(0, len(X), batch_size):\n",
    "# \t\tyield X[b:b+batch_size], y[b:b+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y = get_batches(X=Xmb, batch_size=100, y=Ymb)\n",
    "\n",
    "# # for each in X:\n",
    "# #     print(len(each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
