{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fNIRS data for Human Activity Recognition (HAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mP12-4-17-2018\u001b[0m/  \u001b[01;34mP14-4-18-2018\u001b[0m/  \u001b[01;34mP16-4-18-2018\u001b[0m/\r\n",
      "\u001b[01;34mP13-4-17-2018\u001b[0m/  \u001b[01;34mP15-4-18-2018\u001b[0m/  \u001b[01;34mP17-4-18-2018\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "% ls ../../../datasets/fNIRs_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m1. Right Hand\u001b[0m/  \u001b[01;34m2. Both Hands\u001b[0m/  \u001b[01;34m3. Left Hand\u001b[0m/  \u001b[01;34m4. Right Leg\u001b[0m/  \u001b[01;34m5. Left Leg\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "% ls ../../../datasets/fNIRs_data/P12-4-17-2018/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m2018-04-17_006\u001b[0m/\r\n",
      "fNIR_data.txt\r\n",
      "head20180417-145130.txt\r\n",
      "NIRS-2018-04-17_006_deoxyhb_T141to2511_C1to20.txt\r\n",
      "NIRS-2018-04-17_006_oxyhb_T141to2511_C1to20.txt\r\n",
      "\u001b[01;34mProcessed\u001b[0m/\r\n",
      "r_hand20180417-145128.txt\r\n",
      "r_lower_arm20180417-145129.txt\r\n",
      "r_upper_arm20180417-145129.txt\r\n"
     ]
    }
   ],
   "source": [
    "% ls ../../../datasets/fNIRs_data/P12-4-17-2018/1.\\ Right\\ Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/5. Left Leg/fNIR_data.txt'],\n",
       " 30)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# % find ../../datasets/fNIRs_data/ | grep fNIR_data # NOT WORKING!!\n",
    "def find_all(name, path):\n",
    "    result = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if name in files:\n",
    "            result.append(os.path.join(root, name))\n",
    "    return result\n",
    "\n",
    "allpaths = find_all(name='fNIR_data.txt', path='/home/arasdar/datasets/fNIRs_data/')\n",
    "allpaths = sorted(allpaths, reverse=False) \n",
    "# print(allpaths, len(allpaths))\n",
    "allpaths, len(allpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/arasdar-DL-env/lib/python3.6/site-packages/pandas/io/parsers.py:709: UserWarning: Duplicate names specified. This will raise an error in the future.\n",
      "  return _read(filepath_or_buffer, kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2372, 42)\n",
      "(1210, 42)\n",
      "(2378, 42)\n",
      "(1202, 42)\n",
      "(1222, 42)\n",
      "(2405, 42)\n",
      "(1196, 42)\n",
      "(2380, 42)\n",
      "(1203, 42)\n",
      "(1242, 42)\n",
      "(2373, 42)\n",
      "(1202, 42)\n",
      "(2386, 42)\n",
      "(1196, 42)\n",
      "(1229, 42)\n",
      "(2387, 42)\n",
      "(1224, 42)\n",
      "(2379, 42)\n",
      "(1230, 42)\n",
      "(1227, 42)\n",
      "(2384, 42)\n",
      "(1230, 42)\n",
      "(2375, 42)\n",
      "(1196, 42)\n",
      "(1197, 42)\n",
      "(2373, 42)\n",
      "(1220, 42)\n",
      "(2372, 42)\n",
      "(1223, 42)\n",
      "(1222, 42)\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# df: data frame object\n",
    "df = []\n",
    "for each_idx in range(len(allpaths)):\n",
    "    file = pd.read_csv(filepath_or_buffer=allpaths[each_idx], names=['time', 'sample', \n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel'],\n",
    "                         header=None)\n",
    "    df.append(file)\n",
    "    \n",
    "for each in range(len(df)):\n",
    "    print(df[each].shape)\n",
    "    df[each]=df[each].drop(axis=1, columns=None, index=None, labels=['time', 'sample'])\n",
    "    df[each] = df[each].dropna()\n",
    "    df[each]['channel.39'] = df[each]['channel.39'].astype(str).str[1:-1].astype(float)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64 (2371, 40) 1\n",
      "float64 (1209, 40) 2\n",
      "float64 (2377, 40) 3\n",
      "float64 (1201, 40) 4\n",
      "float64 (1221, 40) 5\n",
      "float64 (2404, 40) 1\n",
      "float64 (1195, 40) 2\n",
      "float64 (2379, 40) 3\n",
      "float64 (1202, 40) 4\n",
      "float64 (1241, 40) 5\n",
      "float64 (2372, 40) 1\n",
      "float64 (1201, 40) 2\n",
      "float64 (2385, 40) 3\n",
      "float64 (1195, 40) 4\n",
      "float64 (1228, 40) 5\n",
      "float64 (2386, 40) 1\n",
      "float64 (1223, 40) 2\n",
      "float64 (2378, 40) 3\n",
      "float64 (1229, 40) 4\n",
      "float64 (1226, 40) 5\n",
      "float64 (2383, 40) 1\n",
      "float64 (1229, 40) 2\n",
      "float64 (2374, 40) 3\n",
      "float64 (1195, 40) 4\n",
      "float64 (1196, 40) 5\n",
      "float64 (2372, 40) 1\n",
      "float64 (1219, 40) 2\n",
      "float64 (2371, 40) 3\n",
      "float64 (1222, 40) 4\n",
      "float64 (1221, 40) 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(30, 30)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why not working? can not convert obj to arr???\n",
    "data, labels = [], []\n",
    "for each in range(0, len(df), 1):\n",
    "    dfmat = df[each].as_matrix()\n",
    "    label=(each%5)+1\n",
    "    print(dfmat.dtype, dfmat.shape, label)\n",
    "    data.append(dfmat)\n",
    "    labels.append(label)\n",
    "len(data), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is very much like a convolution for extracting the windows\n",
    "# size/width, stride/overlap, padding, dilation, num filters/out channel\n",
    "def minibatching(X, Y, stride, width):\n",
    "    Xmb, Ymb = [], []\n",
    "    print(len(X), len(Y))\n",
    "    # 1st and 1st\n",
    "    for eachX in range(len(X)):\n",
    "        num_mb = ((X[eachX].shape[0]-width)//stride)+1\n",
    "        for each in range(num_mb):\n",
    "            # The max is (num_mb-1)*stride+width==X[idx].shape[0]\n",
    "            # The last each is (num_mb-1)\n",
    "            # each = ((each-1)*stride)+width\n",
    "            each *= stride\n",
    "            Xmb.append(X[eachX][each:each+width])\n",
    "            # There is only one label for one image signal or signal window or temporal window\n",
    "            #Ymb.append(Y[eachX][each:each+1])\n",
    "            Ymb.append(Y[eachX])\n",
    "    return Xmb, Ymb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 30\n",
      "42935 42935\n",
      "(250, 40) float64\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "width = 250\n",
    "Xmb, Ymb = minibatching(X=data, Y=labels, stride=1, width=width)\n",
    "# for eachX, eachY in zip(Xmb, Ymb):\n",
    "#     print(eachX.shape, eachY)\n",
    "print(len(Xmb), len(Ymb))\n",
    "print(Xmb[0].shape, Xmb[0].dtype)\n",
    "print(Ymb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42935, 250, 40) float64 (42935,) int64\n"
     ]
    }
   ],
   "source": [
    "# Conversion from python list to numpy array\n",
    "X, Y=np.array(object=Xmb, dtype=float), np.array(object=Ymb, dtype=int)\n",
    "print(X.shape, X.dtype, Y.shape, Y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30054, 250, 40) (12881, 250, 40) (30054,) (12881,)\n",
      "float64 float64 int64 int64\n"
     ]
    }
   ],
   "source": [
    "# Now I should devide the data into train and test\n",
    "# Train and valid split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 30% of the training data/ entire training data is assigned to validation.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.30)\n",
    "print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)\n",
    "print(Xtrain.dtype, Xtest.dtype, Ytrain.dtype, Ytest.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30054, 250, 40) float64\n",
      "(12881, 250, 40) float64\n"
     ]
    }
   ],
   "source": [
    "# # standardizing/normalizing the train and test data\n",
    "# def standardize(train, test):\n",
    "# \t\"\"\" Standardize data \"\"\"\n",
    "\n",
    "# \t# Standardize train and test\n",
    "# \tX_train = (train - np.mean(train, axis=0)[None,:,:]) / np.std(train, axis=0)[None,:,:]\n",
    "# \tX_test = (test - np.mean(test, axis=0)[None,:,:]) / np.std(test, axis=0)[None,:,:]\n",
    "\n",
    "# \treturn X_train, X_test\n",
    "\n",
    "Xtrain = (Xtrain-Xtrain.mean(axis=0))/Xtrain.std(axis=0)\n",
    "Xtest = (Xtest-Xtest.mean(axis=0))/Xtest.std(axis=0)\n",
    "print(Xtrain.shape, Xtrain.dtype)\n",
    "print(Xtest.shape, Xtest.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30054, 5) float64 (12881, 5) float64\n"
     ]
    }
   ],
   "source": [
    "# Onehotencoding of the output labels\n",
    "def onehot(labels, n_class):\n",
    "\t\"\"\" One-hot encoding \"\"\"\n",
    "\texpansion = np.eye(n_class)\n",
    "\ty = expansion[:, labels-1].T\n",
    "\tassert y.shape[1] == n_class, \"Wrong number of labels!\"\n",
    "\n",
    "\treturn y\n",
    "\n",
    "# print(Y.shape, Y.dtype, Y.max(axis=0), Ytrain.max(axis=0), Ytest.max(axis=0))\n",
    "# # assert Y.max(axis=0) == Ytrain.max(axis=0) == Ytest.max(axis=0), 'wrong labels'\n",
    "\n",
    "Ytrain=onehot(labels=Ytrain, n_class=Ytrain.max(axis=0))\n",
    "Ytest=onehot(labels=Ytest, n_class=Ytest.max(axis=0))\n",
    "print(Ytrain.shape, Ytrain.dtype, Ytest.shape, Ytest.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21037, 250, 40) (9017, 250, 40) (12881, 250, 40) float64 float64 float64\n",
      "(21037, 5) (9017, 5) (12881, 5) float64 float64 float64\n"
     ]
    }
   ],
   "source": [
    "# Now separating train and validation set\n",
    "# 30% of the training data/ entire training data is assigned to validation.\n",
    "Xtrain, Xvalid, Ytrain, Yvalid = train_test_split(Xtrain, Ytrain, test_size=0.30)\n",
    "print(Xtrain.shape, Xvalid.shape, Xtest.shape, Xtrain.dtype, Xvalid.dtype, Xtest.dtype)\n",
    "print(Ytrain.shape, Yvalid.shape, Ytest.shape, Ytrain.dtype, Yvalid.dtype, Ytest.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9017, 250, 40) <dtype: 'float32'> (21037, 250, 40) float64 (9017, 250, 40) float64 (12881, 250, 40) float64\n"
     ]
    }
   ],
   "source": [
    "# now I can design the actual input and output tensors\n",
    "N, W, Cin = Xvalid.shape[0], Xvalid.shape[1], Xvalid.shape[2]\n",
    "X = tf.placeholder(dtype=tf.float32, name=None, shape=[N, W, Cin])\n",
    "print(X.shape, X.dtype, Xtrain.shape, Xtrain.dtype, Xvalid.shape, Xvalid.dtype, Xtest.shape, Xtest.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9017, 5) <dtype: 'float32'> (21037, 5) float64 (9017, 5) float64 (12881, 5) float64\n"
     ]
    }
   ],
   "source": [
    "# This is the output tensor for labels\n",
    "N, Cout = Yvalid.shape[0], Yvalid.shape[1]\n",
    "Y = tf.placeholder(dtype=tf.float32, name=None, shape=[N, Cout])\n",
    "print(Y.shape, Y.dtype, Ytrain.shape, Ytrain.dtype, Yvalid.shape, Yvalid.dtype, Ytest.shape, Ytest.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9017, 250, 40) <dtype: 'float32'>\n",
      "(125, 40, 80) <dtype: 'float32_ref'>\n",
      "(9017, 125, 80) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, X.dtype)\n",
    "Wwidth, Wchannels, Wnumber = X.shape[1].value//2, X.shape[2].value, X.shape[2].value*2\n",
    "shape = [Wwidth, Wchannels, Wnumber]\n",
    "initial_value=tf.random_normal(dtype=X.dtype, mean=0.0, name=None, shape=shape, stddev=1.0)\n",
    "Wconv = tf.Variable(dtype=X.dtype, initial_value=initial_value, name=None, trainable=True)\n",
    "print(Wconv.shape, Wconv.dtype)\n",
    "Xconv = tf.nn.conv1d(data_format='NWC', filters=Wconv, name=None, padding='SAME', stride=2, use_cudnn_on_gpu=True, \n",
    "                     value=X)\n",
    "Xconv = tf.maximum(name=None, x=-0.1*Xconv, y=Xconv)\n",
    "print(Xconv.shape, Xconv.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9017, 10000) <dtype: 'float32'>\n",
      "(10000, 5) <dtype: 'float32_ref'>\n",
      "(9017, 5) <dtype: 'float32'>\n",
      "(9017, 5) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "# This is the multiplication layer\n",
    "# this part is flatening the input\n",
    "shape = [Xconv.shape[0].value, Xconv.shape[1].value*Xconv.shape[2].value]\n",
    "Xconv_reshaped = tf.reshape(name=None, shape=shape, tensor=Xconv)\n",
    "print(Xconv_reshaped.shape, Xconv_reshaped.dtype)\n",
    "# their first axis or dimension stay the same\n",
    "shape = [Xconv_reshaped.shape[1].value, Y.shape[1].value]\n",
    "initial_value = tf.random_normal(dtype=Xconv_reshaped.dtype, mean=0.0, name=None, shape=shape, stddev=1.0)\n",
    "W = tf.Variable(dtype=Xconv_reshaped.dtype, initial_value=initial_value, name=None, trainable=True)\n",
    "print(W.shape, W.dtype)\n",
    "# The actual multiplication\n",
    "# Y_ = Xconv_reshaped@W\n",
    "Y_ = tf.matmul(a=Xconv_reshaped, b=W, name=None)\n",
    "print(Y_.shape, Y_.dtype)\n",
    "print(Y.shape, Y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9017,) <dtype: 'float32'>\n",
      "Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Now I need to calculate the loss\n",
    "loss_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=Y_, name=None)\n",
    "print(loss_tensor.shape, loss_tensor.dtype)\n",
    "loss = tf.reduce_mean(axis=0, input_tensor=loss_tensor, name=None)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"Adam_1\"\n",
      "op: \"NoOp\"\n",
      "input: \"^Adam_1/update_Variable_3/ApplyAdam\"\n",
      "input: \"^Adam_1/update_Variable_4/ApplyAdam\"\n",
      "input: \"^Adam_1/Assign\"\n",
      "input: \"^Adam_1/Assign_1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Backprop and SGD now using adam\n",
    "opt = tf.train.AdamOptimizer().minimize(loss)\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(X, Y, batch_size):\n",
    "    \"\"\" Return a generator for batches \"\"\"\n",
    "    n_batches = len(X) // batch_size\n",
    "    X, Y = X[:n_batches*batch_size], Y[:n_batches*batch_size]\n",
    "\n",
    "    # Loop over batches and yield\n",
    "    for b in range(0, len(X), batch_size):\n",
    "        yield X[b:b+batch_size], Y[b:b+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss: 3874.2515 valid_loss: 2774.6213\n",
      "epoch: 1 train_loss: 3202.841 valid_loss: 2434.7798\n",
      "epoch: 2 train_loss: 2797.827 valid_loss: 2181.8523\n",
      "epoch: 3 train_loss: 2478.0923 valid_loss: 1929.2441\n",
      "epoch: 4 train_loss: 2195.7578 valid_loss: 1725.6342\n",
      "epoch: 5 train_loss: 1977.8629 valid_loss: 1585.1162\n",
      "epoch: 6 train_loss: 1817.8864 valid_loss: 1474.4725\n",
      "epoch: 7 train_loss: 1687.0972 valid_loss: 1375.8387\n",
      "epoch: 8 train_loss: 1570.9487 valid_loss: 1280.8412\n",
      "epoch: 9 train_loss: 1461.9316 valid_loss: 1195.5887\n",
      "epoch: 10 train_loss: 1366.5676 valid_loss: 1116.7451\n",
      "epoch: 11 train_loss: 1278.4269 valid_loss: 1045.6666\n",
      "epoch: 12 train_loss: 1200.5681 valid_loss: 987.24915\n",
      "epoch: 13 train_loss: 1136.1434 valid_loss: 938.2145\n",
      "epoch: 14 train_loss: 1079.7233 valid_loss: 891.92224\n",
      "epoch: 15 train_loss: 1026.7909 valid_loss: 848.342\n",
      "epoch: 16 train_loss: 977.5639 valid_loss: 809.0359\n",
      "epoch: 17 train_loss: 933.097 valid_loss: 773.52155\n",
      "epoch: 18 train_loss: 892.7174 valid_loss: 741.5409\n",
      "epoch: 19 train_loss: 856.07336 valid_loss: 711.59436\n",
      "epoch: 20 train_loss: 821.7887 valid_loss: 683.7964\n",
      "epoch: 21 train_loss: 790.19336 valid_loss: 658.5242\n",
      "epoch: 22 train_loss: 761.2964 valid_loss: 635.08435\n",
      "epoch: 23 train_loss: 734.3536 valid_loss: 613.03265\n",
      "epoch: 24 train_loss: 709.12695 valid_loss: 592.6027\n",
      "epoch: 25 train_loss: 685.766 valid_loss: 573.68646\n",
      "epoch: 26 train_loss: 664.0479 valid_loss: 555.9234\n",
      "epoch: 27 train_loss: 643.6271 valid_loss: 539.1788\n",
      "epoch: 28 train_loss: 624.40485 valid_loss: 523.468\n",
      "epoch: 29 train_loss: 606.3482 valid_loss: 508.7251\n",
      "epoch: 30 train_loss: 589.3547 valid_loss: 494.79626\n",
      "epoch: 31 train_loss: 573.3082 valid_loss: 481.66492\n",
      "epoch: 32 train_loss: 558.1867 valid_loss: 469.267\n",
      "epoch: 33 train_loss: 543.89795 valid_loss: 457.52377\n",
      "epoch: 34 train_loss: 530.3591 valid_loss: 446.40375\n",
      "epoch: 35 train_loss: 517.52 valid_loss: 435.86047\n",
      "epoch: 36 train_loss: 505.3274 valid_loss: 425.83408\n",
      "epoch: 37 train_loss: 493.72183 valid_loss: 416.27823\n",
      "epoch: 38 train_loss: 482.6615 valid_loss: 407.17688\n",
      "epoch: 39 train_loss: 472.1267 valid_loss: 398.49884\n",
      "epoch: 40 train_loss: 462.0727 valid_loss: 390.20554\n",
      "epoch: 41 train_loss: 452.46088 valid_loss: 382.28397\n",
      "epoch: 42 train_loss: 443.27066 valid_loss: 374.70673\n",
      "epoch: 43 train_loss: 434.47168 valid_loss: 367.44583\n",
      "epoch: 44 train_loss: 426.03693 valid_loss: 360.48187\n",
      "epoch: 45 train_loss: 417.94604 valid_loss: 353.799\n",
      "epoch: 46 train_loss: 410.1792 valid_loss: 347.37848\n",
      "epoch: 47 train_loss: 402.71484 valid_loss: 341.20398\n",
      "epoch: 48 train_loss: 395.5353 valid_loss: 335.26248\n",
      "epoch: 49 train_loss: 388.62518 valid_loss: 329.54028\n",
      "epoch: 50 train_loss: 381.96872 valid_loss: 324.025\n",
      "epoch: 51 train_loss: 375.5517 valid_loss: 318.70578\n",
      "epoch: 52 train_loss: 369.36102 valid_loss: 313.5723\n",
      "epoch: 53 train_loss: 363.3846 valid_loss: 308.61398\n",
      "epoch: 54 train_loss: 357.61108 valid_loss: 303.8214\n",
      "epoch: 55 train_loss: 352.02963 valid_loss: 299.18686\n",
      "epoch: 56 train_loss: 346.6308 valid_loss: 294.70227\n",
      "epoch: 57 train_loss: 341.4055 valid_loss: 290.36008\n",
      "epoch: 58 train_loss: 336.34525 valid_loss: 286.15344\n",
      "epoch: 59 train_loss: 331.44193 valid_loss: 282.0759\n",
      "epoch: 60 train_loss: 326.68817 valid_loss: 278.12125\n",
      "epoch: 61 train_loss: 322.07693 valid_loss: 274.2838\n",
      "epoch: 62 train_loss: 317.6016 valid_loss: 270.55804\n",
      "epoch: 63 train_loss: 313.25592 valid_loss: 266.93896\n",
      "epoch: 64 train_loss: 309.03418 valid_loss: 263.42194\n",
      "epoch: 65 train_loss: 304.9309 valid_loss: 260.00272\n",
      "epoch: 66 train_loss: 300.94113 valid_loss: 256.67728\n",
      "epoch: 67 train_loss: 297.05975 valid_loss: 253.44177\n",
      "epoch: 68 train_loss: 293.2825 valid_loss: 250.29228\n",
      "epoch: 69 train_loss: 289.60507 valid_loss: 247.22505\n",
      "epoch: 70 train_loss: 286.0235 valid_loss: 244.23701\n",
      "epoch: 71 train_loss: 282.534 valid_loss: 241.32498\n",
      "epoch: 72 train_loss: 279.13287 valid_loss: 238.48608\n",
      "epoch: 73 train_loss: 275.81662 valid_loss: 235.71727\n",
      "epoch: 74 train_loss: 272.58188 valid_loss: 233.0157\n",
      "epoch: 75 train_loss: 269.42554 valid_loss: 230.37906\n",
      "epoch: 76 train_loss: 266.34476 valid_loss: 227.80496\n",
      "epoch: 77 train_loss: 263.3368 valid_loss: 225.29094\n",
      "epoch: 78 train_loss: 260.3989 valid_loss: 222.83478\n",
      "epoch: 79 train_loss: 257.52847 valid_loss: 220.43437\n",
      "epoch: 80 train_loss: 254.72328 valid_loss: 218.08784\n",
      "epoch: 81 train_loss: 251.98099 valid_loss: 215.79323\n",
      "epoch: 82 train_loss: 249.29932 valid_loss: 213.54878\n",
      "epoch: 83 train_loss: 246.67632 valid_loss: 211.35275\n",
      "epoch: 84 train_loss: 244.10992 valid_loss: 209.20363\n",
      "epoch: 85 train_loss: 241.5983 valid_loss: 207.09998\n",
      "epoch: 86 train_loss: 239.13968 valid_loss: 205.04016\n",
      "epoch: 87 train_loss: 236.73247 valid_loss: 203.02277\n",
      "epoch: 88 train_loss: 234.37485 valid_loss: 201.04662\n",
      "epoch: 89 train_loss: 232.06537 valid_loss: 199.1104\n",
      "epoch: 90 train_loss: 229.80247 valid_loss: 197.21288\n",
      "epoch: 91 train_loss: 227.58476 valid_loss: 195.35292\n",
      "epoch: 92 train_loss: 225.41093 valid_loss: 193.52925\n",
      "epoch: 93 train_loss: 223.27954 valid_loss: 191.74083\n",
      "epoch: 94 train_loss: 221.18933 valid_loss: 189.98672\n",
      "epoch: 95 train_loss: 219.13905 valid_loss: 188.26593\n",
      "epoch: 96 train_loss: 217.1276 valid_loss: 186.5775\n",
      "epoch: 97 train_loss: 215.1538 valid_loss: 184.9205\n",
      "epoch: 98 train_loss: 213.2166 valid_loss: 183.29413\n",
      "epoch: 99 train_loss: 211.315 valid_loss: 181.69748\n"
     ]
    }
   ],
   "source": [
    "# now that we can calculate loss and optimize, we can start a session for calculating the error.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(fetches=tf.global_variables_initializer())\n",
    "    train_loss_list, valid_loss_list = [], []\n",
    "    \n",
    "    # for every epoch start feeding the arrays into the tensors in the model\n",
    "    for epoch in range(0, 100, 1):\n",
    "        \n",
    "        # Training minibatches and feed them into the tensor\n",
    "        for Xarr, Yarr in get_batches(X=Xtrain, Y=Ytrain, batch_size=Xvalid.shape[0]):\n",
    "            \n",
    "            feed_dict = {X:Xarr, Y:Yarr}\n",
    "            train_loss, _ = sess.run(feed_dict=feed_dict, fetches=[loss, opt])\n",
    "            train_loss_list.append(train_loss)\n",
    "            \n",
    "        # Validation now which is one batch on every iteration\n",
    "        for Xarr, Yarr in get_batches(X=Xvalid, Y=Yvalid, batch_size=Xvalid.shape[0]): \n",
    "            # X_NxWxCin, Y_NxCout\n",
    "            feed_dict = {X:Xarr, Y:Yarr}\n",
    "            valid_loss = sess.run(feed_dict=feed_dict, fetches=[loss])\n",
    "            valid_loss_list.append(valid_loss)\n",
    "        \n",
    "        # printing out train and validation loss\n",
    "        print('epoch:', epoch, 'train_loss:', np.mean(train_loss_list), 'valid_loss:', np.mean(valid_loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
