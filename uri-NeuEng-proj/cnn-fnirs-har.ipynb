{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for fNIRS data analysis for Human Activity Recognition (HAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mP12-4-17-2018\u001b[0m/  \u001b[01;34mP14-4-18-2018\u001b[0m/  \u001b[01;34mP16-4-18-2018\u001b[0m/\r\n",
      "\u001b[01;34mP13-4-17-2018\u001b[0m/  \u001b[01;34mP15-4-18-2018\u001b[0m/  \u001b[01;34mP17-4-18-2018\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "% ls ../../../datasets/fNIRs_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m1. Right Hand\u001b[0m/  \u001b[01;34m2. Both Hands\u001b[0m/  \u001b[01;34m3. Left Hand\u001b[0m/  \u001b[01;34m4. Right Leg\u001b[0m/  \u001b[01;34m5. Left Leg\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "% ls ../../../datasets/fNIRs_data/P12-4-17-2018/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m2018-04-17_006\u001b[0m/\r\n",
      "fNIR_data.txt\r\n",
      "head20180417-145130.txt\r\n",
      "NIRS-2018-04-17_006_deoxyhb_T141to2511_C1to20.txt\r\n",
      "NIRS-2018-04-17_006_oxyhb_T141to2511_C1to20.txt\r\n",
      "\u001b[01;34mProcessed\u001b[0m/\r\n",
      "r_hand20180417-145128.txt\r\n",
      "r_lower_arm20180417-145129.txt\r\n",
      "r_upper_arm20180417-145129.txt\r\n"
     ]
    }
   ],
   "source": [
    "% ls ../../../datasets/fNIRs_data/P12-4-17-2018/1.\\ Right\\ Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/5. Left Leg/fNIR_data.txt'],\n",
       " 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# % find ../../datasets/fNIRs_data/ | grep fNIR_data # NOT WORKING!!\n",
    "def find_all(name, path):\n",
    "    result = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if name in files:\n",
    "            result.append(os.path.join(root, name))\n",
    "    return result\n",
    "\n",
    "allpaths = find_all(name='fNIR_data.txt', path='/home/arasdar/datasets/fNIRs_data/')\n",
    "allpaths = sorted(allpaths, reverse=False) \n",
    "# print(allpaths, len(allpaths))\n",
    "allpaths, len(allpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/arasdar-DL-env/lib/python3.6/site-packages/pandas/io/parsers.py:709: UserWarning: Duplicate names specified. This will raise an error in the future.\n",
      "  return _read(filepath_or_buffer, kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2372, 42)\n",
      "(1210, 42)\n",
      "(2378, 42)\n",
      "(1202, 42)\n",
      "(1222, 42)\n",
      "(2405, 42)\n",
      "(1196, 42)\n",
      "(2380, 42)\n",
      "(1203, 42)\n",
      "(1242, 42)\n",
      "(2373, 42)\n",
      "(1202, 42)\n",
      "(2386, 42)\n",
      "(1196, 42)\n",
      "(1229, 42)\n",
      "(2387, 42)\n",
      "(1224, 42)\n",
      "(2379, 42)\n",
      "(1230, 42)\n",
      "(1227, 42)\n",
      "(2384, 42)\n",
      "(1230, 42)\n",
      "(2375, 42)\n",
      "(1196, 42)\n",
      "(1197, 42)\n",
      "(2373, 42)\n",
      "(1220, 42)\n",
      "(2372, 42)\n",
      "(1223, 42)\n",
      "(1222, 42)\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# df: data frame object\n",
    "df = []\n",
    "for each_idx in range(len(allpaths)):\n",
    "    file = pd.read_csv(filepath_or_buffer=allpaths[each_idx], names=['time', 'sample', \n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel'],\n",
    "                         header=None)\n",
    "    df.append(file)\n",
    "    \n",
    "for each in range(len(df)):\n",
    "    print(df[each].shape)\n",
    "    df[each]=df[each].drop(axis=1, columns=None, index=None, labels=['time', 'sample'])\n",
    "    df[each] = df[each].dropna()\n",
    "    df[each]['channel.39'] = df[each]['channel.39'].astype(str).str[1:-1].astype(float)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64 (2371, 40) 1\n",
      "float64 (1209, 40) 2\n",
      "float64 (2377, 40) 3\n",
      "float64 (1201, 40) 4\n",
      "float64 (1221, 40) 5\n",
      "float64 (2404, 40) 1\n",
      "float64 (1195, 40) 2\n",
      "float64 (2379, 40) 3\n",
      "float64 (1202, 40) 4\n",
      "float64 (1241, 40) 5\n",
      "float64 (2372, 40) 1\n",
      "float64 (1201, 40) 2\n",
      "float64 (2385, 40) 3\n",
      "float64 (1195, 40) 4\n",
      "float64 (1228, 40) 5\n",
      "float64 (2386, 40) 1\n",
      "float64 (1223, 40) 2\n",
      "float64 (2378, 40) 3\n",
      "float64 (1229, 40) 4\n",
      "float64 (1226, 40) 5\n",
      "float64 (2383, 40) 1\n",
      "float64 (1229, 40) 2\n",
      "float64 (2374, 40) 3\n",
      "float64 (1195, 40) 4\n",
      "float64 (1196, 40) 5\n",
      "float64 (2372, 40) 1\n",
      "float64 (1219, 40) 2\n",
      "float64 (2371, 40) 3\n",
      "float64 (1222, 40) 4\n",
      "float64 (1221, 40) 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(30, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, labels = [], []\n",
    "for each in range(0, len(df), 1):\n",
    "    dfmat = df[each].as_matrix()\n",
    "    label = (each%5)+1\n",
    "    print(dfmat.dtype, dfmat.shape, label)\n",
    "    data.append(dfmat)\n",
    "    labels.append(label)\n",
    "len(data), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is very much like a convolution for extracting the windows\n",
    "# size/width, stride/overlap, padding, dilation, num filters/out channel\n",
    "def minibatching(X, Y, stride, width):\n",
    "    Xmb, Ymb = [], []\n",
    "    print(len(X), len(Y))\n",
    "    # 1st and 1st\n",
    "    for eachX in range(len(X)):\n",
    "        num_mb = ((X[eachX].shape[0]-width)//stride)+1\n",
    "        for each in range(num_mb):\n",
    "            # The max is (num_mb-1)*stride+width==X[idx].shape[0]\n",
    "            # The last each is (num_mb-1)\n",
    "            # each = ((each-1)*stride)+width\n",
    "            each *= stride\n",
    "            Xmb.append(X[eachX][each:each+width])\n",
    "            # There is only one label for one image signal or signal window or temporal window\n",
    "            #Ymb.append(Y[eachX][each:each+1])\n",
    "            Ymb.append(Y[eachX])\n",
    "    return Xmb, Ymb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 30\n",
      "42935 42935\n",
      "(250, 40) float64\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Width is based on the sampling rate which is roughly about 233 points per window\n",
    "# for 10sec rest and 20 sec activity\n",
    "width = 250\n",
    "Xmb, Ymb = minibatching(X=data, Y=labels, stride=1, width=width)\n",
    "# for eachX, eachY in zip(Xmb, Ymb):\n",
    "#     print(eachX.shape, eachY)\n",
    "print(len(Xmb), len(Ymb))\n",
    "print(Xmb[0].shape, Xmb[0].dtype)\n",
    "print(Ymb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42935, 250, 40) float64 (42935,) int64\n"
     ]
    }
   ],
   "source": [
    "# Conversion from python list to numpy array\n",
    "X, Y=np.array(object=Xmb, dtype=float), np.array(object=Ymb, dtype=int)\n",
    "print(X.shape, X.dtype, Y.shape, Y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30054, 250, 40) (12881, 250, 40) (30054,) (12881,)\n",
      "float64 float64 int64 int64\n"
     ]
    }
   ],
   "source": [
    "# Now I should devide the data into train and test\n",
    "# Train and valid split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 30% of the training data/ entire training data is assigned to validation.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.30)\n",
    "print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)\n",
    "print(Xtrain.dtype, Xtest.dtype, Ytrain.dtype, Ytest.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30054, 250, 40) float64\n",
      "(12881, 250, 40) float64\n"
     ]
    }
   ],
   "source": [
    "# # standardizing/normalizing the train and test data\n",
    "# def standardize(train, test):\n",
    "# \"\"\" Standardize data \"\"\"\n",
    "# # Standardize train and test\n",
    "# X_train = (train - np.mean(train, axis=0)[None,:,:]) / np.std(train, axis=0)[None,:,:]\n",
    "# X_test = (test - np.mean(test, axis=0)[None,:,:]) / np.std(test, axis=0)[None,:,:]\n",
    "# return X_train, X_test\n",
    "\n",
    "Xtrain = (Xtrain - Xtrain.mean(axis=0))/ Xtrain.std(axis=0)\n",
    "Xtest = (Xtest - Xtest.mean(axis=0))/ Xtest.std(axis=0)\n",
    "print(Xtrain.shape, Xtrain.dtype)\n",
    "print(Xtest.shape, Xtest.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5 5\n",
      "(30054, 5) float64 (12881, 5) float64\n"
     ]
    }
   ],
   "source": [
    "# Onehotencoding of the output labels\n",
    "def onehot(labels, n_class):\n",
    "\t\"\"\" One-hot encoding \"\"\"\n",
    "\texpansion = np.eye(n_class)\n",
    "\ty = expansion[:, labels-1].T\n",
    "\tassert y.shape[1] == n_class, \"Wrong number of labels!\"\n",
    "\n",
    "\treturn y\n",
    "\n",
    "print(Y.max(axis=0), Ytrain.max(axis=0), Ytest.max(axis=0))\n",
    "# # assert Y.max(axis=0) == Ytrain.max(axis=0) == Ytest.max(axis=0), 'wrong labels'\n",
    "Ytrain=onehot(labels=Ytrain, n_class=Ytrain.max(axis=0))\n",
    "Ytest=onehot(labels=Ytest, n_class=Ytest.max(axis=0))\n",
    "print(Ytrain.shape, Ytrain.dtype, Ytest.shape, Ytest.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21037, 250, 40) (9017, 250, 40) (12881, 250, 40) float64 float64 float64\n",
      "(21037, 5) (9017, 5) (12881, 5) float64 float64 float64\n"
     ]
    }
   ],
   "source": [
    "# Now separating train and validation set\n",
    "# 30% of the training data/ entire training data is assigned to validation.\n",
    "Xtrain, Xvalid, Ytrain, Yvalid = train_test_split(Xtrain, Ytrain, test_size=0.30)\n",
    "print(Xtrain.shape, Xvalid.shape, Xtest.shape, Xtrain.dtype, Xvalid.dtype, Xtest.dtype)\n",
    "print(Ytrain.shape, Yvalid.shape, Ytest.shape, Ytrain.dtype, Yvalid.dtype, Ytest.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9017, 250, 40) <dtype: 'float32'> (21037, 250, 40) float64 (9017, 250, 40) float64 (12881, 250, 40) float64\n"
     ]
    }
   ],
   "source": [
    "# now I can design the actual input and output tensors\n",
    "N, W, Cin = Xvalid.shape[0], Xvalid.shape[1], Xvalid.shape[2]\n",
    "X = tf.placeholder(dtype=tf.float32, name=None, shape=[N, W, Cin])\n",
    "print(X.shape, X.dtype, Xtrain.shape, Xtrain.dtype, Xvalid.shape, Xvalid.dtype, Xtest.shape, Xtest.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9017, 5) <dtype: 'float32'> (21037, 5) float64 (9017, 5) float64 (12881, 5) float64\n"
     ]
    }
   ],
   "source": [
    "# This is the output tensor for labels\n",
    "N, Cout = Yvalid.shape[0], Yvalid.shape[1]\n",
    "Y = tf.placeholder(dtype=tf.float32, name=None, shape=[N, Cout])\n",
    "print(Y.shape, Y.dtype, Ytrain.shape, Ytrain.dtype, Yvalid.shape, Yvalid.dtype, Ytest.shape, Ytest.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9017, 250, 40) <dtype: 'float32'>\n",
      "(125, 40, 80) <dtype: 'float32_ref'>\n",
      "(9017, 125, 80) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, X.dtype)\n",
    "Wwidth, Wchannels, Wnumber = X.shape[1].value//2, X.shape[2].value, X.shape[2].value*2\n",
    "shape = [Wwidth, Wchannels, Wnumber]\n",
    "initial_value=tf.random_normal(dtype=X.dtype, mean=0.0, name=None, shape=shape, stddev=1.0)\n",
    "Wconv = tf.Variable(dtype=X.dtype, initial_value=initial_value, name=None, trainable=True)\n",
    "print(Wconv.shape, Wconv.dtype)\n",
    "Xconv = tf.nn.conv1d(data_format='NWC', filters=Wconv, name=None, padding='SAME', stride=2, use_cudnn_on_gpu=True, \n",
    "                     value=X)\n",
    "Xconv = tf.maximum(name=None, x=(-0.1*Xconv), y=Xconv)\n",
    "print(Xconv.shape, Xconv.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9017, 10000) <dtype: 'float32'>\n",
      "(10000, 5) <dtype: 'float32_ref'>\n",
      "(9017, 5) <dtype: 'float32'>\n",
      "(9017, 5) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "# This is the multiplication layer\n",
    "# this part is flatening the input\n",
    "shape = [Xconv.shape[0].value, Xconv.shape[1].value*Xconv.shape[2].value]\n",
    "Xconv_reshaped = tf.reshape(name=None, shape=shape, tensor=Xconv)\n",
    "print(Xconv_reshaped.shape, Xconv_reshaped.dtype)\n",
    "# their first axis or dimension stay the same\n",
    "shape = [Xconv_reshaped.shape[1].value, Y.shape[1].value]\n",
    "initial_value = tf.random_normal(dtype=Xconv_reshaped.dtype, mean=0.0, name=None, shape=shape, stddev=1.0)\n",
    "W = tf.Variable(dtype=Xconv_reshaped.dtype, initial_value=initial_value, name=None, trainable=True)\n",
    "print(W.shape, W.dtype)\n",
    "# The actual multiplication\n",
    "# Y_ = Xconv_reshaped @ W\n",
    "Y_ = tf.matmul(a=Xconv_reshaped, b=W, name=None)\n",
    "print(Y_.shape, Y_.dtype)\n",
    "print(Y.shape, Y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9017,) <dtype: 'float32'>\n",
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Now I need to calculate the loss\n",
    "loss_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=Y_, name=None)\n",
    "print(loss_tensor.shape, loss_tensor.dtype)\n",
    "loss = tf.reduce_mean(axis=0, input_tensor=loss_tensor, name=None)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"Adam\"\n",
      "op: \"NoOp\"\n",
      "input: \"^Adam/update_Variable/ApplyAdam\"\n",
      "input: \"^Adam/update_Variable_1/ApplyAdam\"\n",
      "input: \"^Adam/Assign\"\n",
      "input: \"^Adam/Assign_1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Backprop and SGD now using adam\n",
    "opt = tf.train.AdamOptimizer().minimize(loss)\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(X, Y, batch_size):\n",
    "    \"\"\" Return a generator for batches \"\"\"\n",
    "    n_batches = len(X) // batch_size\n",
    "    X, Y = X[:n_batches*batch_size], Y[:n_batches*batch_size]\n",
    "\n",
    "    # Loop over batches and yield\n",
    "    for b in range(0, len(X), batch_size):\n",
    "        yield X[b:b+batch_size], Y[b:b+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 train_loss: 6044.829 valid_loss: 4639.1323\n",
      "epoch: 2 train_loss: 4480.9287 valid_loss: 3718.0908\n",
      "epoch: 3 train_loss: 3608.4355 valid_loss: 2960.721\n",
      "epoch: 4 train_loss: 2858.021 valid_loss: 2237.9512\n",
      "epoch: 5 train_loss: 2166.218 valid_loss: 1751.6997\n",
      "epoch: 6 train_loss: 1678.8126 valid_loss: 1331.314\n",
      "epoch: 7 train_loss: 1297.9954 valid_loss: 1109.1411\n",
      "epoch: 8 train_loss: 1073.7577 valid_loss: 964.071\n",
      "epoch: 9 train_loss: 912.8506 valid_loss: 873.42267\n",
      "epoch: 10 train_loss: 847.3008 valid_loss: 886.55206\n",
      "epoch: 11 train_loss: 854.18945 valid_loss: 856.419\n",
      "epoch: 12 train_loss: 783.25555 valid_loss: 685.6979\n",
      "epoch: 13 train_loss: 624.46387 valid_loss: 555.0571\n",
      "epoch: 14 train_loss: 514.1279 valid_loss: 475.4181\n",
      "epoch: 15 train_loss: 442.11707 valid_loss: 414.05972\n",
      "epoch: 16 train_loss: 391.08157 valid_loss: 371.60388\n",
      "epoch: 17 train_loss: 354.35968 valid_loss: 360.4485\n",
      "epoch: 18 train_loss: 336.1776 valid_loss: 301.12637\n",
      "epoch: 19 train_loss: 273.9873 valid_loss: 247.11914\n",
      "epoch: 20 train_loss: 232.45996 valid_loss: 213.65565\n",
      "epoch: 21 train_loss: 207.9031 valid_loss: 197.79654\n",
      "epoch: 22 train_loss: 192.79002 valid_loss: 184.39633\n",
      "epoch: 23 train_loss: 172.68304 valid_loss: 173.10577\n",
      "epoch: 24 train_loss: 164.9942 valid_loss: 169.19554\n",
      "epoch: 25 train_loss: 155.41351 valid_loss: 148.80656\n",
      "epoch: 26 train_loss: 136.20679 valid_loss: 132.10738\n",
      "epoch: 27 train_loss: 124.37134 valid_loss: 123.863464\n",
      "epoch: 28 train_loss: 120.18864 valid_loss: 118.36229\n",
      "epoch: 29 train_loss: 114.22362 valid_loss: 109.58366\n",
      "epoch: 30 train_loss: 104.987404 valid_loss: 106.098564\n",
      "epoch: 31 train_loss: 100.72133 valid_loss: 103.09617\n",
      "epoch: 32 train_loss: 96.370285 valid_loss: 94.95611\n",
      "epoch: 33 train_loss: 90.11015 valid_loss: 90.73719\n",
      "epoch: 34 train_loss: 87.35005 valid_loss: 88.94752\n",
      "epoch: 35 train_loss: 84.926346 valid_loss: 85.82059\n",
      "epoch: 36 train_loss: 81.44142 valid_loss: 82.89542\n",
      "epoch: 37 train_loss: 78.60814 valid_loss: 80.24598\n",
      "epoch: 38 train_loss: 75.954544 valid_loss: 77.7361\n",
      "epoch: 39 train_loss: 73.7162 valid_loss: 75.27608\n",
      "epoch: 40 train_loss: 71.62986 valid_loss: 72.59432\n",
      "epoch: 41 train_loss: 69.566315 valid_loss: 70.70923\n",
      "epoch: 42 train_loss: 67.81583 valid_loss: 69.04615\n",
      "epoch: 43 train_loss: 65.905975 valid_loss: 67.51574\n",
      "epoch: 44 train_loss: 64.29872 valid_loss: 65.91178\n",
      "epoch: 45 train_loss: 62.805183 valid_loss: 64.20998\n",
      "epoch: 46 train_loss: 61.319305 valid_loss: 62.885895\n",
      "epoch: 47 train_loss: 59.95376 valid_loss: 61.871666\n",
      "epoch: 48 train_loss: 58.70349 valid_loss: 60.451813\n",
      "epoch: 49 train_loss: 57.32201 valid_loss: 59.03689\n",
      "epoch: 50 train_loss: 56.141563 valid_loss: 57.884125\n",
      "epoch: 51 train_loss: 55.066887 valid_loss: 56.819077\n",
      "epoch: 52 train_loss: 54.006653 valid_loss: 55.77832\n",
      "epoch: 53 train_loss: 53.00273 valid_loss: 54.73276\n",
      "epoch: 54 train_loss: 52.004044 valid_loss: 53.786404\n",
      "epoch: 55 train_loss: 51.030518 valid_loss: 52.915154\n",
      "epoch: 56 train_loss: 50.123238 valid_loss: 52.04336\n",
      "epoch: 57 train_loss: 49.234787 valid_loss: 51.139717\n",
      "epoch: 58 train_loss: 48.387733 valid_loss: 50.270184\n",
      "epoch: 59 train_loss: 47.58383 valid_loss: 49.452137\n",
      "epoch: 60 train_loss: 46.78113 valid_loss: 48.679382\n",
      "epoch: 61 train_loss: 45.99511 valid_loss: 47.93397\n",
      "epoch: 62 train_loss: 45.241264 valid_loss: 47.217575\n",
      "epoch: 63 train_loss: 44.50581 valid_loss: 46.488518\n",
      "epoch: 64 train_loss: 43.784153 valid_loss: 45.7252\n",
      "epoch: 65 train_loss: 43.08895 valid_loss: 45.000336\n",
      "epoch: 66 train_loss: 42.416016 valid_loss: 44.33505\n",
      "epoch: 67 train_loss: 41.740196 valid_loss: 43.663685\n",
      "epoch: 68 train_loss: 41.0786 valid_loss: 42.987934\n",
      "epoch: 69 train_loss: 40.437836 valid_loss: 42.376293\n",
      "epoch: 70 train_loss: 39.814796 valid_loss: 41.783127\n",
      "epoch: 71 train_loss: 39.19895 valid_loss: 41.145462\n",
      "epoch: 72 train_loss: 38.599667 valid_loss: 40.532143\n",
      "epoch: 73 train_loss: 38.02629 valid_loss: 40.00377\n",
      "epoch: 74 train_loss: 37.448063 valid_loss: 39.451916\n",
      "epoch: 75 train_loss: 36.871086 valid_loss: 38.846992\n",
      "epoch: 76 train_loss: 36.31385 valid_loss: 38.28475\n",
      "epoch: 77 train_loss: 35.765957 valid_loss: 37.739555\n",
      "epoch: 78 train_loss: 35.22882 valid_loss: 37.185074\n",
      "epoch: 79 train_loss: 34.70513 valid_loss: 36.6808\n",
      "epoch: 80 train_loss: 34.197365 valid_loss: 36.19102\n",
      "epoch: 81 train_loss: 33.708374 valid_loss: 35.698673\n",
      "epoch: 82 train_loss: 33.2384 valid_loss: 35.215336\n",
      "epoch: 83 train_loss: 32.784447 valid_loss: 34.78138\n",
      "epoch: 84 train_loss: 32.34595 valid_loss: 34.37772\n",
      "epoch: 85 train_loss: 31.921568 valid_loss: 33.961205\n",
      "epoch: 86 train_loss: 31.507778 valid_loss: 33.555325\n",
      "epoch: 87 train_loss: 31.109095 valid_loss: 33.16148\n",
      "epoch: 88 train_loss: 30.71352 valid_loss: 32.769714\n",
      "epoch: 89 train_loss: 30.325352 valid_loss: 32.39935\n",
      "epoch: 90 train_loss: 29.94774 valid_loss: 32.05536\n",
      "epoch: 91 train_loss: 29.576431 valid_loss: 31.711025\n",
      "epoch: 92 train_loss: 29.207138 valid_loss: 31.331783\n",
      "epoch: 93 train_loss: 28.8456 valid_loss: 30.963444\n",
      "epoch: 94 train_loss: 28.49419 valid_loss: 30.633705\n",
      "epoch: 95 train_loss: 28.149263 valid_loss: 30.327545\n",
      "epoch: 96 train_loss: 27.806696 valid_loss: 30.016768\n",
      "epoch: 97 train_loss: 27.469019 valid_loss: 29.680523\n",
      "epoch: 98 train_loss: 27.140232 valid_loss: 29.37143\n",
      "epoch: 99 train_loss: 26.81594 valid_loss: 29.056456\n",
      "epoch: 100 train_loss: 26.495584 valid_loss: 28.749424\n"
     ]
    }
   ],
   "source": [
    "# We should save the after training and validation\n",
    "saver = tf.train.Saver() \n",
    "train_loss_mean, valid_loss_mean = [], []\n",
    "\n",
    "# now that we can calculate loss and optimize, we can start a session for calculating the error.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(fetches=tf.global_variables_initializer())\n",
    "    \n",
    "    # for every epoch start feeding the arrays into the tensors in the model\n",
    "    for epoch in range(0, 100, 1):\n",
    "        train_loss, valid_loss = [], []\n",
    "        \n",
    "        # Training minibatches and feed them into the tensor\n",
    "        for Xarr, Yarr in get_batches(X=Xtrain, Y=Ytrain, batch_size=Xvalid.shape[0]):\n",
    "            # X_NxWxCin, Y_NxCout\n",
    "            feed_dict = {X:Xarr, Y:Yarr}\n",
    "            lossarr, _ = sess.run(feed_dict=feed_dict, fetches=[loss, opt])\n",
    "            train_loss.append(lossarr)\n",
    "            \n",
    "        # Validation now which is one batch on every iteration\n",
    "        for Xarr, Yarr in get_batches(X=Xvalid, Y=Yvalid, batch_size=Xvalid.shape[0]): \n",
    "            # X_NxWxCin, Y_NxCout\n",
    "            feed_dict = {X:Xarr, Y:Yarr}\n",
    "            lossarr = sess.run(feed_dict=feed_dict, fetches=[loss])\n",
    "            valid_loss.append(lossarr)\n",
    "        \n",
    "        # printing out train and validation loss\n",
    "        print('epoch:', epoch+1, 'train_loss:', np.mean(train_loss), 'valid_loss:', np.mean(valid_loss))\n",
    "        \n",
    "        # Every epoch, for drawing the plot and their learning curve\n",
    "        train_loss_mean.append(np.mean(train_loss))\n",
    "        valid_loss_mean.append(np.mean(valid_loss))\n",
    "        \n",
    "    # After all epochs and at the end of training and validation\n",
    "    saver.save(sess,'checkpoints/cnn-fnirs-har.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe8e80a2668>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as mplot\n",
    "%matplotlib inline\n",
    "\n",
    "mplot.plot(train_loss_mean, label='train_loss_mean')\n",
    "mplot.plot(valid_loss_mean, label='valid_loss_mean')\n",
    "mplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/cnn-fnirs-har.ckpt\n",
      "test_loss: 28.37745\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(fetches=tf.global_variables_initializer())\n",
    "    \n",
    "    # Loading the trained and validated model\n",
    "    saver.restore(save_path=tf.train.latest_checkpoint(checkpoint_dir='checkpoints/'), sess=sess)\n",
    "    \n",
    "    # Saving the test loss for every batch/minibtch\n",
    "    test_loss = []\n",
    "    \n",
    "    # applying the loaded model on test data\n",
    "    for Xarr, Yarr in get_batches(X=Xtest, Y=Ytest, batch_size=Xvalid.shape[0]): \n",
    "        # X_NxWXCin, Y_NxCout\n",
    "        feed_dict={X:Xarr, Y:Yarr}\n",
    "        lossarr = sess.run(feed_dict=feed_dict, fetches=[loss])\n",
    "        test_loss.append(lossarr)\n",
    "        \n",
    "    # Printing the test loss\n",
    "    print('test_loss:', np.mean(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
