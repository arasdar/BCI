{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fNIRS data for Human Activity Recognition (HAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mP12-4-17-2018\u001b[0m/  \u001b[01;34mP14-4-18-2018\u001b[0m/  \u001b[01;34mP16-4-18-2018\u001b[0m/\r\n",
      "\u001b[01;34mP13-4-17-2018\u001b[0m/  \u001b[01;34mP15-4-18-2018\u001b[0m/  \u001b[01;34mP17-4-18-2018\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "% ls ../../../datasets/fNIRs_data/\n",
    "# % find ../../datasets/fNIRs_data/ | grep fNIR_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m1. Right Hand\u001b[0m/  \u001b[01;34m2. Both Hands\u001b[0m/  \u001b[01;34m3. Left Hand\u001b[0m/  \u001b[01;34m4. Right Leg\u001b[0m/  \u001b[01;34m5. Left Leg\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "% ls ../../../datasets/fNIRs_data/P12-4-17-2018/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m2018-04-17_006\u001b[0m/\r\n",
      "fNIR_data.txt\r\n",
      "head20180417-145130.txt\r\n",
      "NIRS-2018-04-17_006_deoxyhb_T141to2511_C1to20.txt\r\n",
      "NIRS-2018-04-17_006_oxyhb_T141to2511_C1to20.txt\r\n",
      "\u001b[01;34mProcessed\u001b[0m/\r\n",
      "r_hand20180417-145128.txt\r\n",
      "r_lower_arm20180417-145129.txt\r\n",
      "r_upper_arm20180417-145129.txt\r\n"
     ]
    }
   ],
   "source": [
    "% ls ../../../datasets/fNIRs_data/P12-4-17-2018/1.\\ Right\\ Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all(name, path):\n",
    "    result = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if name in files:\n",
    "            result.append(os.path.join(root, name))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P12-4-17-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P13-4-17-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P14-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P15-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P16-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  '/home/arasdar/datasets/fNIRs_data/P17-4-18-2018/5. Left Leg/fNIR_data.txt'],\n",
       " 30)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allpaths = find_all(name='fNIR_data.txt', path='/home/arasdar/datasets/fNIRs_data/')\n",
    "((sorted(allpaths, reverse=False)), len(allpaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/arasdar-DL-env/lib/python3.6/site-packages/pandas/io/parsers.py:709: UserWarning: Duplicate names specified. This will raise an error in the future.\n",
      "  return _read(filepath_or_buffer, kwds)\n"
     ]
    }
   ],
   "source": [
    "# df: data frame object\n",
    "df = []\n",
    "for each_idx in range(len(allpaths)):\n",
    "    df.append(pd.read_csv(filepath_or_buffer=allpaths[each_idx], names=['time', 'sample', \n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/arasdar-DL-env/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/arasdar/anaconda3/envs/arasdar-DL-env/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for each in range(len(df)):\n",
    "    df[each]['sample'][1:] = df[each]['sample'][1:].astype(str).str[2:]\n",
    "    df[each]['channel.39'][1:] = df[each]['channel.39'][1:].astype(str).str[1:-1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = []\n",
    "for each in range(len(df)):\n",
    "    matrices.append(df[each][1:].as_matrix().astype(np.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1195, 5)\n",
      "(2383, 5)\n",
      "(1229, 5)\n",
      "(2374, 5)\n",
      "(1196, 5)\n",
      "(1202, 5)\n",
      "(2404, 5)\n",
      "(1195, 5)\n",
      "(2379, 5)\n",
      "(1241, 5)\n",
      "(1229, 5)\n",
      "(2386, 5)\n",
      "(1223, 5)\n",
      "(2378, 5)\n",
      "(1226, 5)\n",
      "(1201, 5)\n",
      "(2371, 5)\n",
      "(1209, 5)\n",
      "(2377, 5)\n",
      "(1221, 5)\n",
      "(1195, 5)\n",
      "(2372, 5)\n",
      "(1201, 5)\n",
      "(2385, 5)\n",
      "(1228, 5)\n",
      "(1222, 5)\n",
      "(2372, 5)\n",
      "(1219, 5)\n",
      "(2371, 5)\n",
      "(1221, 5)\n"
     ]
    }
   ],
   "source": [
    "# List of data and labels\n",
    "data, labels = [], []\n",
    "for row in range(0, len(matrices), 1):\n",
    "    data.append(matrices[row][:, 2:])\n",
    "    mat = np.zeros([data[row].shape[0], 5], dtype=np.float16)\n",
    "    # for each in range(mat.shape[0]):\n",
    "    mat[:, row%5] = 1\n",
    "    print(mat.shape)\n",
    "    labels.append(mat)\n",
    "# print(data[0].shape, labels[0].shape)\n",
    "# # data[0][1], labels[0][1]\n",
    "# print(len(data), len(labels))\n",
    "# # data[-1][1], labels[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(836, 5) (359, 5)\n"
     ]
    }
   ],
   "source": [
    "# 0.70 percent each data whihc fNIRstxt is train and 0.30 is test\n",
    "# 30% of train data is validation which 21% and ~50% of the data is training.\n",
    "# The total is 50% train, 20% valid, 30% test\n",
    "ratio = 0.7\n",
    "Ltrain, Ltest = [], []\n",
    "Ytrain, Ytest = [], []\n",
    "\n",
    "each=0\n",
    "limit = int(data[each].shape[0]*ratio)\n",
    "Xtrain, Xtest = data[each][:limit], data[each][limit:]\n",
    "Ytrain.append(labels[each][:limit])\n",
    "Ytest.append(labels[each][limit:])\n",
    "Ltrain.append(data[each][:limit].shape[0])\n",
    "Ltest.append(data[each][limit:].shape[0])\n",
    "# print(Xtrain.shape, Xtest.shape) \n",
    "print(Ytrain[each].shape, Ytest[each].shape) \n",
    "# print(Ltrain[each], Ltest[each]) \n",
    "\n",
    "for each in range(1, len(data), 1): # start, stop, step\n",
    "    # each=0\n",
    "    limit = int(data[each].shape[0]*ratio)\n",
    "    Xtrain, Xtest = np.vstack(tup=(Xtrain, data[each][:limit])), np.vstack(tup=(Xtest, data[each][limit:]))\n",
    "    Ltrain.append(data[each][:limit].shape[0])\n",
    "    Ltest.append(data[each][limit:].shape[0])\n",
    "    Ytrain.append(labels[each][:limit])\n",
    "    Ytest.append(labels[each][limit:])\n",
    "#     print(Ytrain[each].shape, Ytest[each].shape) \n",
    "#     print(Xtrain.shape, Xtest.shape) \n",
    "#     print(Ltrain[each], Ltest[each]) \n",
    "#     print(len(Ltrain), len(Ltest)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalizing(X):\n",
    "    Xnorm = (X - X.mean(axis=0))/ X.std(axis=0)\n",
    "#     Xnorm = np.float16(Xnorm)\n",
    "    print(Xnorm.shape, Xnorm.dtype, Xnorm.mean(axis=0), Xnorm.std(axis=0))\n",
    "    return Xnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35269, 40) float16 [-2.5463e-04 -5.4407e-04 -1.8346e-04  3.3331e-04 -6.8665e-04 -6.7329e-04\n",
      "  2.3639e-04 -4.9353e-04 -9.2924e-05 -3.2997e-04 -1.4257e-04 -2.3687e-04\n",
      "  8.2397e-04  4.2295e-04  4.2248e-04 -2.5058e-04  1.0270e-04  6.1274e-04\n",
      "  3.4523e-04 -2.9612e-04 -1.1444e-05 -4.1080e-04  2.1756e-04  8.7857e-05\n",
      " -1.0419e-04 -5.8532e-05 -3.0518e-05  9.9659e-04 -1.4198e-04  1.3947e-04\n",
      " -4.1008e-05  2.4867e-04  2.4033e-04 -4.2963e-04  3.2282e-04  1.3459e-04\n",
      " -7.9679e-04 -1.1975e-04 -5.2309e-04  1.7333e-04] [0.9995 0.9995 0.9995 0.9995 1.     0.9995 1.     1.     1.     1.001\n",
      " 1.     1.     1.     1.     1.     1.     1.     1.     1.     1.\n",
      " 1.     0.9995 1.     1.     0.9995 1.     1.     0.9995 1.     0.9995\n",
      " 0.9995 1.     1.     1.     1.     1.     0.9995 1.     1.     1.    ]\n"
     ]
    }
   ],
   "source": [
    "Xtrain_norm = Normalizing(X=Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15136, 40) float16 [-4.704e-04 -9.990e-05 -1.875e-04  7.410e-04  6.032e-05  4.935e-04\n",
      " -7.200e-05 -1.172e-04 -5.591e-05  1.913e-04 -2.658e-04  8.774e-05\n",
      " -9.151e-04  6.957e-04 -5.713e-04  3.352e-04 -7.927e-06 -2.003e-04\n",
      " -2.497e-05 -1.315e-04  3.369e-04  3.867e-04  1.019e-04 -6.480e-04\n",
      " -7.639e-04 -6.324e-05  3.538e-04  4.458e-04 -3.066e-04 -2.491e-04\n",
      "  2.249e-04 -5.598e-04 -6.618e-04 -8.029e-05  5.537e-05  1.783e-04\n",
      " -6.957e-04 -2.302e-04 -5.178e-04 -2.102e-04] [0.9995 1.     0.999  0.9995 0.9995 0.9995 1.     1.     0.9995 1.\n",
      " 1.     0.9995 0.9995 0.999  1.     1.     1.     0.9995 0.9995 1.\n",
      " 0.9995 1.     1.     0.9995 0.9995 1.     1.     1.     0.9995 0.9995\n",
      " 1.     1.     1.     1.     1.     1.     0.9995 1.     0.9995 1.    ]\n"
     ]
    }
   ],
   "source": [
    "Xtest_norm = Normalizing(X=Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(854, 40) (367, 40)\n",
      "(854, 5) (367, 5)\n",
      "float16 float16\n",
      "30 30\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Xtest = [], []\n",
    "h_train, h_test = 0, 0\n",
    "# print(len(Ltrain), len(Ltest))\n",
    "\n",
    "for each in range(len(Ltrain)): # start, stop, step\n",
    "    # each = 0\n",
    "    l_train = h_train\n",
    "    h_train += Ltrain[each] \n",
    "    l_test = h_test\n",
    "    h_test += Ltest[each]\n",
    "    Xtrain.append(Xtrain_norm[l_train:h_train])\n",
    "    Xtest.append(Xtest_norm[l_test:h_test])\n",
    "print(Xtrain[each].shape, Xtest[each].shape)\n",
    "print(Ytrain[each].shape, Ytest[each].shape) \n",
    "print(Xtrain[each].dtype, Xtest[each].dtype)\n",
    "print(len(Xtrain), len(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sr = 0.129 # 111933.573-111933.504 is the difference between each two samples or sampling rate\n",
    "# each_trial=30 # 30seconds=20sec+10sec\n",
    "# # num_samples_per_trial\n",
    "# # each trial or epoch based on BCI course\n",
    "# # height or window size\n",
    "# 30/0.129, np.floor(30/0.129), np.ceil(30/0.129)\n",
    "# # The size of each image for convnet will be 233x40x1 which is equal to the format NHWC, txn or NWC\n",
    "# # NWC is for signals since all the channels are projected the same as all channels in an image as well.\n",
    "# # In each convolutionn for image or signals the channels are all included for filtering\n",
    "# # Image or video might be part of this\n",
    "# # Image is NHWC or NCHW\n",
    "# # Signal is NWC or NCW\n",
    "# # N is the number of trials/epochs/windows\n",
    "# # C is the number of channels is 40 in this experiment: 20 for hemoglobin with o2 and 20 for hemoglobin with co2\n",
    "# # W is the window width size or signal window size which is 233\n",
    "# width = np.ceil(30/0.129)\n",
    "# # This is the number of minibatches per file\n",
    "# num_mb = mat.shape[0] - width +1\n",
    "# # num_mb # number of minibatches\n",
    "# # totla number of all the windows with the overlapping windows of 1 sample\n",
    "# num_mb, width\n",
    "# # len(allpaths), allpaths[44], num_mb*len(allpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = Ltrain\n",
    "Xntrain, Xnvalid = [], []\n",
    "Yntrain, Ynvalid = [], []\n",
    "for each in range(0, len(size), 1):\n",
    "    limit = int(size[each]*ratio)\n",
    "#     print(limit, size[each]-limit)\n",
    "    Xntrain.append(Xtrain[each][:limit])\n",
    "#     print(Xntrain[each].shape)\n",
    "    Xnvalid.append(Xtrain[each][limit:])\n",
    "#     print(Xnvalid[each].shape)\n",
    "#     print(Xtrain[each].shape, Xntrain[each].shape, Xnvalid[each].shape)\n",
    "#     print(Xtrain[each].dtype, Xntrain[each].dtype, Xnvalid[each].dtype)\n",
    "    Yntrain.append(Ytrain[each][:limit])\n",
    "#     print(Xntrain[each].shape)\n",
    "    Ynvalid.append(Ytrain[each][limit:])\n",
    "#     print(Xnvalid[each].shape)\n",
    "#     print(Ytrain[each].shape, Yntrain[each].shape, Ynvalid[each].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is very much like a convolution for extracting the windows\n",
    "# size/width, stride/overlap, padding, dilation, num filters/out channel\n",
    "def minibatching(X, Y, stride, width):\n",
    "    Xmb, Ymb = [], []\n",
    "#     print(len(X), len(Y))\n",
    "    for eachX in range(len(X)):\n",
    "        num_mb = ((X[eachX].shape[0]-width)//stride)+1\n",
    "        for each in range(num_mb):\n",
    "            # The max is (num_mb-1)*stride+width==X[idx].shape[0]\n",
    "            # The last each is (num_mb-1)\n",
    "            # each = ((each-1)*stride)+width\n",
    "            each *= stride\n",
    "            Xmb.append(X[eachX][each:each+width])\n",
    "            # There is only one label for one image signal or signal window or temporal window\n",
    "            Ymb.append(Y[eachX][each:each+1])\n",
    "    # Converting python list to numpy array\n",
    "    # It is very good to remember this order: Python list Xlist, NumPy array Xarr, and TensorFlow tensor Xtensor\n",
    "    # Xlist, Xarr, Xtensor\n",
    "#     Xmb = np.array(Xmb, dtype=np.float16)\n",
    "#     Ymb = np.array(Ymb, dtype=np.int8).reshape(len(Ymb), -1)\n",
    "    return Xmb, Ymb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 (250, 40) float16 float16\n",
      "84 (1, 5) float16\n"
     ]
    }
   ],
   "source": [
    "width = 250\n",
    "stride = width\n",
    "Xmbtrain, Ymbtrain = minibatching(X=Xntrain, Y=Yntrain, stride=stride, width=width)\n",
    "Xmbvalid, Ymbvalid = minibatching(X=Xnvalid, Y=Ynvalid, stride=stride, width=width)\n",
    "Xmbtest, Ymbtest = minibatching(X=Xtest, Y=Ytest, stride=stride, width=width)\n",
    "# print(Xmbtest.shape, Xmbtrain.shape, Xmbvalid.shape)\n",
    "# print(Ymbtest.shape, Ymbtrain.shape, Ymbvalid.shape)\n",
    "print(len(Xmbtrain), Xmbtrain[0].shape, Xmbtrain[0].dtype, np.float16(Xmbtrain[0]).dtype)\n",
    "print(len(Ymbtrain), Ymbtrain[0].shape, Ymbtrain[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 250 40\n",
      "(84, 250, 40) <dtype: 'float16'>\n",
      "(84, 1, 5) <dtype: 'float16'>\n"
     ]
    }
   ],
   "source": [
    "# N, W, C = Xmbtrain.shape[0], Xmbtrain.shape[1], Xmbtrain.shape[2]\n",
    "N, W, C = len(Xmbtrain), Xmbtrain[0].shape[0], Xmbtrain[0].shape[1]\n",
    "print(N, W, C)\n",
    "Xin = tf.placeholder(dtype=tf.float16, name=None, shape=[N, W, C])\n",
    "Yin = tf.placeholder(dtype=tf.float16, name=None, shape=[N, 1, 5])\n",
    "print(Xin.shape, Xin.dtype)\n",
    "print(Yin.shape, Yin.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 250, 40) <dtype: 'float16'>\n",
      "(125, 40, 80) <dtype: 'float16_ref'>\n",
      "(84, 50, 80) <dtype: 'float16'>\n"
     ]
    }
   ],
   "source": [
    "# Conv layer\n",
    "print(Xin.shape, Xin.dtype)\n",
    "Ww, Wc, Wn = Xin.shape[1].value//2, Xin.shape[2].value, Xin.shape[2].value*2\n",
    "shape = [Ww, Wc, Wn]\n",
    "init_val = tf.random_normal(dtype=tf.float16, mean=0.0, name=None, shape=shape, stddev=1.0)\n",
    "W1 = tf.Variable(dtype=tf.float16, initial_value=init_val, name=None, trainable=True)\n",
    "print(W1.shape, W1.dtype)\n",
    "Xconv = tf.nn.conv1d(data_format='NWC', filters=W1, name=None, padding='SAME', stride=5, use_cudnn_on_gpu=True, \n",
    "                     value=Xin)\n",
    "Xconv = tf.nn.relu(features=Xconv, name=None)\n",
    "print(Xconv.shape, Xconv.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 50, 80) <dtype: 'float16'>\n",
      "(84, 1, 5) <dtype: 'float16'>\n",
      "(4000, 5) <dtype: 'float16_ref'>\n",
      "[84, 4000]\n",
      "(84, 4000)\n",
      "(84, 5) <dtype: 'float16'>\n"
     ]
    }
   ],
   "source": [
    "# Multiplication layer\n",
    "print(Xconv.shape, Xconv.dtype)\n",
    "print(Yin.shape, Yin.dtype)\n",
    "# Xin=Xout\n",
    "Ww, Wc, Wn = Xconv.shape[1].value, Xconv.shape[2].value, Yin.shape[2].value\n",
    "shape = [Ww*Wc, Wn]\n",
    "init_val = tf.random_normal(dtype=tf.float16, mean=0.0, name=None, shape=shape, stddev=1.0)\n",
    "W4 = tf.Variable(dtype=tf.float16, initial_value=init_val, name=None, trainable=True)\n",
    "print(W4.shape, W4.dtype)\n",
    "N, W, C = Xconv.shape[0].value, Xconv.shape[1].value, Xconv.shape[2].value\n",
    "shape = [N, W*C]\n",
    "print(shape)\n",
    "Xconv_reshaped = tf.reshape(name=None, shape=shape, tensor=Xconv)\n",
    "print(Xconv_reshaped.shape)\n",
    "Xlogit = tf.matmul(a=Xconv_reshaped, b=W4, name=None)\n",
    "print(Xlogit.shape, Xlogit.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 5) <dtype: 'float16'>\n",
      "(84, 1, 5) <dtype: 'float16'>\n",
      "(84, 5)\n",
      "Tensor(\"softmax_cross_entropy_with_logits_10/Cast_2:0\", shape=(84,), dtype=float16)\n",
      "Tensor(\"Mean_10:0\", shape=(), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "print(Xlogit.shape, Xlogit.dtype)\n",
    "print(Yin.shape, Yin.dtype)\n",
    "Yin_reshaped = tf.reshape(name=None, shape=Xlogit.shape, tensor=Yin)\n",
    "print(Yin_reshaped.shape)\n",
    "err_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(labels=Yin_reshaped, logits=Xlogit, name=None)\n",
    "print(err_tensor)\n",
    "err = tf.reduce_mean(input_tensor=err_tensor, name=None)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is BP + SGD for backpropagating the error and optimizing the error using SGD\n",
    "# __init__(\n",
    "#     learning_rate=0.001,\n",
    "#     beta1=0.9,\n",
    "#     beta2=0.999,\n",
    "#     epsilon=1e-08,\n",
    "#     use_locking=False,\n",
    "#     name='Adam'\n",
    "# )\n",
    "# opt = tf.train.AdamOptimizer(beta1=0.9, beta2=0.999, epsilon=1e-08, learning_rate=0.001, name='Adam', \n",
    "#                              use_locking=False).minimize(err)\n",
    "opt = tf.train.AdamOptimizer(learning_rate=0.001).minimize(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 (250, 40) float16\n",
      "84 (1, 5) float16\n",
      "3920.0\n",
      "84 (250, 40) float16\n",
      "84 (1, 5) float16\n",
      "nan\n",
      "84 (250, 40) float16\n",
      "84 (1, 5) float16\n",
      "nan\n",
      "84 (250, 40) float16\n",
      "84 (1, 5) float16\n",
      "nan\n",
      "84 (250, 40) float16\n",
      "84 (1, 5) float16\n",
      "nan\n",
      "84 (250, 40) float16\n",
      "84 (1, 5) float16\n",
      "nan\n",
      "84 (250, 40) float16\n",
      "84 (1, 5) float16\n",
      "nan\n",
      "84 (250, 40) float16\n",
      "84 (1, 5) float16\n",
      "nan\n",
      "84 (250, 40) float16\n",
      "84 (1, 5) float16\n",
      "nan\n",
      "84 (250, 40) float16\n",
      "84 (1, 5) float16\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "# Save the training result or trained and validated model params\n",
    "epochs = 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # my assumption is the local variables are the parameters and hyperparameters\n",
    "    # the globala variables are the one needed/required by the seession/graph to run\n",
    "    sess.run(fetches=tf.global_variables_initializer())\n",
    "   \n",
    "    # Loop over epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        ######################## Training in every epoch/iteration\n",
    "        # Feed dictionary\n",
    "        feed_dict = {Xin:Xmbtrain, Yin:Ymbtrain}\n",
    "        print(len(Xmbtrain), Xmbtrain[0].shape, Xmbtrain[0].dtype)\n",
    "        print(len(Ymbtrain), Ymbtrain[0].shape, Ymbtrain[0].dtype)\n",
    "            \n",
    "        # Loss/Error\n",
    "        fetchedErr, _ = sess.run(fetches=[err, opt], feed_dict=feed_dict)\n",
    "        print(fetchedErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
