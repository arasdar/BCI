{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.16675734494 0.145946681175 0.134113166485 0.174918389554 0.186887921654 0.191376496192 0.0\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "import numpy as np\n",
    "from utilities import *\n",
    "\n",
    "# test and train read\n",
    "X_train_valid, Y_train_valid, list_ch_train_valid = read_data(data_path=\"../../datasets/har-data/\", \n",
    "                                                              split=\"train\")\n",
    "X_test, Y_test, list_ch_test = read_data(data_path=\"../../datasets/har-data/\", split=\"test\")\n",
    "\n",
    "assert list_ch_train_valid == list_ch_test, \"Mistmatch in channels!\"\n",
    "assert Y_train_valid.max(axis=0) == Y_test.max(axis=0)\n",
    "\n",
    "print(np.mean(Y_train_valid==0), np.mean(Y_train_valid==1), np.mean(Y_train_valid==2), \n",
    "      np.mean(Y_train_valid==3), np.mean(Y_train_valid==4), np.mean(Y_train_valid==5),\n",
    "      np.mean(Y_train_valid==6), np.mean(Y_train_valid==7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.16675734494 0.145946681175 0.134113166485 0.174918389554 0.186887921654 0.191376496192 0.0\n",
      "(7352, 6) float64 (2947, 6) float64\n"
     ]
    }
   ],
   "source": [
    "# Preparing input and output data\n",
    "# from utilities import *\n",
    "\n",
    "# Normalizing/standardizing the input data features\n",
    "X_train_valid_norm, X_test_norm = standardize(test=X_test, train=X_train_valid)\n",
    "\n",
    "# Onehot encoding/vectorizing the output data labels\n",
    "print(np.mean((Y_train_valid).reshape(-1)==0), np.mean((Y_train_valid).reshape(-1)==1),\n",
    "     np.mean((Y_train_valid).reshape(-1)==2), np.mean((Y_train_valid).reshape(-1)==3),\n",
    "     np.mean((Y_train_valid).reshape(-1)==4), np.mean((Y_train_valid).reshape(-1)==5),\n",
    "     np.mean((Y_train_valid).reshape(-1)==6), np.mean((Y_train_valid).reshape(-1)==7))\n",
    "\n",
    "Y_train_valid_onehot = one_hot(labels=Y_train_valid.reshape(-1), n_class=6) \n",
    "Y_test_onehot = one_hot(labels=Y_test.reshape(-1), n_class=6) \n",
    "\n",
    "print(Y_train_valid_onehot.shape, Y_train_valid_onehot.dtype, \n",
    "      Y_test_onehot.shape, Y_test_onehot.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5146, 128, 9) (2206, 128, 9) (5146, 6) (2206, 6)\n"
     ]
    }
   ],
   "source": [
    "# Train and valid split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_norm, X_valid_norm, Y_train_onehot, Y_valid_onehot = train_test_split(X_train_valid_norm, \n",
    "                                                                              Y_train_valid_onehot,\n",
    "                                                                              test_size=0.30)\n",
    "\n",
    "print(X_train_norm.shape, X_valid_norm.shape, Y_train_onehot.shape, Y_valid_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size, seq_len, n_channels 51 128 9\n",
      "n_classes 6\n"
     ]
    }
   ],
   "source": [
    "## Hyperparameters\n",
    "# Input data\n",
    "batch_size = X_train_norm.shape[0]// 100 # minibatch size & number of minibatches\n",
    "seq_len = X_train_norm.shape[1] # Number of steps: each trial length\n",
    "n_channels = X_train_norm.shape[2] # number of channels in each trial\n",
    "print('batch_size, seq_len, n_channels', batch_size, seq_len, n_channels)\n",
    "\n",
    "# Output labels\n",
    "n_classes = Y_train_valid.max(axis=0)\n",
    "assert Y_train_valid.max(axis=0) == Y_test.max(axis=0)\n",
    "print('n_classes', n_classes)\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = 0.0001 #1e-4\n",
    "epochs = 100 # num iterations for updating model\n",
    "keep_prob = 0.50 # 90% neurons are kept and 10% are dropped out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.3.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the data from python/numpy to tensorflow framework\n",
    "inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs_')\n",
    "labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels_')\n",
    "keep_prob_ = tf.placeholder(tf.float32, name = 'keep_prob_')\n",
    "learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate_')# Construct the LSTM inputs and LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 128, 9) (?, 9)\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# Construct the LSTM inputs and LSTM cells\n",
    "# inputs_ (seq_len, N, n_channels)\n",
    "lstm_in = tf.transpose(inputs_, [1,0,2]) # reshape into (seq_len, N, channels)\n",
    "lstm_in = tf.reshape(lstm_in, [-1, n_channels]) # Now (seq_len*N, n_channels)\n",
    "print(inputs_.shape, lstm_in.shape)\n",
    "\n",
    "# RNN-LSTM parameters\n",
    "lstm_size = 3 * n_channels # 3 times the amount of channels\n",
    "lstm_layers = 1 # Number of layers\n",
    "print(lstm_size)\n",
    "\n",
    "# To cells\n",
    "lstm_in = tf.layers.dense(lstm_in, lstm_size, activation=None) # or tf.nn.relu, tf.nn.sigmoid, tf.nn.tanh?\n",
    "\n",
    "# Open up the tensor into a list of seq_len pieces\n",
    "lstm_in = tf.split(lstm_in, seq_len, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LSTM layers\n",
    "# lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob_)\n",
    "cell = tf.nn.rnn_cell.MultiRNNCell([drop] * lstm_layers)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, final_state = tf.nn.static_rnn(cell, lstm_in, dtype=tf.float32, initial_state = initial_state)\n",
    "\n",
    "# We only need the last output tensor to pass into a classifier\n",
    "logits = tf.layers.dense(outputs[-1], n_classes, name='logits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost_tensor, cost Tensor(\"Reshape_3:0\", shape=(51,), dtype=float32) Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "optimizer name: \"Adam\"\n",
      "op: \"NoOp\"\n",
      "input: \"^Adam/update_dense/kernel/ApplyAdam\"\n",
      "input: \"^Adam/update_dense/bias/ApplyAdam\"\n",
      "input: \"^Adam/update_rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/ApplyAdam\"\n",
      "input: \"^Adam/update_rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias/ApplyAdam\"\n",
      "input: \"^Adam/update_logits/kernel/ApplyAdam\"\n",
      "input: \"^Adam/update_logits/bias/ApplyAdam\"\n",
      "input: \"^Adam/Assign\"\n",
      "input: \"^Adam/Assign_1\"\n",
      "\n",
      "correct_pred, accuracy Tensor(\"Equal:0\", shape=(51,), dtype=bool) Tensor(\"accuracy:0\", shape=(), dtype=float32)\n",
      "confusion_matrix Tensor(\"confusion_matrix/SparseTensorDenseAdd:0\", shape=(?, ?), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Backward pass: error backpropagation\n",
    "# Cost function\n",
    "cost_tensor = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_)\n",
    "cost = tf.reduce_mean(input_tensor=cost_tensor)\n",
    "print('cost_tensor, cost', cost_tensor, cost)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_).minimize(cost)\n",
    "print('optimizer', optimizer)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "print('correct_pred, accuracy', correct_pred, accuracy)\n",
    "\n",
    "# Confusion matrix\n",
    "confusion_matrix = tf.confusion_matrix(predictions=tf.argmax(logits, 1),\n",
    "                                       labels=tf.argmax(labels_, 1))\n",
    "print('confusion_matrix', confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 Train loss: 1.680429 Valid loss: 1.642171 Train acc: 0.306471 Valid acc: 0.364843\n",
      "Epoch: 2/100 Train loss: 1.606170 Valid loss: 1.568892 Train acc: 0.353824 Valid acc: 0.413744\n",
      "Epoch: 3/100 Train loss: 1.542997 Valid loss: 1.505562 Train acc: 0.393529 Valid acc: 0.453438\n",
      "Epoch: 4/100 Train loss: 1.486444 Valid loss: 1.447408 Train acc: 0.431029 Valid acc: 0.487011\n",
      "Epoch: 5/100 Train loss: 1.428570 Valid loss: 1.388381 Train acc: 0.465843 Valid acc: 0.518082\n",
      "Epoch: 6/100 Train loss: 1.372214 Valid loss: 1.326726 Train acc: 0.493105 Valid acc: 0.546669\n",
      "Epoch: 7/100 Train loss: 1.316213 Valid loss: 1.266902 Train acc: 0.517395 Valid acc: 0.570342\n",
      "Epoch: 8/100 Train loss: 1.266622 Valid loss: 1.212968 Train acc: 0.536495 Valid acc: 0.589536\n",
      "Epoch: 9/100 Train loss: 1.222169 Valid loss: 1.165339 Train acc: 0.553399 Valid acc: 0.605972\n",
      "Epoch: 10/100 Train loss: 1.182383 Valid loss: 1.123426 Train acc: 0.568000 Valid acc: 0.620007\n",
      "Epoch: 11/100 Train loss: 1.147781 Valid loss: 1.086433 Train acc: 0.579733 Valid acc: 0.632031\n",
      "Epoch: 12/100 Train loss: 1.117049 Valid loss: 1.053386 Train acc: 0.590049 Valid acc: 0.642408\n",
      "Epoch: 13/100 Train loss: 1.088940 Valid loss: 1.023551 Train acc: 0.599653 Valid acc: 0.651623\n",
      "Epoch: 14/100 Train loss: 1.062938 Valid loss: 0.996295 Train acc: 0.608684 Valid acc: 0.660199\n",
      "Epoch: 15/100 Train loss: 1.039179 Valid loss: 0.971139 Train acc: 0.617281 Valid acc: 0.668270\n",
      "Epoch: 16/100 Train loss: 1.016913 Valid loss: 0.947691 Train acc: 0.625662 Valid acc: 0.676221\n",
      "Epoch: 17/100 Train loss: 0.996577 Valid loss: 0.925695 Train acc: 0.632722 Valid acc: 0.683994\n"
     ]
    }
   ],
   "source": [
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "\n",
    "# Save the training result or trained and validated model params\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "    # Loop over epochs\n",
    "    for e in range(epochs):\n",
    "        # Initialize \n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        # Loop over batches\n",
    "        for x, y in get_batches(X_train_norm, Y_train_onehot, batch_size):\n",
    "            \n",
    "            ######################## Training\n",
    "            # Feed dictionary\n",
    "            feed = {inputs_ : x, labels_ : y, keep_prob_ : 0.5, initial_state : state, \n",
    "                    learning_rate_ : learning_rate}\n",
    "            \n",
    "            # Loss\n",
    "            loss, _ , state, acc = sess.run([cost, optimizer, final_state, accuracy], feed_dict = feed)\n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss)\n",
    "            \n",
    "            ################## Validation\n",
    "            state_valid = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "            acc_batch = []\n",
    "            loss_batch = []    \n",
    "            # Loop over batches\n",
    "            for x, y in get_batches(X_valid_norm, Y_valid_onehot, batch_size):\n",
    "\n",
    "                # Feed dictionary\n",
    "                feed = {inputs_ : x, labels_ : y, keep_prob_ : 1.0, initial_state : state_valid}\n",
    "\n",
    "                # Loss\n",
    "                loss, state_valid, acc = sess.run([cost, final_state, accuracy], feed_dict = feed)\n",
    "                acc_batch.append(acc)\n",
    "                loss_batch.append(loss)\n",
    "\n",
    "            # Store\n",
    "            valid_acc.append(np.mean(acc_batch))\n",
    "            valid_loss.append(np.mean(loss_batch))\n",
    "            \n",
    "        # Print info for every iter/epoch\n",
    "        print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "              \"Train loss: {:6f}\".format(np.mean(train_loss)),\n",
    "              \"Valid loss: {:.6f}\".format(np.mean(valid_loss)),\n",
    "              \"Train acc: {:6f}\".format(np.mean(train_acc)),\n",
    "              \"Valid acc: {:.6f}\".format(np.mean(valid_acc)))\n",
    "                \n",
    "    saver.save(sess,\"checkpoints/lstm-har.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as mplot\n",
    "\n",
    "mplot.plot(train_loss, label='har train_loss')\n",
    "mplot.plot(valid_loss, label='har valid_loss')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as mplot\n",
    "mplot.plot(train_acc, label='har train_acc')\n",
    "mplot.plot(valid_acc, label='har valid_acc')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, test_loss = [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore the validated model\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    \n",
    "    ################## Test\n",
    "    state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    acc_batch = []\n",
    "    loss_batch = []    \n",
    "    # Loop over batches\n",
    "    for x, y in get_batches(X_test_norm, Y_test_onehot, batch_size):\n",
    "\n",
    "        # Feed dictionary\n",
    "        feed = {inputs_ : x, labels_ : y, keep_prob_ : 1.0, initial_state : state}\n",
    "\n",
    "        # Loss\n",
    "        loss, state, acc = sess.run([cost, final_state, accuracy], feed_dict = feed)\n",
    "        acc_batch.append(acc)\n",
    "        loss_batch.append(loss)\n",
    "\n",
    "    # Store\n",
    "    test_acc.append(np.mean(acc_batch))\n",
    "    test_loss.append(np.mean(loss_batch))\n",
    "\n",
    "    # Print info for every iter/epoch\n",
    "    print(\"Test loss: {:6f}\".format(np.mean(test_loss)),\n",
    "          \"Test acc: {:.6f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
