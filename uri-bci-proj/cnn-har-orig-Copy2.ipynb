{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.16675734494015235 0.1459466811751904 0.13411316648531013 0.1749183895538629 0.1868879216539717 0.1913764961915125 0.0\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "import numpy as np\n",
    "from utilities import *\n",
    "\n",
    "# test and train read\n",
    "X_train_valid, Y_train_valid, list_ch_train_valid = read_data(data_path=\"../../../datasets/har/har-data/\", \n",
    "                                                              split=\"train\")\n",
    "X_test, Y_test, list_ch_test = read_data(data_path=\"../../../datasets/har/har-data/\", split=\"test\")\n",
    "\n",
    "assert list_ch_train_valid == list_ch_test, \"Mistmatch in channels!\"\n",
    "assert Y_train_valid.max(axis=0) == Y_test.max(axis=0)\n",
    "\n",
    "print(np.mean(Y_train_valid==0), np.mean(Y_train_valid==1), np.mean(Y_train_valid==2), \n",
    "      np.mean(Y_train_valid==3), np.mean(Y_train_valid==4), np.mean(Y_train_valid==5),\n",
    "      np.mean(Y_train_valid==6), np.mean(Y_train_valid==7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.16675734494015235 0.1459466811751904 0.13411316648531013 0.1749183895538629 0.1868879216539717 0.1913764961915125 0.0\n",
      "(7352, 6) float64 (2947, 6) float64\n"
     ]
    }
   ],
   "source": [
    "# Preparing input and output data\n",
    "# from utilities import *\n",
    "\n",
    "# Normalizing/standardizing the input data features\n",
    "X_train_valid_norm, X_test_norm = standardize(test=X_test, train=X_train_valid)\n",
    "\n",
    "# Onehot encoding/vectorizing the output data labels\n",
    "print(np.mean((Y_train_valid).reshape(-1)==0), np.mean((Y_train_valid).reshape(-1)==1),\n",
    "     np.mean((Y_train_valid).reshape(-1)==2), np.mean((Y_train_valid).reshape(-1)==3),\n",
    "     np.mean((Y_train_valid).reshape(-1)==4), np.mean((Y_train_valid).reshape(-1)==5),\n",
    "     np.mean((Y_train_valid).reshape(-1)==6), np.mean((Y_train_valid).reshape(-1)==7))\n",
    "\n",
    "Y_train_valid_onehot = one_hot(labels=Y_train_valid.reshape(-1), n_class=6) \n",
    "Y_test_onehot = one_hot(labels=Y_test.reshape(-1), n_class=6) \n",
    "\n",
    "print(Y_train_valid_onehot.shape, Y_train_valid_onehot.dtype, \n",
    "      Y_test_onehot.shape, Y_test_onehot.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5146, 128, 9) (2206, 128, 9) (5146, 6) (2206, 6)\n"
     ]
    }
   ],
   "source": [
    "# Train and valid split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_norm, X_valid_norm, Y_train_onehot, Y_valid_onehot = train_test_split(X_train_valid_norm, \n",
    "                                                                              Y_train_valid_onehot,\n",
    "                                                                              test_size=0.30)\n",
    "\n",
    "print(X_train_norm.shape, X_valid_norm.shape, Y_train_onehot.shape, Y_valid_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size, seq_len, n_channels 51 128 9\n",
      "n_classes 6\n"
     ]
    }
   ],
   "source": [
    "## Hyperparameters\n",
    "# Input data\n",
    "batch_size = X_train_norm.shape[0]// 100 # minibatch size & number of minibatches\n",
    "seq_len = X_train_norm.shape[1] # Number of steps: each trial length\n",
    "n_channels = X_train_norm.shape[2] # number of channels in each trial\n",
    "print('batch_size, seq_len, n_channels', batch_size, seq_len, n_channels)\n",
    "\n",
    "# Output labels\n",
    "n_classes = Y_train_valid.max(axis=0)\n",
    "assert Y_train_valid.max(axis=0) == Y_test.max(axis=0)\n",
    "print('n_classes', n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2206, 128, 9) <dtype: 'float32'>\n",
      "(2206, 6) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "# Feed the data from python/numpy to tensorflow framework\n",
    "# NWC or NHWC: Number, Height/Width, Channels\n",
    "# print(X_train_norm.shape, X_train_norm.dtype)\n",
    "# The common mistake the N or number of batches or number of input should be minimum of train, valid, or test\n",
    "# obviously should be validation set since it is the smallest one then.\n",
    "# The rest of other dimensions/axes/axises are all the same\n",
    "# print(X_train_norm.shape, X_valid_norm.shape, X_test_norm.shape)\n",
    "# print(X_train_norm.dtype, X_valid_norm.dtype, X_test_norm.dtype)\n",
    "# The N should be the minimum of these three bacthes so that we can train, validate and test.\n",
    "# W, Cin are the same for all training, validation, testing\n",
    "N, W, Cin = X_valid_norm.shape[0], X_train_norm.shape[1], X_train_norm.shape[2]\n",
    "# print(N, W, Cin)\n",
    "# inputs_ = tf.placeholder(tf.float32, [N, W, Cin], name =None)\n",
    "inputs_ = tf.placeholder(dtype=tf.float32, name=None, shape=[N, W, Cin])\n",
    "print(inputs_.shape, inputs_.dtype)\n",
    "\n",
    "# Channels for output or classes or dimensions for output\n",
    "# print(Y_train_valid.shape, Y_train_valid.dtype)\n",
    "# print(Y_train_valid.max(axis=0), Y_test.max(axis=0))\n",
    "assert Y_train_valid.max(axis=0)==Y_test.max(axis=0)\n",
    "# This is the class label or number from 1, ..., 6\n",
    "Cout = Y_train_valid.max(axis=0)\n",
    "# labels_ = tf.placeholder(tf.float32, [N, Cout], name =None)\n",
    "labels_ = tf.placeholder(dtype=tf.float32, name=None, shape=[N, Cout])\n",
    "print(labels_.shape, labels_.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2206, 128, 9) <dtype: 'float32'>\n",
      "(64, 9, 18) <dtype: 'float32_ref'>\n",
      "(2206, 64, 18) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "# This is one convolution channel for example\n",
    "print(inputs_.shape, inputs_.dtype)\n",
    "# variable/weight shape should be based on NWC but the other way around WCN\n",
    "# Width, Channels, Number of weights\n",
    "Wwidth, Wchannels, Wnumber = inputs_.shape[1].value//2, inputs_.shape[2].value, inputs_.shape[2].value*2 # double up the input channels\n",
    "shape = [Wwidth, Wchannels, Wnumber]\n",
    "# print(shape)\n",
    "# initializing the weight using normal\n",
    "initial_value = tf.random_normal(dtype=inputs_.dtype, mean=0.0, name=None, shape=shape, stddev=1.0)\n",
    "# Weight, variable and itsshape\n",
    "Wconv = tf.Variable(dtype=inputs_.dtype, initial_value=initial_value, name=None, trainable=True)\n",
    "print(Wconv.shape, Wconv.dtype)\n",
    "# input shape and type as the input tensor\n",
    "# convolution operation as well\n",
    "Xconv = tf.nn.conv1d(data_format='NWC', filters=Wconv, name=None, padding='SAME', stride=2, use_cudnn_on_gpu=True, \n",
    "                     value=inputs_)\n",
    "print(Xconv.shape, Xconv.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2206, 1152) <dtype: 'float32'>\n",
      "(1152, 6) <dtype: 'float32_ref'>\n",
      "(2206, 6) <dtype: 'float32'>\n",
      "(2206, 6) <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "# This is one multiplication channel for example\n",
    "# Flatten and add dropout + predicted output\n",
    "# WHY!!!!!!!! this is not working!!\n",
    "shape = [Xconv.shape[0].value, Xconv.shape[1].value*Xconv.shape[2].value]\n",
    "# shape = [-1, Xconv.shape[1].value*Xconv.shape[2].value]\n",
    "# print(shape, 64*18) # as a double check\n",
    "Xconv_reshaped = tf.reshape(name=None, shape=shape, tensor=Xconv)\n",
    "print(Xconv_reshaped.shape, Xconv_reshaped.dtype)\n",
    "# The weight for fully connected/dense layer and for multiplication not convolution though.\n",
    "# multiplication is so much easier than convolution in terms of implementation.\n",
    "# shape = this shape should be the same NWC as well but the other way around or maybe transposed\n",
    "# X is NWC which is describing the tensor shape0, 1, 2\n",
    "# labels which are the output labels are supposed to be NC, \n",
    "# N:batch size, C: output channels or output classes or dimensions \n",
    "Wchannels, Wnumber = Xconv_reshaped.shape[1].value, labels_.shape[1].value\n",
    "shape = [Wchannels, Wnumber]\n",
    "# print(shape)\n",
    "initial_value = tf.random_normal(dtype=Xconv_reshaped.dtype, mean=0.0, name=None, shape=shape, stddev=1.0)\n",
    "W = tf.Variable(dtype=Xconv_reshaped.dtype, initial_value=initial_value, name=None, trainable=True)\n",
    "print(W.shape, W.dtype)\n",
    "logits = tf.matmul(a=Xconv_reshaped, b=W, name=None)\n",
    "print(logits.shape, logits.dtype)\n",
    "print(labels_.shape, labels_.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost_tensor, cost Tensor(\"softmax_cross_entropy_with_logits_2/Reshape_2:0\", shape=(2206,), dtype=float32) Tensor(\"Mean_2:0\", shape=(), dtype=float32)\n",
      "optimizer name: \"Adam_2\"\n",
      "op: \"NoOp\"\n",
      "input: \"^Adam_2/update_Variable_6/ApplyAdam\"\n",
      "input: \"^Adam_2/update_Variable_7/ApplyAdam\"\n",
      "input: \"^Adam_2/Assign\"\n",
      "input: \"^Adam_2/Assign_1\"\n",
      "\n",
      "correct_pred, accuracy Tensor(\"Equal_2:0\", shape=(2206,), dtype=bool) Tensor(\"accuracy_2:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Backward pass: error backpropagation\n",
    "# Cost function\n",
    "cost_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels_)\n",
    "cost = tf.reduce_mean(input_tensor=cost_tensor)\n",
    "print('cost_tensor, cost', cost_tensor, cost)\n",
    "\n",
    "# Optimizer\n",
    "# __init__(\n",
    "#     learning_rate=0.001,\n",
    "#     beta1=0.9,\n",
    "#     beta2=0.999,\n",
    "#     epsilon=1e-08,\n",
    "#     use_locking=False,\n",
    "#     name='Adam'\n",
    "# )\n",
    "# minimize(\n",
    "#     loss,\n",
    "#     global_step=None,\n",
    "#     var_list=None,\n",
    "#     gate_gradients=GATE_OP,\n",
    "#     aggregation_method=None,\n",
    "#     colocate_gradients_with_ops=False,\n",
    "#     name=None,\n",
    "#     grad_loss=None\n",
    "# )\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss=cost)\n",
    "print('optimizer', optimizer)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "print('correct_pred, accuracy', correct_pred, accuracy)\n",
    "\n",
    "# # Confusion matrix\n",
    "# confusion_matrix = tf.confusion_matrix(predictions=tf.argmax(logits, 1),\n",
    "#                                        labels=tf.argmax(labels_, 1))\n",
    "# print('confusion_matrix', confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 training loss: 1165.7986 validation loss: 1109.2313\n",
      "epoch: 1 training loss: 1109.7051 validation loss: 1054.3641\n",
      "epoch: 2 training loss: 1055.0192 validation loss: 1000.85394\n",
      "epoch: 3 training loss: 1001.7489 validation loss: 948.82947\n",
      "epoch: 4 training loss: 949.80505 validation loss: 898.37964\n",
      "epoch: 5 training loss: 899.2035 validation loss: 849.49054\n",
      "epoch: 6 training loss: 850.5282 validation loss: 802.74054\n",
      "epoch: 7 training loss: 804.11426 validation loss: 758.63556\n",
      "epoch: 8 training loss: 759.7362 validation loss: 716.9684\n",
      "epoch: 9 training loss: 717.8921 validation loss: 678.64594\n",
      "epoch: 10 training loss: 681.025 validation loss: 649.33655\n",
      "epoch: 11 training loss: 652.01416 validation loss: 626.81635\n",
      "epoch: 12 training loss: 628.7167 validation loss: 608.4412\n",
      "epoch: 13 training loss: 609.09863 validation loss: 592.8341\n",
      "epoch: 14 training loss: 592.20374 validation loss: 578.7435\n",
      "epoch: 15 training loss: 577.1592 validation loss: 565.86566\n",
      "epoch: 16 training loss: 563.2141 validation loss: 553.8109\n",
      "epoch: 17 training loss: 550.10547 validation loss: 542.40924\n",
      "epoch: 18 training loss: 537.5031 validation loss: 531.3323\n",
      "epoch: 19 training loss: 525.23944 validation loss: 520.6024\n",
      "epoch: 20 training loss: 513.33295 validation loss: 510.26093\n",
      "epoch: 21 training loss: 501.72717 validation loss: 500.11218\n",
      "epoch: 22 training loss: 490.40375 validation loss: 490.1414\n",
      "epoch: 23 training loss: 479.31555 validation loss: 480.35013\n",
      "epoch: 24 training loss: 468.46198 validation loss: 470.7766\n",
      "epoch: 25 training loss: 457.86972 validation loss: 461.3903\n",
      "epoch: 26 training loss: 447.56403 validation loss: 452.20248\n",
      "epoch: 27 training loss: 437.50702 validation loss: 443.2797\n",
      "epoch: 28 training loss: 427.7055 validation loss: 434.52563\n",
      "epoch: 29 training loss: 418.18057 validation loss: 425.91736\n",
      "epoch: 30 training loss: 408.90207 validation loss: 417.49033\n",
      "epoch: 31 training loss: 399.8412 validation loss: 409.2244\n",
      "epoch: 32 training loss: 390.98953 validation loss: 401.12695\n",
      "epoch: 33 training loss: 382.35913 validation loss: 393.16354\n",
      "epoch: 34 training loss: 373.92584 validation loss: 385.36176\n",
      "epoch: 35 training loss: 365.67432 validation loss: 377.68634\n",
      "epoch: 36 training loss: 357.59717 validation loss: 370.1162\n",
      "epoch: 37 training loss: 349.71637 validation loss: 362.69092\n",
      "epoch: 38 training loss: 342.0667 validation loss: 355.41302\n",
      "epoch: 39 training loss: 334.60675 validation loss: 348.35263\n",
      "epoch: 40 training loss: 327.34055 validation loss: 341.59045\n",
      "epoch: 41 training loss: 320.35333 validation loss: 335.19965\n",
      "epoch: 42 training loss: 313.80426 validation loss: 329.24582\n",
      "epoch: 43 training loss: 307.74255 validation loss: 323.7394\n",
      "epoch: 44 training loss: 302.18387 validation loss: 318.66898\n",
      "epoch: 45 training loss: 297.0548 validation loss: 314.06665\n",
      "epoch: 46 training loss: 292.30878 validation loss: 309.77368\n",
      "epoch: 47 training loss: 287.8443 validation loss: 305.7308\n",
      "epoch: 48 training loss: 283.6449 validation loss: 301.90018\n",
      "epoch: 49 training loss: 279.63364 validation loss: 298.2324\n",
      "epoch: 50 training loss: 275.78748 validation loss: 294.71362\n",
      "epoch: 51 training loss: 272.06107 validation loss: 291.33725\n",
      "epoch: 52 training loss: 268.44885 validation loss: 288.09088\n",
      "epoch: 53 training loss: 264.95053 validation loss: 285.01675\n",
      "epoch: 54 training loss: 261.6385 validation loss: 282.13623\n",
      "epoch: 55 training loss: 258.45642 validation loss: 279.41266\n",
      "epoch: 56 training loss: 255.39984 validation loss: 276.76953\n",
      "epoch: 57 training loss: 252.42102 validation loss: 274.20694\n",
      "epoch: 58 training loss: 249.51718 validation loss: 271.71362\n",
      "epoch: 59 training loss: 246.70477 validation loss: 269.27985\n",
      "epoch: 60 training loss: 243.95801 validation loss: 266.8995\n",
      "epoch: 61 training loss: 241.27261 validation loss: 264.57068\n",
      "epoch: 62 training loss: 238.65686 validation loss: 262.30154\n",
      "epoch: 63 training loss: 236.09607 validation loss: 260.08374\n",
      "epoch: 64 training loss: 233.58281 validation loss: 257.91345\n",
      "epoch: 65 training loss: 231.13152 validation loss: 255.78896\n",
      "epoch: 66 training loss: 228.73154 validation loss: 253.72076\n",
      "epoch: 67 training loss: 226.37456 validation loss: 251.71246\n",
      "epoch: 68 training loss: 224.07219 validation loss: 249.74991\n",
      "epoch: 69 training loss: 221.80867 validation loss: 247.8218\n",
      "epoch: 70 training loss: 219.58905 validation loss: 245.92708\n",
      "epoch: 71 training loss: 217.40689 validation loss: 244.05862\n",
      "epoch: 72 training loss: 215.26028 validation loss: 242.2208\n",
      "epoch: 73 training loss: 213.14877 validation loss: 240.41614\n",
      "epoch: 74 training loss: 211.08261 validation loss: 238.63449\n",
      "epoch: 75 training loss: 209.05457 validation loss: 236.87056\n",
      "epoch: 76 training loss: 207.05359 validation loss: 235.12611\n",
      "epoch: 77 training loss: 205.08546 validation loss: 233.40599\n",
      "epoch: 78 training loss: 203.15125 validation loss: 231.70798\n",
      "epoch: 79 training loss: 201.24698 validation loss: 230.03406\n",
      "epoch: 80 training loss: 199.36761 validation loss: 228.38383\n",
      "epoch: 81 training loss: 197.51163 validation loss: 226.7576\n",
      "epoch: 82 training loss: 195.67874 validation loss: 225.16486\n",
      "epoch: 83 training loss: 193.86568 validation loss: 223.5976\n",
      "epoch: 84 training loss: 192.07965 validation loss: 222.05484\n",
      "epoch: 85 training loss: 190.31985 validation loss: 220.52821\n",
      "epoch: 86 training loss: 188.57886 validation loss: 219.01309\n",
      "epoch: 87 training loss: 186.85461 validation loss: 217.51\n",
      "epoch: 88 training loss: 185.14471 validation loss: 216.0163\n",
      "epoch: 89 training loss: 183.45505 validation loss: 214.53049\n",
      "epoch: 90 training loss: 181.80289 validation loss: 213.07806\n",
      "epoch: 91 training loss: 180.18968 validation loss: 211.64905\n",
      "epoch: 92 training loss: 178.60913 validation loss: 210.24298\n",
      "epoch: 93 training loss: 177.06755 validation loss: 208.8756\n",
      "epoch: 94 training loss: 175.56314 validation loss: 207.55278\n",
      "epoch: 95 training loss: 174.08914 validation loss: 206.26193\n",
      "epoch: 96 training loss: 172.65137 validation loss: 204.98776\n",
      "epoch: 97 training loss: 171.24135 validation loss: 203.73221\n",
      "epoch: 98 training loss: 169.86108 validation loss: 202.4921\n",
      "epoch: 99 training loss: 168.50269 validation loss: 201.26115\n",
      "epoch: 100 training loss: 167.15958 validation loss: 200.04024\n",
      "epoch: 101 training loss: 165.83516 validation loss: 198.84128\n",
      "epoch: 102 training loss: 164.53015 validation loss: 197.65672\n",
      "epoch: 103 training loss: 163.24353 validation loss: 196.49272\n",
      "epoch: 104 training loss: 161.97197 validation loss: 195.34573\n",
      "epoch: 105 training loss: 160.71255 validation loss: 194.21004\n",
      "epoch: 106 training loss: 159.46954 validation loss: 193.08694\n",
      "epoch: 107 training loss: 158.23761 validation loss: 191.97537\n",
      "epoch: 108 training loss: 157.01662 validation loss: 190.8777\n",
      "epoch: 109 training loss: 155.81119 validation loss: 189.7966\n",
      "epoch: 110 training loss: 154.62108 validation loss: 188.73273\n",
      "epoch: 111 training loss: 153.45132 validation loss: 187.6788\n",
      "epoch: 112 training loss: 152.29251 validation loss: 186.6335\n",
      "epoch: 113 training loss: 151.14368 validation loss: 185.60338\n",
      "epoch: 114 training loss: 150.00317 validation loss: 184.59206\n",
      "epoch: 115 training loss: 148.87402 validation loss: 183.5828\n",
      "epoch: 116 training loss: 147.75717 validation loss: 182.57375\n",
      "epoch: 117 training loss: 146.65631 validation loss: 181.56421\n",
      "epoch: 118 training loss: 145.56717 validation loss: 180.56589\n",
      "epoch: 119 training loss: 144.49573 validation loss: 179.57983\n",
      "epoch: 120 training loss: 143.43582 validation loss: 178.61415\n",
      "epoch: 121 training loss: 142.38171 validation loss: 177.66208\n",
      "epoch: 122 training loss: 141.3357 validation loss: 176.72826\n",
      "epoch: 123 training loss: 140.29828 validation loss: 175.81322\n",
      "epoch: 124 training loss: 139.26892 validation loss: 174.92015\n",
      "epoch: 125 training loss: 138.24762 validation loss: 174.0405\n",
      "epoch: 126 training loss: 137.23611 validation loss: 173.16444\n",
      "epoch: 127 training loss: 136.23242 validation loss: 172.29323\n",
      "epoch: 128 training loss: 135.24252 validation loss: 171.43245\n",
      "epoch: 129 training loss: 134.26404 validation loss: 170.58499\n",
      "epoch: 130 training loss: 133.2961 validation loss: 169.74677\n",
      "epoch: 131 training loss: 132.34009 validation loss: 168.91623\n",
      "epoch: 132 training loss: 131.39546 validation loss: 168.09055\n",
      "epoch: 133 training loss: 130.45929 validation loss: 167.27567\n",
      "epoch: 134 training loss: 129.53027 validation loss: 166.46936\n",
      "epoch: 135 training loss: 128.61102 validation loss: 165.66249\n",
      "epoch: 136 training loss: 127.70489 validation loss: 164.85574\n",
      "epoch: 137 training loss: 126.81211 validation loss: 164.05406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 138 training loss: 125.928345 validation loss: 163.26187\n",
      "epoch: 139 training loss: 125.05356 validation loss: 162.4831\n",
      "epoch: 140 training loss: 124.18941 validation loss: 161.71605\n",
      "epoch: 141 training loss: 123.332825 validation loss: 160.95895\n",
      "epoch: 142 training loss: 122.48862 validation loss: 160.21153\n",
      "epoch: 143 training loss: 121.653656 validation loss: 159.46944\n",
      "epoch: 144 training loss: 120.829834 validation loss: 158.73338\n",
      "epoch: 145 training loss: 120.019135 validation loss: 158.0048\n",
      "epoch: 146 training loss: 119.21651 validation loss: 157.28633\n",
      "epoch: 147 training loss: 118.422295 validation loss: 156.57623\n",
      "epoch: 148 training loss: 117.635216 validation loss: 155.87866\n",
      "epoch: 149 training loss: 116.85449 validation loss: 155.19386\n",
      "epoch: 150 training loss: 116.0804 validation loss: 154.52197\n",
      "epoch: 151 training loss: 115.311905 validation loss: 153.86494\n",
      "epoch: 152 training loss: 114.550064 validation loss: 153.21692\n",
      "epoch: 153 training loss: 113.795 validation loss: 152.57497\n",
      "epoch: 154 training loss: 113.045876 validation loss: 151.93604\n",
      "epoch: 155 training loss: 112.30369 validation loss: 151.30054\n",
      "epoch: 156 training loss: 111.56691 validation loss: 150.6672\n",
      "epoch: 157 training loss: 110.83401 validation loss: 150.03728\n",
      "epoch: 158 training loss: 110.1092 validation loss: 149.41881\n",
      "epoch: 159 training loss: 109.39227 validation loss: 148.80942\n",
      "epoch: 160 training loss: 108.68257 validation loss: 148.20645\n",
      "epoch: 161 training loss: 107.97995 validation loss: 147.60858\n",
      "epoch: 162 training loss: 107.28143 validation loss: 147.01266\n",
      "epoch: 163 training loss: 106.58952 validation loss: 146.41917\n",
      "epoch: 164 training loss: 105.90411 validation loss: 145.82845\n",
      "epoch: 165 training loss: 105.22473 validation loss: 145.23851\n",
      "epoch: 166 training loss: 104.552055 validation loss: 144.64749\n",
      "epoch: 167 training loss: 103.887054 validation loss: 144.05687\n",
      "epoch: 168 training loss: 103.22954 validation loss: 143.46648\n",
      "epoch: 169 training loss: 102.57818 validation loss: 142.8791\n",
      "epoch: 170 training loss: 101.93155 validation loss: 142.29741\n",
      "epoch: 171 training loss: 101.288025 validation loss: 141.7201\n",
      "epoch: 172 training loss: 100.64683 validation loss: 141.14436\n",
      "epoch: 173 training loss: 100.01108 validation loss: 140.5723\n",
      "epoch: 174 training loss: 99.383514 validation loss: 140.0069\n",
      "epoch: 175 training loss: 98.76015 validation loss: 139.44646\n",
      "epoch: 176 training loss: 98.1418 validation loss: 138.88867\n",
      "epoch: 177 training loss: 97.52844 validation loss: 138.33575\n",
      "epoch: 178 training loss: 96.92067 validation loss: 137.79169\n",
      "epoch: 179 training loss: 96.32243 validation loss: 137.25906\n",
      "epoch: 180 training loss: 95.73338 validation loss: 136.73584\n",
      "epoch: 181 training loss: 95.1487 validation loss: 136.22159\n",
      "epoch: 182 training loss: 94.571754 validation loss: 135.7208\n",
      "epoch: 183 training loss: 94.00192 validation loss: 135.22826\n",
      "epoch: 184 training loss: 93.43792 validation loss: 134.73883\n",
      "epoch: 185 training loss: 92.88155 validation loss: 134.24796\n",
      "epoch: 186 training loss: 92.33177 validation loss: 133.75754\n",
      "epoch: 187 training loss: 91.78395 validation loss: 133.27045\n",
      "epoch: 188 training loss: 91.23873 validation loss: 132.78711\n",
      "epoch: 189 training loss: 90.69884 validation loss: 132.30453\n",
      "epoch: 190 training loss: 90.16542 validation loss: 131.8227\n",
      "epoch: 191 training loss: 89.64038 validation loss: 131.34355\n",
      "epoch: 192 training loss: 89.12106 validation loss: 130.86624\n",
      "epoch: 193 training loss: 88.60525 validation loss: 130.39114\n",
      "epoch: 194 training loss: 88.09305 validation loss: 129.92084\n",
      "epoch: 195 training loss: 87.58507 validation loss: 129.45337\n",
      "epoch: 196 training loss: 87.081184 validation loss: 128.98349\n",
      "epoch: 197 training loss: 86.58196 validation loss: 128.511\n",
      "epoch: 198 training loss: 86.08745 validation loss: 128.03964\n",
      "epoch: 199 training loss: 85.5965 validation loss: 127.57465\n",
      "epoch: 200 training loss: 85.11009 validation loss: 127.11695\n",
      "epoch: 201 training loss: 84.62907 validation loss: 126.66101\n",
      "epoch: 202 training loss: 84.15492 validation loss: 126.20693\n",
      "epoch: 203 training loss: 83.68505 validation loss: 125.757904\n",
      "epoch: 204 training loss: 83.21898 validation loss: 125.31533\n",
      "epoch: 205 training loss: 82.75792 validation loss: 124.87531\n",
      "epoch: 206 training loss: 82.301636 validation loss: 124.43716\n",
      "epoch: 207 training loss: 81.849976 validation loss: 124.00225\n",
      "epoch: 208 training loss: 81.40225 validation loss: 123.572235\n",
      "epoch: 209 training loss: 80.95769 validation loss: 123.148415\n",
      "epoch: 210 training loss: 80.518394 validation loss: 122.73097\n",
      "epoch: 211 training loss: 80.08388 validation loss: 122.31718\n",
      "epoch: 212 training loss: 79.6541 validation loss: 121.901276\n",
      "epoch: 213 training loss: 79.227295 validation loss: 121.48345\n",
      "epoch: 214 training loss: 78.804794 validation loss: 121.06531\n",
      "epoch: 215 training loss: 78.38538 validation loss: 120.64947\n",
      "epoch: 216 training loss: 77.96866 validation loss: 120.23864\n",
      "epoch: 217 training loss: 77.55453 validation loss: 119.833084\n",
      "epoch: 218 training loss: 77.143 validation loss: 119.42937\n",
      "epoch: 219 training loss: 76.73544 validation loss: 119.0237\n",
      "epoch: 220 training loss: 76.33179 validation loss: 118.61805\n",
      "epoch: 221 training loss: 75.93255 validation loss: 118.21895\n",
      "epoch: 222 training loss: 75.53763 validation loss: 117.826\n",
      "epoch: 223 training loss: 75.14751 validation loss: 117.440346\n",
      "epoch: 224 training loss: 74.76124 validation loss: 117.06117\n",
      "epoch: 225 training loss: 74.37909 validation loss: 116.68566\n",
      "epoch: 226 training loss: 74.00151 validation loss: 116.31545\n",
      "epoch: 227 training loss: 73.62671 validation loss: 115.95021\n",
      "epoch: 228 training loss: 73.25404 validation loss: 115.58787\n",
      "epoch: 229 training loss: 72.88469 validation loss: 115.2279\n",
      "epoch: 230 training loss: 72.51752 validation loss: 114.872986\n",
      "epoch: 231 training loss: 72.152725 validation loss: 114.52409\n",
      "epoch: 232 training loss: 71.791214 validation loss: 114.177704\n",
      "epoch: 233 training loss: 71.43269 validation loss: 113.83236\n",
      "epoch: 234 training loss: 71.076256 validation loss: 113.48507\n",
      "epoch: 235 training loss: 70.722244 validation loss: 113.1373\n",
      "epoch: 236 training loss: 70.37045 validation loss: 112.79169\n",
      "epoch: 237 training loss: 70.02101 validation loss: 112.45057\n",
      "epoch: 238 training loss: 69.67292 validation loss: 112.115135\n",
      "epoch: 239 training loss: 69.3264 validation loss: 111.782585\n",
      "epoch: 240 training loss: 68.98171 validation loss: 111.449486\n",
      "epoch: 241 training loss: 68.63924 validation loss: 111.116035\n",
      "epoch: 242 training loss: 68.30069 validation loss: 110.78232\n",
      "epoch: 243 training loss: 67.96777 validation loss: 110.45276\n",
      "epoch: 244 training loss: 67.6382 validation loss: 110.1288\n",
      "epoch: 245 training loss: 67.310616 validation loss: 109.80909\n",
      "epoch: 246 training loss: 66.98644 validation loss: 109.49363\n",
      "epoch: 247 training loss: 66.66662 validation loss: 109.18389\n",
      "epoch: 248 training loss: 66.35063 validation loss: 108.87881\n",
      "epoch: 249 training loss: 66.03705 validation loss: 108.575096\n",
      "epoch: 250 training loss: 65.72496 validation loss: 108.27047\n",
      "epoch: 251 training loss: 65.41504 validation loss: 107.96687\n",
      "epoch: 252 training loss: 65.10864 validation loss: 107.66594\n",
      "epoch: 253 training loss: 64.8056 validation loss: 107.36603\n",
      "epoch: 254 training loss: 64.50541 validation loss: 107.06732\n",
      "epoch: 255 training loss: 64.20909 validation loss: 106.77148\n",
      "epoch: 256 training loss: 63.91561 validation loss: 106.478806\n",
      "epoch: 257 training loss: 63.6241 validation loss: 106.18783\n",
      "epoch: 258 training loss: 63.33451 validation loss: 105.89888\n",
      "epoch: 259 training loss: 63.046715 validation loss: 105.61456\n",
      "epoch: 260 training loss: 62.761047 validation loss: 105.33604\n",
      "epoch: 261 training loss: 62.477646 validation loss: 105.05968\n",
      "epoch: 262 training loss: 62.196712 validation loss: 104.784065\n",
      "epoch: 263 training loss: 61.918022 validation loss: 104.50963\n",
      "epoch: 264 training loss: 61.641068 validation loss: 104.236145\n",
      "epoch: 265 training loss: 61.36653 validation loss: 103.96531\n",
      "epoch: 266 training loss: 61.0953 validation loss: 103.698166\n",
      "epoch: 267 training loss: 60.82759 validation loss: 103.43379\n",
      "epoch: 268 training loss: 60.562443 validation loss: 103.17013\n",
      "epoch: 269 training loss: 60.29931 validation loss: 102.90843\n",
      "epoch: 270 training loss: 60.038918 validation loss: 102.64921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 271 training loss: 59.7808 validation loss: 102.388954\n",
      "epoch: 272 training loss: 59.525574 validation loss: 102.12558\n",
      "epoch: 273 training loss: 59.272697 validation loss: 101.86236\n",
      "epoch: 274 training loss: 59.021095 validation loss: 101.602974\n",
      "epoch: 275 training loss: 58.77108 validation loss: 101.34812\n",
      "epoch: 276 training loss: 58.52294 validation loss: 101.097336\n",
      "epoch: 277 training loss: 58.27716 validation loss: 100.84952\n",
      "epoch: 278 training loss: 58.034077 validation loss: 100.605255\n",
      "epoch: 279 training loss: 57.79262 validation loss: 100.36364\n",
      "epoch: 280 training loss: 57.552795 validation loss: 100.121346\n",
      "epoch: 281 training loss: 57.315018 validation loss: 99.87747\n",
      "epoch: 282 training loss: 57.078667 validation loss: 99.63331\n",
      "epoch: 283 training loss: 56.843983 validation loss: 99.38909\n",
      "epoch: 284 training loss: 56.612938 validation loss: 99.1479\n",
      "epoch: 285 training loss: 56.385742 validation loss: 98.91182\n",
      "epoch: 286 training loss: 56.160126 validation loss: 98.680984\n",
      "epoch: 287 training loss: 55.935726 validation loss: 98.45366\n",
      "epoch: 288 training loss: 55.71321 validation loss: 98.22643\n",
      "epoch: 289 training loss: 55.49215 validation loss: 97.99701\n",
      "epoch: 290 training loss: 55.27227 validation loss: 97.764885\n",
      "epoch: 291 training loss: 55.05339 validation loss: 97.531845\n",
      "epoch: 292 training loss: 54.835426 validation loss: 97.299675\n",
      "epoch: 293 training loss: 54.61853 validation loss: 97.06882\n",
      "epoch: 294 training loss: 54.402775 validation loss: 96.83967\n",
      "epoch: 295 training loss: 54.188267 validation loss: 96.612976\n",
      "epoch: 296 training loss: 53.975132 validation loss: 96.38856\n",
      "epoch: 297 training loss: 53.76335 validation loss: 96.1646\n",
      "epoch: 298 training loss: 53.553024 validation loss: 95.93963\n",
      "epoch: 299 training loss: 53.34456 validation loss: 95.715164\n",
      "epoch: 300 training loss: 53.13794 validation loss: 95.49361\n",
      "epoch: 301 training loss: 52.933754 validation loss: 95.27391\n",
      "epoch: 302 training loss: 52.73226 validation loss: 95.05532\n",
      "epoch: 303 training loss: 52.53199 validation loss: 94.839516\n",
      "epoch: 304 training loss: 52.33269 validation loss: 94.627716\n",
      "epoch: 305 training loss: 52.135155 validation loss: 94.418465\n",
      "epoch: 306 training loss: 51.93908 validation loss: 94.209724\n",
      "epoch: 307 training loss: 51.744072 validation loss: 94.002426\n",
      "epoch: 308 training loss: 51.550293 validation loss: 93.79835\n",
      "epoch: 309 training loss: 51.3578 validation loss: 93.59743\n",
      "epoch: 310 training loss: 51.166367 validation loss: 93.39672\n",
      "epoch: 311 training loss: 50.975826 validation loss: 93.19533\n",
      "epoch: 312 training loss: 50.786617 validation loss: 92.9958\n",
      "epoch: 313 training loss: 50.59886 validation loss: 92.79969\n",
      "epoch: 314 training loss: 50.41185 validation loss: 92.60612\n",
      "epoch: 315 training loss: 50.225544 validation loss: 92.41378\n",
      "epoch: 316 training loss: 50.040077 validation loss: 92.2223\n",
      "epoch: 317 training loss: 49.855316 validation loss: 92.03333\n",
      "epoch: 318 training loss: 49.67104 validation loss: 91.846664\n",
      "epoch: 319 training loss: 49.48735 validation loss: 91.66143\n",
      "epoch: 320 training loss: 49.30487 validation loss: 91.47845\n",
      "epoch: 321 training loss: 49.12326 validation loss: 91.297195\n",
      "epoch: 322 training loss: 48.94252 validation loss: 91.11636\n",
      "epoch: 323 training loss: 48.762665 validation loss: 90.93631\n",
      "epoch: 324 training loss: 48.58379 validation loss: 90.75841\n",
      "epoch: 325 training loss: 48.405838 validation loss: 90.58287\n",
      "epoch: 326 training loss: 48.2293 validation loss: 90.40868\n",
      "epoch: 327 training loss: 48.054028 validation loss: 90.23528\n",
      "epoch: 328 training loss: 47.879513 validation loss: 90.06145\n",
      "epoch: 329 training loss: 47.70611 validation loss: 89.887764\n",
      "epoch: 330 training loss: 47.534157 validation loss: 89.71662\n",
      "epoch: 331 training loss: 47.364136 validation loss: 89.54643\n",
      "epoch: 332 training loss: 47.195442 validation loss: 89.37591\n",
      "epoch: 333 training loss: 47.027397 validation loss: 89.20598\n",
      "epoch: 334 training loss: 46.860046 validation loss: 89.037186\n",
      "epoch: 335 training loss: 46.69339 validation loss: 88.87031\n",
      "epoch: 336 training loss: 46.52735 validation loss: 88.70584\n",
      "epoch: 337 training loss: 46.36187 validation loss: 88.5432\n",
      "epoch: 338 training loss: 46.1972 validation loss: 88.38205\n",
      "epoch: 339 training loss: 46.0339 validation loss: 88.22138\n",
      "epoch: 340 training loss: 45.872005 validation loss: 88.05889\n",
      "epoch: 341 training loss: 45.71118 validation loss: 87.89436\n",
      "epoch: 342 training loss: 45.5511 validation loss: 87.730515\n",
      "epoch: 343 training loss: 45.39166 validation loss: 87.56935\n",
      "epoch: 344 training loss: 45.233032 validation loss: 87.4108\n",
      "epoch: 345 training loss: 45.074913 validation loss: 87.25421\n",
      "epoch: 346 training loss: 44.917442 validation loss: 87.098366\n",
      "epoch: 347 training loss: 44.760845 validation loss: 86.943085\n",
      "epoch: 348 training loss: 44.605095 validation loss: 86.78802\n",
      "epoch: 349 training loss: 44.4504 validation loss: 86.632805\n",
      "epoch: 350 training loss: 44.296574 validation loss: 86.478195\n",
      "epoch: 351 training loss: 44.14374 validation loss: 86.32493\n",
      "epoch: 352 training loss: 43.99183 validation loss: 86.172386\n",
      "epoch: 353 training loss: 43.840755 validation loss: 86.01989\n",
      "epoch: 354 training loss: 43.690674 validation loss: 85.86842\n",
      "epoch: 355 training loss: 43.54138 validation loss: 85.71885\n",
      "epoch: 356 training loss: 43.39302 validation loss: 85.571\n",
      "epoch: 357 training loss: 43.24526 validation loss: 85.424515\n",
      "epoch: 358 training loss: 43.098183 validation loss: 85.279335\n",
      "epoch: 359 training loss: 42.952133 validation loss: 85.13557\n",
      "epoch: 360 training loss: 42.80692 validation loss: 84.99354\n",
      "epoch: 361 training loss: 42.66246 validation loss: 84.85401\n",
      "epoch: 362 training loss: 42.5188 validation loss: 84.71668\n",
      "epoch: 363 training loss: 42.37581 validation loss: 84.58128\n",
      "epoch: 364 training loss: 42.233513 validation loss: 84.447205\n",
      "epoch: 365 training loss: 42.092068 validation loss: 84.31318\n",
      "epoch: 366 training loss: 41.951538 validation loss: 84.17938\n",
      "epoch: 367 training loss: 41.81198 validation loss: 84.044495\n",
      "epoch: 368 training loss: 41.67328 validation loss: 83.90828\n",
      "epoch: 369 training loss: 41.535522 validation loss: 83.772964\n",
      "epoch: 370 training loss: 41.39933 validation loss: 83.640434\n",
      "epoch: 371 training loss: 41.26466 validation loss: 83.5091\n",
      "epoch: 372 training loss: 41.131203 validation loss: 83.376434\n",
      "epoch: 373 training loss: 40.99894 validation loss: 83.241104\n",
      "epoch: 374 training loss: 40.867607 validation loss: 83.104576\n",
      "epoch: 375 training loss: 40.736847 validation loss: 82.968925\n",
      "epoch: 376 training loss: 40.60698 validation loss: 82.83292\n",
      "epoch: 377 training loss: 40.47849 validation loss: 82.696884\n",
      "epoch: 378 training loss: 40.351143 validation loss: 82.56307\n",
      "epoch: 379 training loss: 40.22433 validation loss: 82.4312\n",
      "epoch: 380 training loss: 40.097977 validation loss: 82.30006\n",
      "epoch: 381 training loss: 39.97226 validation loss: 82.17001\n",
      "epoch: 382 training loss: 39.84722 validation loss: 82.04099\n",
      "epoch: 383 training loss: 39.722664 validation loss: 81.911804\n",
      "epoch: 384 training loss: 39.598457 validation loss: 81.782684\n",
      "epoch: 385 training loss: 39.474762 validation loss: 81.65432\n",
      "epoch: 386 training loss: 39.35159 validation loss: 81.52675\n",
      "epoch: 387 training loss: 39.228752 validation loss: 81.40014\n",
      "epoch: 388 training loss: 39.1063 validation loss: 81.2742\n",
      "epoch: 389 training loss: 38.984703 validation loss: 81.149506\n",
      "epoch: 390 training loss: 38.863747 validation loss: 81.025406\n",
      "epoch: 391 training loss: 38.74314 validation loss: 80.9021\n",
      "epoch: 392 training loss: 38.623066 validation loss: 80.78048\n",
      "epoch: 393 training loss: 38.503548 validation loss: 80.65981\n",
      "epoch: 394 training loss: 38.384663 validation loss: 80.53806\n",
      "epoch: 395 training loss: 38.26639 validation loss: 80.41562\n",
      "epoch: 396 training loss: 38.148758 validation loss: 80.29486\n",
      "epoch: 397 training loss: 38.03195 validation loss: 80.175804\n",
      "epoch: 398 training loss: 37.9161 validation loss: 80.057274\n",
      "epoch: 399 training loss: 37.800934 validation loss: 79.94001\n",
      "epoch: 400 training loss: 37.68626 validation loss: 79.82374\n",
      "epoch: 401 training loss: 37.572155 validation loss: 79.707664\n",
      "epoch: 402 training loss: 37.458813 validation loss: 79.59228\n",
      "epoch: 403 training loss: 37.346333 validation loss: 79.47831\n",
      "epoch: 404 training loss: 37.234398 validation loss: 79.36508\n",
      "epoch: 405 training loss: 37.12285 validation loss: 79.25193\n",
      "epoch: 406 training loss: 37.011913 validation loss: 79.13925\n",
      "epoch: 407 training loss: 36.90155 validation loss: 79.02665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 408 training loss: 36.791603 validation loss: 78.912834\n",
      "epoch: 409 training loss: 36.68217 validation loss: 78.79815\n",
      "epoch: 410 training loss: 36.57319 validation loss: 78.68409\n",
      "epoch: 411 training loss: 36.464607 validation loss: 78.57083\n",
      "epoch: 412 training loss: 36.356342 validation loss: 78.457886\n",
      "epoch: 413 training loss: 36.248436 validation loss: 78.34516\n",
      "epoch: 414 training loss: 36.140923 validation loss: 78.2322\n",
      "epoch: 415 training loss: 36.033813 validation loss: 78.118935\n",
      "epoch: 416 training loss: 35.927223 validation loss: 78.00643\n",
      "epoch: 417 training loss: 35.82122 validation loss: 77.89512\n",
      "epoch: 418 training loss: 35.715683 validation loss: 77.78434\n",
      "epoch: 419 training loss: 35.610504 validation loss: 77.673676\n",
      "epoch: 420 training loss: 35.505825 validation loss: 77.5637\n",
      "epoch: 421 training loss: 35.40165 validation loss: 77.45468\n",
      "epoch: 422 training loss: 35.297924 validation loss: 77.34643\n",
      "epoch: 423 training loss: 35.19465 validation loss: 77.23957\n",
      "epoch: 424 training loss: 35.09181 validation loss: 77.134026\n",
      "epoch: 425 training loss: 34.98932 validation loss: 77.02885\n",
      "epoch: 426 training loss: 34.887062 validation loss: 76.92333\n",
      "epoch: 427 training loss: 34.785057 validation loss: 76.81736\n",
      "epoch: 428 training loss: 34.683407 validation loss: 76.711044\n",
      "epoch: 429 training loss: 34.582375 validation loss: 76.60409\n",
      "epoch: 430 training loss: 34.48208 validation loss: 76.49712\n",
      "epoch: 431 training loss: 34.382263 validation loss: 76.39054\n",
      "epoch: 432 training loss: 34.28309 validation loss: 76.28408\n",
      "epoch: 433 training loss: 34.18454 validation loss: 76.17782\n",
      "epoch: 434 training loss: 34.08651 validation loss: 76.07191\n",
      "epoch: 435 training loss: 33.989067 validation loss: 75.96642\n",
      "epoch: 436 training loss: 33.89203 validation loss: 75.86157\n",
      "epoch: 437 training loss: 33.795303 validation loss: 75.75765\n",
      "epoch: 438 training loss: 33.698967 validation loss: 75.65467\n",
      "epoch: 439 training loss: 33.60308 validation loss: 75.5527\n",
      "epoch: 440 training loss: 33.507698 validation loss: 75.45161\n",
      "epoch: 441 training loss: 33.412766 validation loss: 75.351\n",
      "epoch: 442 training loss: 33.318184 validation loss: 75.250626\n",
      "epoch: 443 training loss: 33.22409 validation loss: 75.150475\n",
      "epoch: 444 training loss: 33.130623 validation loss: 75.050606\n",
      "epoch: 445 training loss: 33.0376 validation loss: 74.95107\n",
      "epoch: 446 training loss: 32.944893 validation loss: 74.85226\n",
      "epoch: 447 training loss: 32.85256 validation loss: 74.7547\n",
      "epoch: 448 training loss: 32.76072 validation loss: 74.65858\n",
      "epoch: 449 training loss: 32.669426 validation loss: 74.563644\n",
      "epoch: 450 training loss: 32.578613 validation loss: 74.469376\n",
      "epoch: 451 training loss: 32.488186 validation loss: 74.37556\n",
      "epoch: 452 training loss: 32.398067 validation loss: 74.28281\n",
      "epoch: 453 training loss: 32.308254 validation loss: 74.191315\n",
      "epoch: 454 training loss: 32.218796 validation loss: 74.10039\n",
      "epoch: 455 training loss: 32.1298 validation loss: 74.009834\n",
      "epoch: 456 training loss: 32.041412 validation loss: 73.920006\n",
      "epoch: 457 training loss: 31.953487 validation loss: 73.83049\n",
      "epoch: 458 training loss: 31.86586 validation loss: 73.74053\n",
      "epoch: 459 training loss: 31.778715 validation loss: 73.64987\n",
      "epoch: 460 training loss: 31.69201 validation loss: 73.558945\n",
      "epoch: 461 training loss: 31.605658 validation loss: 73.46836\n",
      "epoch: 462 training loss: 31.51978 validation loss: 73.378426\n",
      "epoch: 463 training loss: 31.434416 validation loss: 73.2894\n",
      "epoch: 464 training loss: 31.349607 validation loss: 73.20119\n",
      "epoch: 465 training loss: 31.265326 validation loss: 73.113495\n",
      "epoch: 466 training loss: 31.181484 validation loss: 73.02639\n",
      "epoch: 467 training loss: 31.097948 validation loss: 72.93962\n",
      "epoch: 468 training loss: 31.014793 validation loss: 72.85242\n",
      "epoch: 469 training loss: 30.932081 validation loss: 72.76457\n",
      "epoch: 470 training loss: 30.84976 validation loss: 72.67616\n",
      "epoch: 471 training loss: 30.767784 validation loss: 72.58755\n",
      "epoch: 472 training loss: 30.686092 validation loss: 72.49924\n",
      "epoch: 473 training loss: 30.604664 validation loss: 72.41151\n",
      "epoch: 474 training loss: 30.523514 validation loss: 72.32434\n",
      "epoch: 475 training loss: 30.442596 validation loss: 72.237724\n",
      "epoch: 476 training loss: 30.36193 validation loss: 72.15139\n",
      "epoch: 477 training loss: 30.281605 validation loss: 72.06468\n",
      "epoch: 478 training loss: 30.201643 validation loss: 71.977325\n",
      "epoch: 479 training loss: 30.121994 validation loss: 71.88971\n",
      "epoch: 480 training loss: 30.042677 validation loss: 71.80159\n",
      "epoch: 481 training loss: 29.963673 validation loss: 71.712975\n",
      "epoch: 482 training loss: 29.885025 validation loss: 71.624084\n",
      "epoch: 483 training loss: 29.806791 validation loss: 71.53506\n",
      "epoch: 484 training loss: 29.729012 validation loss: 71.44581\n",
      "epoch: 485 training loss: 29.65157 validation loss: 71.356606\n",
      "epoch: 486 training loss: 29.57449 validation loss: 71.26838\n",
      "epoch: 487 training loss: 29.497784 validation loss: 71.18125\n",
      "epoch: 488 training loss: 29.421417 validation loss: 71.094215\n",
      "epoch: 489 training loss: 29.34548 validation loss: 71.00762\n",
      "epoch: 490 training loss: 29.269615 validation loss: 70.922\n",
      "epoch: 491 training loss: 29.193913 validation loss: 70.83649\n",
      "epoch: 492 training loss: 29.118587 validation loss: 70.75081\n",
      "epoch: 493 training loss: 29.04361 validation loss: 70.66442\n",
      "epoch: 494 training loss: 28.96899 validation loss: 70.57668\n",
      "epoch: 495 training loss: 28.894674 validation loss: 70.48879\n",
      "epoch: 496 training loss: 28.820787 validation loss: 70.40204\n",
      "epoch: 497 training loss: 28.74736 validation loss: 70.31623\n",
      "epoch: 498 training loss: 28.674162 validation loss: 70.23127\n",
      "epoch: 499 training loss: 28.601234 validation loss: 70.14715\n",
      "epoch: 500 training loss: 28.528675 validation loss: 70.06289\n",
      "epoch: 501 training loss: 28.456541 validation loss: 69.97843\n",
      "epoch: 502 training loss: 28.38476 validation loss: 69.89459\n",
      "epoch: 503 training loss: 28.31324 validation loss: 69.8114\n",
      "epoch: 504 training loss: 28.24197 validation loss: 69.72909\n",
      "epoch: 505 training loss: 28.170929 validation loss: 69.64774\n",
      "epoch: 506 training loss: 28.10014 validation loss: 69.56693\n",
      "epoch: 507 training loss: 28.029617 validation loss: 69.486496\n",
      "epoch: 508 training loss: 27.959408 validation loss: 69.40636\n",
      "epoch: 509 training loss: 27.88955 validation loss: 69.32641\n",
      "epoch: 510 training loss: 27.820026 validation loss: 69.2465\n",
      "epoch: 511 training loss: 27.750732 validation loss: 69.16668\n",
      "epoch: 512 training loss: 27.68166 validation loss: 69.08689\n",
      "epoch: 513 training loss: 27.612816 validation loss: 69.00732\n",
      "epoch: 514 training loss: 27.544266 validation loss: 68.92789\n",
      "epoch: 515 training loss: 27.476034 validation loss: 68.848434\n",
      "epoch: 516 training loss: 27.408115 validation loss: 68.76935\n",
      "epoch: 517 training loss: 27.340439 validation loss: 68.69079\n",
      "epoch: 518 training loss: 27.273035 validation loss: 68.61245\n",
      "epoch: 519 training loss: 27.205942 validation loss: 68.533585\n",
      "epoch: 520 training loss: 27.139118 validation loss: 68.45418\n",
      "epoch: 521 training loss: 27.072496 validation loss: 68.37457\n",
      "epoch: 522 training loss: 27.006138 validation loss: 68.29513\n",
      "epoch: 523 training loss: 26.940067 validation loss: 68.21593\n",
      "epoch: 524 training loss: 26.874298 validation loss: 68.136925\n",
      "epoch: 525 training loss: 26.808777 validation loss: 68.0576\n",
      "epoch: 526 training loss: 26.743477 validation loss: 67.97819\n",
      "epoch: 527 training loss: 26.67836 validation loss: 67.89873\n",
      "epoch: 528 training loss: 26.613544 validation loss: 67.81938\n",
      "epoch: 529 training loss: 26.549019 validation loss: 67.73975\n",
      "epoch: 530 training loss: 26.48481 validation loss: 67.660255\n",
      "epoch: 531 training loss: 26.420876 validation loss: 67.580536\n",
      "epoch: 532 training loss: 26.357277 validation loss: 67.50116\n",
      "epoch: 533 training loss: 26.29398 validation loss: 67.421745\n",
      "epoch: 534 training loss: 26.230938 validation loss: 67.343025\n",
      "epoch: 535 training loss: 26.168146 validation loss: 67.26475\n",
      "epoch: 536 training loss: 26.105606 validation loss: 67.18784\n",
      "epoch: 537 training loss: 26.043322 validation loss: 67.11136\n",
      "epoch: 538 training loss: 25.981194 validation loss: 67.03603\n",
      "epoch: 539 training loss: 25.919317 validation loss: 66.96114\n",
      "epoch: 540 training loss: 25.857637 validation loss: 66.88758\n",
      "epoch: 541 training loss: 25.796286 validation loss: 66.81441\n",
      "epoch: 542 training loss: 25.735067 validation loss: 66.74235\n",
      "epoch: 543 training loss: 25.674126 validation loss: 66.67021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 544 training loss: 25.61321 validation loss: 66.598946\n",
      "epoch: 545 training loss: 25.552647 validation loss: 66.52807\n",
      "epoch: 546 training loss: 25.492157 validation loss: 66.45843\n",
      "epoch: 547 training loss: 25.43209 validation loss: 66.38911\n",
      "epoch: 548 training loss: 25.372044 validation loss: 66.32076\n",
      "epoch: 549 training loss: 25.312397 validation loss: 66.25265\n",
      "epoch: 550 training loss: 25.252708 validation loss: 66.18555\n",
      "epoch: 551 training loss: 25.193495 validation loss: 66.11795\n",
      "epoch: 552 training loss: 25.134163 validation loss: 66.051\n",
      "epoch: 553 training loss: 25.075375 validation loss: 65.98252\n",
      "epoch: 554 training loss: 25.016296 validation loss: 65.91589\n",
      "epoch: 555 training loss: 24.95803 validation loss: 65.84728\n",
      "epoch: 556 training loss: 24.899403 validation loss: 65.78201\n",
      "epoch: 557 training loss: 24.841816 validation loss: 65.711945\n",
      "epoch: 558 training loss: 24.783371 validation loss: 65.647705\n",
      "epoch: 559 training loss: 24.72655 validation loss: 65.574326\n",
      "epoch: 560 training loss: 24.668198 validation loss: 65.512276\n",
      "epoch: 561 training loss: 24.612434 validation loss: 65.436584\n",
      "epoch: 562 training loss: 24.554268 validation loss: 65.379135\n",
      "epoch: 563 training loss: 24.499691 validation loss: 65.30254\n",
      "epoch: 564 training loss: 24.442022 validation loss: 65.247894\n",
      "epoch: 565 training loss: 24.38802 validation loss: 65.169815\n",
      "epoch: 566 training loss: 24.330978 validation loss: 65.11531\n",
      "epoch: 567 training loss: 24.276897 validation loss: 65.04014\n",
      "epoch: 568 training loss: 24.220535 validation loss: 64.98018\n",
      "epoch: 569 training loss: 24.165089 validation loss: 64.91404\n",
      "epoch: 570 training loss: 24.110138 validation loss: 64.8444\n",
      "epoch: 571 training loss: 24.053112 validation loss: 64.78822\n",
      "epoch: 572 training loss: 23.99942 validation loss: 64.711525\n",
      "epoch: 573 training loss: 23.942131 validation loss: 64.658005\n",
      "epoch: 574 training loss: 23.887983 validation loss: 64.58116\n",
      "epoch: 575 training loss: 23.832327 validation loss: 64.527145\n",
      "epoch: 576 training loss: 23.776978 validation loss: 64.45267\n",
      "epoch: 577 training loss: 23.723347 validation loss: 64.39736\n",
      "epoch: 578 training loss: 23.667004 validation loss: 64.32711\n",
      "epoch: 579 training loss: 23.614845 validation loss: 64.2688\n",
      "epoch: 580 training loss: 23.558662 validation loss: 64.20212\n",
      "epoch: 581 training loss: 23.507668 validation loss: 64.13911\n",
      "epoch: 582 training loss: 23.452257 validation loss: 64.077034\n",
      "epoch: 583 training loss: 23.402275 validation loss: 64.00921\n",
      "epoch: 584 training loss: 23.34734 validation loss: 63.95142\n",
      "epoch: 585 training loss: 23.297062 validation loss: 63.88064\n",
      "epoch: 586 training loss: 23.241962 validation loss: 63.826653\n",
      "epoch: 587 training loss: 23.190023 validation loss: 63.756763\n",
      "epoch: 588 training loss: 23.136066 validation loss: 63.70386\n",
      "epoch: 589 training loss: 23.083061 validation loss: 63.63622\n",
      "epoch: 590 training loss: 23.031132 validation loss: 63.58365\n",
      "epoch: 591 training loss: 22.978186 validation loss: 63.51542\n",
      "epoch: 592 training loss: 22.92744 validation loss: 63.465965\n",
      "epoch: 593 training loss: 22.874893 validation loss: 63.395626\n",
      "epoch: 594 training loss: 22.824715 validation loss: 63.350296\n",
      "epoch: 595 training loss: 22.772697 validation loss: 63.27892\n",
      "epoch: 596 training loss: 22.723095 validation loss: 63.2344\n",
      "epoch: 597 training loss: 22.671263 validation loss: 63.163063\n",
      "epoch: 598 training loss: 22.622553 validation loss: 63.118065\n",
      "epoch: 599 training loss: 22.570864 validation loss: 63.046444\n",
      "epoch: 600 training loss: 22.522694 validation loss: 63.002995\n",
      "epoch: 601 training loss: 22.471596 validation loss: 62.930653\n",
      "epoch: 602 training loss: 22.423306 validation loss: 62.88884\n",
      "epoch: 603 training loss: 22.372974 validation loss: 62.816105\n",
      "epoch: 604 training loss: 22.324223 validation loss: 62.77428\n",
      "epoch: 605 training loss: 22.274416 validation loss: 62.70207\n",
      "epoch: 606 training loss: 22.22556 validation loss: 62.659027\n",
      "epoch: 607 training loss: 22.175781 validation loss: 62.588383\n",
      "epoch: 608 training loss: 22.127373 validation loss: 62.54336\n",
      "epoch: 609 training loss: 22.07743 validation loss: 62.475437\n",
      "epoch: 610 training loss: 22.02956 validation loss: 62.428356\n",
      "epoch: 611 training loss: 21.97961 validation loss: 62.364162\n",
      "epoch: 612 training loss: 21.932392 validation loss: 62.315334\n",
      "epoch: 613 training loss: 21.88251 validation loss: 62.254944\n",
      "epoch: 614 training loss: 21.836163 validation loss: 62.20362\n",
      "epoch: 615 training loss: 21.786327 validation loss: 62.146633\n",
      "epoch: 616 training loss: 21.740704 validation loss: 62.093006\n",
      "epoch: 617 training loss: 21.69103 validation loss: 62.039898\n",
      "epoch: 618 training loss: 21.645763 validation loss: 61.984028\n",
      "epoch: 619 training loss: 21.596466 validation loss: 61.93417\n",
      "epoch: 620 training loss: 21.551151 validation loss: 61.875942\n",
      "epoch: 621 training loss: 21.502544 validation loss: 61.828217\n",
      "epoch: 622 training loss: 21.456932 validation loss: 61.769344\n",
      "epoch: 623 training loss: 21.409204 validation loss: 61.722206\n",
      "epoch: 624 training loss: 21.363174 validation loss: 61.663177\n",
      "epoch: 625 training loss: 21.316502 validation loss: 61.615494\n",
      "epoch: 626 training loss: 21.270073 validation loss: 61.556927\n",
      "epoch: 627 training loss: 21.224636 validation loss: 61.507282\n",
      "epoch: 628 training loss: 21.17789 validation loss: 61.450844\n",
      "epoch: 629 training loss: 21.133791 validation loss: 61.39772\n",
      "epoch: 630 training loss: 21.08661 validation loss: 61.3448\n",
      "epoch: 631 training loss: 21.043642 validation loss: 61.288246\n",
      "epoch: 632 training loss: 20.99604 validation loss: 61.238583\n",
      "epoch: 633 training loss: 20.95399 validation loss: 61.17961\n",
      "epoch: 634 training loss: 20.90636 validation loss: 61.131107\n",
      "epoch: 635 training loss: 20.865173 validation loss: 61.071495\n",
      "epoch: 636 training loss: 20.81813 validation loss: 61.023876\n",
      "epoch: 637 training loss: 20.777521 validation loss: 60.96417\n",
      "epoch: 638 training loss: 20.731354 validation loss: 60.918186\n",
      "epoch: 639 training loss: 20.69014 validation loss: 60.856697\n",
      "epoch: 640 training loss: 20.644867 validation loss: 60.81267\n",
      "epoch: 641 training loss: 20.602467 validation loss: 60.750134\n",
      "epoch: 642 training loss: 20.558231 validation loss: 60.70607\n",
      "epoch: 643 training loss: 20.514545 validation loss: 60.64527\n",
      "epoch: 644 training loss: 20.471842 validation loss: 60.598076\n",
      "epoch: 645 training loss: 20.42696 validation loss: 60.540028\n",
      "epoch: 646 training loss: 20.385872 validation loss: 60.48874\n",
      "epoch: 647 training loss: 20.340042 validation loss: 60.43492\n",
      "epoch: 648 training loss: 20.300636 validation loss: 60.37782\n",
      "epoch: 649 training loss: 20.253849 validation loss: 60.330414\n",
      "epoch: 650 training loss: 20.216003 validation loss: 60.267956\n",
      "epoch: 651 training loss: 20.16843 validation loss: 60.22653\n",
      "epoch: 652 training loss: 20.13202 validation loss: 60.161167\n",
      "epoch: 653 training loss: 20.08413 validation loss: 60.123447\n",
      "epoch: 654 training loss: 20.049038 validation loss: 60.05696\n",
      "epoch: 655 training loss: 20.001202 validation loss: 60.022346\n",
      "epoch: 656 training loss: 19.966892 validation loss: 59.953644\n",
      "epoch: 657 training loss: 19.918982 validation loss: 59.92189\n",
      "epoch: 658 training loss: 19.884193 validation loss: 59.85253\n",
      "epoch: 659 training loss: 19.836716 validation loss: 59.81957\n",
      "epoch: 660 training loss: 19.799812 validation loss: 59.755424\n",
      "epoch: 661 training loss: 19.75409 validation loss: 59.716404\n",
      "epoch: 662 training loss: 19.714603 validation loss: 59.659393\n",
      "epoch: 663 training loss: 19.671787 validation loss: 59.614773\n",
      "epoch: 664 training loss: 19.630745 validation loss: 59.562416\n",
      "epoch: 665 training loss: 19.590343 validation loss: 59.514767\n",
      "epoch: 666 training loss: 19.548439 validation loss: 59.464268\n",
      "epoch: 667 training loss: 19.509415 validation loss: 59.414886\n",
      "epoch: 668 training loss: 19.46717 validation loss: 59.36533\n",
      "epoch: 669 training loss: 19.428877 validation loss: 59.315422\n",
      "epoch: 670 training loss: 19.38673 validation loss: 59.264835\n",
      "epoch: 671 training loss: 19.349077 validation loss: 59.21663\n",
      "epoch: 672 training loss: 19.307009 validation loss: 59.164284\n",
      "epoch: 673 training loss: 19.26997 validation loss: 59.116768\n",
      "epoch: 674 training loss: 19.227592 validation loss: 59.067055\n",
      "epoch: 675 training loss: 19.190903 validation loss: 59.014786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 676 training loss: 19.148224 validation loss: 58.972157\n",
      "epoch: 677 training loss: 19.112146 validation loss: 58.91173\n",
      "epoch: 678 training loss: 19.06918 validation loss: 58.87674\n",
      "epoch: 679 training loss: 19.033789 validation loss: 58.807423\n",
      "epoch: 680 training loss: 18.99146 validation loss: 58.78026\n",
      "epoch: 681 training loss: 18.955627 validation loss: 58.703785\n",
      "epoch: 682 training loss: 18.915022 validation loss: 58.681168\n",
      "epoch: 683 training loss: 18.875902 validation loss: 58.612366\n",
      "epoch: 684 training loss: 18.837055 validation loss: 58.585884\n",
      "epoch: 685 training loss: 18.796316 validation loss: 58.515305\n",
      "epoch: 686 training loss: 18.759827 validation loss: 58.489227\n",
      "epoch: 687 training loss: 18.71949 validation loss: 58.418438\n",
      "epoch: 688 training loss: 18.684433 validation loss: 58.38753\n",
      "epoch: 689 training loss: 18.644028 validation loss: 58.319805\n",
      "epoch: 690 training loss: 18.6089 validation loss: 58.287605\n",
      "epoch: 691 training loss: 18.568321 validation loss: 58.221695\n",
      "epoch: 692 training loss: 18.532715 validation loss: 58.185703\n",
      "epoch: 693 training loss: 18.492058 validation loss: 58.12322\n",
      "epoch: 694 training loss: 18.456495 validation loss: 58.080456\n",
      "epoch: 695 training loss: 18.416073 validation loss: 58.022552\n",
      "epoch: 696 training loss: 18.381348 validation loss: 57.97954\n",
      "epoch: 697 training loss: 18.341204 validation loss: 57.91567\n",
      "epoch: 698 training loss: 18.30627 validation loss: 57.881786\n",
      "epoch: 699 training loss: 18.267635 validation loss: 57.814426\n",
      "epoch: 700 training loss: 18.231186 validation loss: 57.784866\n",
      "epoch: 701 training loss: 18.194519 validation loss: 57.71549\n",
      "epoch: 702 training loss: 18.158463 validation loss: 57.683613\n",
      "epoch: 703 training loss: 18.123972 validation loss: 57.616196\n",
      "epoch: 704 training loss: 18.0891 validation loss: 57.58374\n",
      "epoch: 705 training loss: 18.055088 validation loss: 57.514248\n",
      "epoch: 706 training loss: 18.0186 validation loss: 57.482254\n",
      "epoch: 707 training loss: 17.977932 validation loss: 57.419376\n",
      "epoch: 708 training loss: 17.941044 validation loss: 57.378143\n",
      "epoch: 709 training loss: 17.89777 validation loss: 57.33132\n",
      "epoch: 710 training loss: 17.86552 validation loss: 57.275837\n",
      "epoch: 711 training loss: 17.82444 validation loss: 57.235638\n",
      "epoch: 712 training loss: 17.794067 validation loss: 57.173206\n",
      "epoch: 713 training loss: 17.756302 validation loss: 57.137623\n",
      "epoch: 714 training loss: 17.72319 validation loss: 57.07358\n",
      "epoch: 715 training loss: 17.686344 validation loss: 57.04509\n",
      "epoch: 716 training loss: 17.650745 validation loss: 56.978798\n",
      "epoch: 717 training loss: 17.614273 validation loss: 56.946133\n",
      "epoch: 718 training loss: 17.577702 validation loss: 56.88593\n",
      "epoch: 719 training loss: 17.545532 validation loss: 56.841938\n",
      "epoch: 720 training loss: 17.50704 validation loss: 56.791626\n",
      "epoch: 721 training loss: 17.476072 validation loss: 56.744843\n",
      "epoch: 722 training loss: 17.435307 validation loss: 56.70201\n",
      "epoch: 723 training loss: 17.40435 validation loss: 56.646385\n",
      "epoch: 724 training loss: 17.364666 validation loss: 56.610107\n",
      "epoch: 725 training loss: 17.334225 validation loss: 56.547035\n",
      "epoch: 726 training loss: 17.297165 validation loss: 56.516068\n",
      "epoch: 727 training loss: 17.267807 validation loss: 56.447956\n",
      "epoch: 728 training loss: 17.235138 validation loss: 56.4211\n",
      "epoch: 729 training loss: 17.20593 validation loss: 56.348076\n",
      "epoch: 730 training loss: 17.174236 validation loss: 56.329258\n",
      "epoch: 731 training loss: 17.136185 validation loss: 56.262115\n",
      "epoch: 732 training loss: 17.104996 validation loss: 56.23354\n",
      "epoch: 733 training loss: 17.06253 validation loss: 56.180935\n",
      "epoch: 734 training loss: 17.036123 validation loss: 56.13916\n",
      "epoch: 735 training loss: 16.993309 validation loss: 56.087936\n",
      "epoch: 736 training loss: 16.96907 validation loss: 56.047146\n",
      "epoch: 737 training loss: 16.926805 validation loss: 55.998653\n",
      "epoch: 738 training loss: 16.902832 validation loss: 55.941807\n",
      "epoch: 739 training loss: 16.86346 validation loss: 55.917126\n",
      "epoch: 740 training loss: 16.838818 validation loss: 55.840374\n",
      "epoch: 741 training loss: 16.803953 validation loss: 55.829468\n",
      "epoch: 742 training loss: 16.773071 validation loss: 55.755997\n",
      "epoch: 743 training loss: 16.742897 validation loss: 55.730587\n",
      "epoch: 744 training loss: 16.704178 validation loss: 55.67538\n",
      "epoch: 745 training loss: 16.680042 validation loss: 55.63347\n",
      "epoch: 746 training loss: 16.637505 validation loss: 55.58464\n",
      "epoch: 747 training loss: 16.616425 validation loss: 55.548237\n",
      "epoch: 748 training loss: 16.571524 validation loss: 55.490303\n",
      "epoch: 749 training loss: 16.547657 validation loss: 55.46399\n",
      "epoch: 750 training loss: 16.504963 validation loss: 55.407413\n",
      "epoch: 751 training loss: 16.479103 validation loss: 55.379654\n",
      "epoch: 752 training loss: 16.441402 validation loss: 55.327488\n",
      "epoch: 753 training loss: 16.414513 validation loss: 55.28919\n",
      "epoch: 754 training loss: 16.380005 validation loss: 55.246464\n",
      "epoch: 755 training loss: 16.355244 validation loss: 55.191013\n",
      "epoch: 756 training loss: 16.32568 validation loss: 55.1656\n",
      "epoch: 757 training loss: 16.301962 validation loss: 55.099926\n",
      "epoch: 758 training loss: 16.272707 validation loss: 55.064823\n",
      "epoch: 759 training loss: 16.239933 validation loss: 55.03169\n",
      "epoch: 760 training loss: 16.210506 validation loss: 54.976715\n",
      "epoch: 761 training loss: 16.173525 validation loss: 54.95678\n",
      "epoch: 762 training loss: 16.148975 validation loss: 54.89461\n",
      "epoch: 763 training loss: 16.114017 validation loss: 54.870144\n",
      "epoch: 764 training loss: 16.09515 validation loss: 54.81087\n",
      "epoch: 765 training loss: 16.058514 validation loss: 54.788097\n",
      "epoch: 766 training loss: 16.039616 validation loss: 54.728184\n",
      "epoch: 767 training loss: 16.001902 validation loss: 54.70661\n",
      "epoch: 768 training loss: 15.973772 validation loss: 54.64169\n",
      "epoch: 769 training loss: 15.940614 validation loss: 54.623363\n",
      "epoch: 770 training loss: 15.907478 validation loss: 54.569553\n",
      "epoch: 771 training loss: 15.879494 validation loss: 54.5201\n",
      "epoch: 772 training loss: 15.84337 validation loss: 54.50816\n",
      "epoch: 773 training loss: 15.8183 validation loss: 54.430042\n",
      "epoch: 774 training loss: 15.782853 validation loss: 54.4203\n",
      "epoch: 775 training loss: 15.753418 validation loss: 54.35578\n",
      "epoch: 776 training loss: 15.721539 validation loss: 54.341824\n",
      "epoch: 777 training loss: 15.686855 validation loss: 54.295685\n",
      "epoch: 778 training loss: 15.66085 validation loss: 54.259773\n",
      "epoch: 779 training loss: 15.624203 validation loss: 54.212788\n",
      "epoch: 780 training loss: 15.605239 validation loss: 54.173214\n",
      "epoch: 781 training loss: 15.567474 validation loss: 54.1226\n",
      "epoch: 782 training loss: 15.549434 validation loss: 54.095123\n",
      "epoch: 783 training loss: 15.512205 validation loss: 54.040443\n",
      "epoch: 784 training loss: 15.493852 validation loss: 54.006603\n",
      "epoch: 785 training loss: 15.460548 validation loss: 53.97419\n",
      "epoch: 786 training loss: 15.441477 validation loss: 53.915276\n",
      "epoch: 787 training loss: 15.410441 validation loss: 53.905937\n",
      "epoch: 788 training loss: 15.386526 validation loss: 53.835896\n",
      "epoch: 789 training loss: 15.362913 validation loss: 53.818142\n",
      "epoch: 790 training loss: 15.333406 validation loss: 53.768013\n",
      "epoch: 791 training loss: 15.31161 validation loss: 53.72266\n",
      "epoch: 792 training loss: 15.279963 validation loss: 53.687874\n",
      "epoch: 793 training loss: 15.257513 validation loss: 53.635403\n",
      "epoch: 794 training loss: 15.2301655 validation loss: 53.596107\n",
      "epoch: 795 training loss: 15.202839 validation loss: 53.57135\n",
      "epoch: 796 training loss: 15.179193 validation loss: 53.509674\n",
      "epoch: 797 training loss: 15.147042 validation loss: 53.49612\n",
      "epoch: 798 training loss: 15.115446 validation loss: 53.442657\n",
      "epoch: 799 training loss: 15.078133 validation loss: 53.41397\n",
      "epoch: 800 training loss: 15.05509 validation loss: 53.383022\n",
      "epoch: 801 training loss: 15.02211 validation loss: 53.337135\n",
      "epoch: 802 training loss: 14.99849 validation loss: 53.30103\n",
      "epoch: 803 training loss: 14.967634 validation loss: 53.26796\n",
      "epoch: 804 training loss: 14.939753 validation loss: 53.21326\n",
      "epoch: 805 training loss: 14.916213 validation loss: 53.19903\n",
      "epoch: 806 training loss: 14.885285 validation loss: 53.120583\n",
      "epoch: 807 training loss: 14.866093 validation loss: 53.115948\n",
      "epoch: 808 training loss: 14.828882 validation loss: 53.044342\n",
      "epoch: 809 training loss: 14.811953 validation loss: 53.02853\n",
      "epoch: 810 training loss: 14.769008 validation loss: 52.98244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 811 training loss: 14.755772 validation loss: 52.953724\n",
      "epoch: 812 training loss: 14.712399 validation loss: 52.913723\n",
      "epoch: 813 training loss: 14.698637 validation loss: 52.86954\n",
      "epoch: 814 training loss: 14.659746 validation loss: 52.843723\n",
      "epoch: 815 training loss: 14.646137 validation loss: 52.786655\n",
      "epoch: 816 training loss: 14.611044 validation loss: 52.780704\n",
      "epoch: 817 training loss: 14.595844 validation loss: 52.70065\n",
      "epoch: 818 training loss: 14.569933 validation loss: 52.71031\n",
      "epoch: 819 training loss: 14.545639 validation loss: 52.623493\n",
      "epoch: 820 training loss: 14.524572 validation loss: 52.6232\n",
      "epoch: 821 training loss: 14.492503 validation loss: 52.560425\n",
      "epoch: 822 training loss: 14.478805 validation loss: 52.53633\n",
      "epoch: 823 training loss: 14.443363 validation loss: 52.49118\n",
      "epoch: 824 training loss: 14.431169 validation loss: 52.45851\n",
      "epoch: 825 training loss: 14.394459 validation loss: 52.418213\n",
      "epoch: 826 training loss: 14.382212 validation loss: 52.384514\n",
      "epoch: 827 training loss: 14.351192 validation loss: 52.341717\n",
      "epoch: 828 training loss: 14.332292 validation loss: 52.298943\n",
      "epoch: 829 training loss: 14.299191 validation loss: 52.26298\n",
      "epoch: 830 training loss: 14.27059 validation loss: 52.2257\n",
      "epoch: 831 training loss: 14.24164 validation loss: 52.186977\n",
      "epoch: 832 training loss: 14.21139 validation loss: 52.164066\n",
      "epoch: 833 training loss: 14.190161 validation loss: 52.122017\n",
      "epoch: 834 training loss: 14.158766 validation loss: 52.094322\n",
      "epoch: 835 training loss: 14.143655 validation loss: 52.060524\n",
      "epoch: 836 training loss: 14.111702 validation loss: 52.022415\n",
      "epoch: 837 training loss: 14.094139 validation loss: 51.984425\n",
      "epoch: 838 training loss: 14.066071 validation loss: 51.946968\n",
      "epoch: 839 training loss: 14.046707 validation loss: 51.904644\n",
      "epoch: 840 training loss: 14.022524 validation loss: 51.872143\n",
      "epoch: 841 training loss: 14.005517 validation loss: 51.82141\n",
      "epoch: 842 training loss: 13.98081 validation loss: 51.791965\n",
      "epoch: 843 training loss: 13.958393 validation loss: 51.751488\n",
      "epoch: 844 training loss: 13.933815 validation loss: 51.720436\n",
      "epoch: 845 training loss: 13.904348 validation loss: 51.69397\n",
      "epoch: 846 training loss: 13.887966 validation loss: 51.644283\n",
      "epoch: 847 training loss: 13.854351 validation loss: 51.631355\n",
      "epoch: 848 training loss: 13.844727 validation loss: 51.56396\n",
      "epoch: 849 training loss: 13.81378 validation loss: 51.56398\n",
      "epoch: 850 training loss: 13.803774 validation loss: 51.48085\n",
      "epoch: 851 training loss: 13.776955 validation loss: 51.495323\n",
      "epoch: 852 training loss: 13.763004 validation loss: 51.416035\n",
      "epoch: 853 training loss: 13.736671 validation loss: 51.419273\n",
      "epoch: 854 training loss: 13.716278 validation loss: 51.35002\n",
      "epoch: 855 training loss: 13.687408 validation loss: 51.340237\n",
      "epoch: 856 training loss: 13.663866 validation loss: 51.274704\n",
      "epoch: 857 training loss: 13.634266 validation loss: 51.270897\n",
      "epoch: 858 training loss: 13.606192 validation loss: 51.21088\n",
      "epoch: 859 training loss: 13.580315 validation loss: 51.19323\n",
      "epoch: 860 training loss: 13.554869 validation loss: 51.156555\n",
      "epoch: 861 training loss: 13.532557 validation loss: 51.118187\n",
      "epoch: 862 training loss: 13.513135 validation loss: 51.08994\n",
      "epoch: 863 training loss: 13.489473 validation loss: 51.064186\n",
      "epoch: 864 training loss: 13.47588 validation loss: 51.020084\n",
      "epoch: 865 training loss: 13.445177 validation loss: 51.0067\n",
      "epoch: 866 training loss: 13.433428 validation loss: 50.94667\n",
      "epoch: 867 training loss: 13.399344 validation loss: 50.937954\n",
      "epoch: 868 training loss: 13.391708 validation loss: 50.8896\n",
      "epoch: 869 training loss: 13.356623 validation loss: 50.873493\n",
      "epoch: 870 training loss: 13.350826 validation loss: 50.82347\n",
      "epoch: 871 training loss: 13.323278 validation loss: 50.808796\n",
      "epoch: 872 training loss: 13.31686 validation loss: 50.739838\n",
      "epoch: 873 training loss: 13.289181 validation loss: 50.737385\n",
      "epoch: 874 training loss: 13.276473 validation loss: 50.658386\n",
      "epoch: 875 training loss: 13.256073 validation loss: 50.6793\n",
      "epoch: 876 training loss: 13.240208 validation loss: 50.593456\n",
      "epoch: 877 training loss: 13.218925 validation loss: 50.59173\n",
      "epoch: 878 training loss: 13.177 validation loss: 50.541286\n",
      "epoch: 879 training loss: 13.152807 validation loss: 50.543087\n",
      "epoch: 880 training loss: 13.118828 validation loss: 50.481133\n",
      "epoch: 881 training loss: 13.102375 validation loss: 50.47377\n",
      "epoch: 882 training loss: 13.07806 validation loss: 50.41908\n",
      "epoch: 883 training loss: 13.062254 validation loss: 50.411053\n",
      "epoch: 884 training loss: 13.039211 validation loss: 50.342815\n",
      "epoch: 885 training loss: 13.024588 validation loss: 50.34351\n",
      "epoch: 886 training loss: 12.994593 validation loss: 50.296513\n",
      "epoch: 887 training loss: 12.978622 validation loss: 50.27739\n",
      "epoch: 888 training loss: 12.947615 validation loss: 50.247253\n",
      "epoch: 889 training loss: 12.940946 validation loss: 50.205696\n",
      "epoch: 890 training loss: 12.910885 validation loss: 50.194553\n",
      "epoch: 891 training loss: 12.907593 validation loss: 50.129322\n",
      "epoch: 892 training loss: 12.87727 validation loss: 50.13839\n",
      "epoch: 893 training loss: 12.871729 validation loss: 50.046223\n",
      "epoch: 894 training loss: 12.843712 validation loss: 50.08497\n",
      "epoch: 895 training loss: 12.828169 validation loss: 49.98658\n",
      "epoch: 896 training loss: 12.814957 validation loss: 50.018734\n",
      "epoch: 897 training loss: 12.784618 validation loss: 49.9338\n",
      "epoch: 898 training loss: 12.775037 validation loss: 49.94043\n",
      "epoch: 899 training loss: 12.733651 validation loss: 49.893257\n",
      "epoch: 900 training loss: 12.7292 validation loss: 49.877083\n",
      "epoch: 901 training loss: 12.686328 validation loss: 49.850624\n",
      "epoch: 902 training loss: 12.674904 validation loss: 49.79496\n",
      "epoch: 903 training loss: 12.645781 validation loss: 49.78953\n",
      "epoch: 904 training loss: 12.625544 validation loss: 49.728275\n",
      "epoch: 905 training loss: 12.612389 validation loss: 49.72402\n",
      "epoch: 906 training loss: 12.590689 validation loss: 49.65832\n",
      "epoch: 907 training loss: 12.575333 validation loss: 49.655697\n",
      "epoch: 908 training loss: 12.546955 validation loss: 49.604366\n",
      "epoch: 909 training loss: 12.52821 validation loss: 49.6026\n",
      "epoch: 910 training loss: 12.503422 validation loss: 49.55713\n",
      "epoch: 911 training loss: 12.485802 validation loss: 49.54414\n",
      "epoch: 912 training loss: 12.457784 validation loss: 49.489857\n",
      "epoch: 913 training loss: 12.439053 validation loss: 49.477913\n",
      "epoch: 914 training loss: 12.411808 validation loss: 49.435844\n",
      "epoch: 915 training loss: 12.396769 validation loss: 49.407112\n",
      "epoch: 916 training loss: 12.375492 validation loss: 49.386635\n",
      "epoch: 917 training loss: 12.355492 validation loss: 49.34447\n",
      "epoch: 918 training loss: 12.337364 validation loss: 49.32983\n",
      "epoch: 919 training loss: 12.312498 validation loss: 49.28914\n",
      "epoch: 920 training loss: 12.302893 validation loss: 49.259468\n",
      "epoch: 921 training loss: 12.27777 validation loss: 49.2273\n",
      "epoch: 922 training loss: 12.264118 validation loss: 49.199364\n",
      "epoch: 923 training loss: 12.239246 validation loss: 49.17062\n",
      "epoch: 924 training loss: 12.223646 validation loss: 49.13091\n",
      "epoch: 925 training loss: 12.199518 validation loss: 49.111084\n",
      "epoch: 926 training loss: 12.184949 validation loss: 49.069004\n",
      "epoch: 927 training loss: 12.166176 validation loss: 49.052517\n",
      "epoch: 928 training loss: 12.15139 validation loss: 49.00358\n",
      "epoch: 929 training loss: 12.136385 validation loss: 48.98112\n",
      "epoch: 930 training loss: 12.120793 validation loss: 48.95184\n",
      "epoch: 931 training loss: 12.111313 validation loss: 48.929195\n",
      "epoch: 932 training loss: 12.084869 validation loss: 48.890198\n",
      "epoch: 933 training loss: 12.077607 validation loss: 48.87735\n",
      "epoch: 934 training loss: 12.044523 validation loss: 48.851124\n",
      "epoch: 935 training loss: 12.046642 validation loss: 48.81504\n",
      "epoch: 936 training loss: 12.01316 validation loss: 48.800392\n",
      "epoch: 937 training loss: 12.018919 validation loss: 48.721825\n",
      "epoch: 938 training loss: 11.9918375 validation loss: 48.771404\n",
      "epoch: 939 training loss: 11.9799795 validation loss: 48.650288\n",
      "epoch: 940 training loss: 11.961378 validation loss: 48.694942\n",
      "epoch: 941 training loss: 11.929108 validation loss: 48.620728\n",
      "epoch: 942 training loss: 11.909483 validation loss: 48.62333\n",
      "epoch: 943 training loss: 11.877174 validation loss: 48.579872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 944 training loss: 11.866491 validation loss: 48.571877\n",
      "epoch: 945 training loss: 11.844725 validation loss: 48.52268\n",
      "epoch: 946 training loss: 11.831921 validation loss: 48.512188\n",
      "epoch: 947 training loss: 11.805768 validation loss: 48.47109\n",
      "epoch: 948 training loss: 11.792693 validation loss: 48.44603\n",
      "epoch: 949 training loss: 11.772317 validation loss: 48.426773\n",
      "epoch: 950 training loss: 11.764011 validation loss: 48.38207\n",
      "epoch: 951 training loss: 11.750172 validation loss: 48.380974\n",
      "epoch: 952 training loss: 11.740205 validation loss: 48.313625\n",
      "epoch: 953 training loss: 11.720427 validation loss: 48.34713\n",
      "epoch: 954 training loss: 11.70733 validation loss: 48.23811\n",
      "epoch: 955 training loss: 11.6818695 validation loss: 48.304043\n",
      "epoch: 956 training loss: 11.667522 validation loss: 48.1892\n",
      "epoch: 957 training loss: 11.638824 validation loss: 48.23101\n",
      "epoch: 958 training loss: 11.624604 validation loss: 48.143055\n",
      "epoch: 959 training loss: 11.605261 validation loss: 48.160007\n",
      "epoch: 960 training loss: 11.58663 validation loss: 48.10392\n",
      "epoch: 961 training loss: 11.573803 validation loss: 48.101616\n",
      "epoch: 962 training loss: 11.552977 validation loss: 48.052883\n",
      "epoch: 963 training loss: 11.551663 validation loss: 48.04532\n",
      "epoch: 964 training loss: 11.525518 validation loss: 48.0216\n",
      "epoch: 965 training loss: 11.527742 validation loss: 47.965668\n",
      "epoch: 966 training loss: 11.496235 validation loss: 47.988804\n",
      "epoch: 967 training loss: 11.492838 validation loss: 47.890965\n",
      "epoch: 968 training loss: 11.467222 validation loss: 47.946957\n",
      "epoch: 969 training loss: 11.449251 validation loss: 47.840126\n",
      "epoch: 970 training loss: 11.431505 validation loss: 47.87189\n",
      "epoch: 971 training loss: 11.402729 validation loss: 47.8101\n",
      "epoch: 972 training loss: 11.3923025 validation loss: 47.801003\n",
      "epoch: 973 training loss: 11.365646 validation loss: 47.78878\n",
      "epoch: 974 training loss: 11.35589 validation loss: 47.745426\n",
      "epoch: 975 training loss: 11.336119 validation loss: 47.735493\n",
      "epoch: 976 training loss: 11.3200245 validation loss: 47.687195\n",
      "epoch: 977 training loss: 11.305773 validation loss: 47.68775\n",
      "epoch: 978 training loss: 11.287309 validation loss: 47.62262\n",
      "epoch: 979 training loss: 11.275299 validation loss: 47.65807\n",
      "epoch: 980 training loss: 11.252312 validation loss: 47.57646\n",
      "epoch: 981 training loss: 11.242458 validation loss: 47.622383\n",
      "epoch: 982 training loss: 11.231373 validation loss: 47.53462\n",
      "epoch: 983 training loss: 11.225765 validation loss: 47.53522\n",
      "epoch: 984 training loss: 11.205118 validation loss: 47.49809\n",
      "epoch: 985 training loss: 11.191006 validation loss: 47.467945\n",
      "epoch: 986 training loss: 11.165356 validation loss: 47.47056\n",
      "epoch: 987 training loss: 11.151367 validation loss: 47.3898\n",
      "epoch: 988 training loss: 11.136035 validation loss: 47.439632\n",
      "epoch: 989 training loss: 11.11536 validation loss: 47.365635\n",
      "epoch: 990 training loss: 11.109225 validation loss: 47.38648\n",
      "epoch: 991 training loss: 11.087149 validation loss: 47.309124\n",
      "epoch: 992 training loss: 11.084424 validation loss: 47.340637\n",
      "epoch: 993 training loss: 11.066929 validation loss: 47.262783\n",
      "epoch: 994 training loss: 11.047146 validation loss: 47.290176\n",
      "epoch: 995 training loss: 11.038143 validation loss: 47.21429\n",
      "epoch: 996 training loss: 11.019789 validation loss: 47.21027\n",
      "epoch: 997 training loss: 11.000017 validation loss: 47.191277\n",
      "epoch: 998 training loss: 10.987746 validation loss: 47.13969\n",
      "epoch: 999 training loss: 10.964401 validation loss: 47.171032\n"
     ]
    }
   ],
   "source": [
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "\n",
    "# Save the training result or trained and validated model params\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "    # Loop over epochs\n",
    "    epochs=1000\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # The entire dataset or the full batch and theen batch or minibatch\n",
    "        # Extracting the batches/minibacthes of valid size\n",
    "        loss_batch, acc_batch = [], []\n",
    "        for X, Y in get_batches(X=X_train_norm, batch_size=N, y=Y_train_onehot):\n",
    "            \n",
    "            # feeding the training data and fetching the output loss and accuracy and gradients as well;\n",
    "            feed_dict = {inputs_:X, labels_:Y}\n",
    "            fetches = [cost, accuracy, optimizer]\n",
    "            loss, acc, _ = sess.run(feed_dict=feed_dict, fetches=fetches)\n",
    "            loss_batch.append(loss)\n",
    "            acc_batch.append(acc)\n",
    "            \n",
    "        train_loss.append(np.mean(loss_batch))\n",
    "        train_acc.append(np.mean(acc_batch))\n",
    "#         train_loss.append((loss_batch))\n",
    "#         train_acc.append((acc_batch))\n",
    "        \n",
    "        # validation\n",
    "        feed_dict = {inputs_:X_valid_norm, labels_:Y_valid_onehot}\n",
    "        fetches = [cost, accuracy] # this one is not dictinoary but like shape either tuple or arraye or python list\n",
    "        loss, acc = sess.run(feed_dict=feed_dict, fetches=fetches)\n",
    "        valid_loss.append(loss)\n",
    "        valid_acc.append(acc)\n",
    "        \n",
    "        print('epoch:', epoch, 'training loss:', np.mean(loss_batch), 'validation loss:', loss)\n",
    "            \n",
    "    saver.save(save_path='checkpoints/cnn-har.ckpt', sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as mplot\n",
    "# % %matplotlib inline\n",
    "# this is keeping the mpl inline or outline\n",
    "# inline would inside this block and outline/out of block would be out of this block.\n",
    "%matplotlib inline\n",
    "\n",
    "mplot.plot(train_loss, label='har train_loss')\n",
    "mplot.plot(valid_loss, label='har valid_loss')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/cnn-har.ckpt\n",
      "Test loss: 66.048729 Test acc: 0.459655\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore the validated model\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "#     saver.restore(save_path=tf.train.load_checkpoint(ckpt_dir_or_file='checkpoints/cnn-har.ckpt'), sess=sess)\n",
    "    \n",
    "    ################## Test\n",
    "    acc_batch = []\n",
    "    loss_batch = []    \n",
    "    # Loop over batches\n",
    "    for X, Y in get_batches(X=X_test_norm, batch_size=N, y=Y_test_onehot):\n",
    "\n",
    "        # Feed dictionary\n",
    "        feed_dict = {inputs_:X, labels_:Y}\n",
    "        fetches = [cost, accuracy]\n",
    "        loss, acc = sess.run(feed_dict=feed_dict, fetches=fetches)\n",
    "        acc_batch.append(acc)\n",
    "        loss_batch.append(loss)\n",
    "\n",
    "    test_acc.append(np.mean(acc_batch))\n",
    "    test_loss.append(np.mean(loss_batch))\n",
    "\n",
    "    # Print info for every iter/epoch\n",
    "    print(\"Test loss: {:6f}\".format(np.mean(test_loss)),\n",
    "          \"Test acc: {:.6f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
