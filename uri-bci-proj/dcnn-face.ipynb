{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18720, 205, 16) float64 (18720, 1) uint8\n",
      "0.833333333333 0.166666666667 0.0 0.0\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "import scipy.io as spio\n",
    "import numpy as np\n",
    "\n",
    "BahramFace = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/BahramFace.mat')\n",
    "DJFace = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/DJFace.mat')\n",
    "NickFace = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/NickFace.mat')\n",
    "RoohiFace = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/RoohiFace.mat')\n",
    "SarahFace = spio.loadmat(file_name='/home/arasdar/datasets/bci-project-data-RAW/SarahFace.mat')\n",
    "\n",
    "AllData = np.concatenate((BahramFace['Intensification_Data'],\n",
    "                            DJFace['Intensification_Data'],\n",
    "                            NickFace['Intensification_Data'],\n",
    "                            RoohiFace['Intensification_Data'],\n",
    "                            SarahFace['Intensification_Data']), axis=0)\n",
    "\n",
    "AllLabels = np.concatenate((BahramFace['Intensification_Label'],\n",
    "                            DJFace['Intensification_Label'],\n",
    "                            NickFace['Intensification_Label'],\n",
    "                            RoohiFace['Intensification_Label'],\n",
    "                            SarahFace['Intensification_Label']), axis=0)\n",
    "\n",
    "print(AllData.shape, AllData.dtype, AllLabels.shape, AllLabels.dtype)\n",
    "print(np.mean(AllLabels==0), np.mean(AllLabels==1), np.mean(AllLabels==2), np.mean(AllLabels==3))\n",
    "print((AllLabels +  1).max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13104, 205, 16) (5616, 205, 16) (13104, 1) (5616, 1)\n"
     ]
    }
   ],
   "source": [
    "# Train and test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_valid, X_test, Y_train_valid, Y_test = train_test_split(AllData, AllLabels, test_size=0.30)\n",
    "\n",
    "print(X_train_valid.shape, X_test.shape, Y_train_valid.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.830738705739 0.169261294261 0.0\n",
      "0.0 0.839387464387 0.160612535613 0.0\n",
      "(5616, 2) float64\n"
     ]
    }
   ],
   "source": [
    "from utilities import *\n",
    "\n",
    "# Normalizing/standardizing the input data features\n",
    "X_train_valid_norm, X_test_norm = standardize(test=X_test, train=X_train_valid)\n",
    "\n",
    "# Onehot encoding/vectorizing the output data labels\n",
    "print(np.mean((Y_train_valid+1).reshape(-1)==0), np.mean((Y_train_valid+1).reshape(-1)==1),\n",
    "     np.mean((Y_train_valid+1).reshape(-1)==2), np.mean((Y_train_valid+1).reshape(-1)==3))\n",
    "\n",
    "print(np.mean((Y_test+1).reshape(-1)==0), np.mean((Y_test+1).reshape(-1)==1),\n",
    "     np.mean((Y_test+1).reshape(-1)==2), np.mean((Y_test+1).reshape(-1)==3))\n",
    "\n",
    "# Y_train_valid_onehot = one_hot(labels=(Y_train_valid+1).reshape(-1), n_class=2) \n",
    "# print(Y_train_valid_onehot.shape, Y_train_valid_onehot.dtype, \n",
    "Y_test_onehot = one_hot(labels=(Y_test+1).reshape(-1), n_class=2) \n",
    "print(Y_test_onehot.shape, Y_test_onehot.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_valid, X_test, Y_train_valid, Y_test = train_test_split(AllData, AllLabels, test_size=0.30)\n",
    "X_train_norm, X_valid_norm, Y_train, Y_valid = train_test_split(X_train_valid_norm, Y_train_valid, \n",
    "                                                                              test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches2(X_norm, Y_labels, kernel_size_ratio=1, strides_ratio=1):\n",
    "    # X normalized and Y NOT onehot encoded/vectorized\n",
    "    X, Y = X_norm, Y_labels\n",
    "    AllLabels = Y_labels # 100%\n",
    "\n",
    "    # non = 0 is 87%  AllLabelZero\n",
    "    # tgt = 1 is 13%  AllLabelOne\n",
    "    AllLabelZero = (AllLabels==0).reshape(-1) # 87%\n",
    "    AllLabelOne = (AllLabels==1).reshape(-1) # 13%\n",
    "\n",
    "    X_non, Y_non = X[AllLabelZero], Y[AllLabelZero] # 87%\n",
    "    X_tgt, Y_tgt = X[AllLabelOne], Y[AllLabelOne] # 13%\n",
    "#     print('X_non.shape, Y_non.shape', X_non.shape, Y_non.shape)\n",
    "#     print('X_tgt.shape, Y_tgt.shape', X_tgt.shape, Y_tgt.shape)\n",
    "\n",
    "    # Non-target batch size for get_batches from non-target data\n",
    "    batch_size = X_tgt.shape[0] # 13% -> tgt = 1 is 13%  AllLabelOne\n",
    "    assert X_tgt.shape[0] == Y_tgt.shape[0]\n",
    "#     print('batch_size', batch_size)\n",
    "    \n",
    "    # Convolvolutional minibatching technique\n",
    "    (inputs, filters, kernel_size, strides, padding) = (X_non, 1, \n",
    "                                                        int(batch_size//kernel_size_ratio), \n",
    "                                                        int(batch_size//strides_ratio), \n",
    "                                                        0)\n",
    "#     print('inputs.shape, filters, kernel_size, strides, padding', inputs.shape, filters, \n",
    "#           kernel_size, strides, padding)\n",
    "    n_batches = int((inputs.shape[0] - kernel_size + (2*padding))//strides  + 1) \n",
    "#     print('n_batches', n_batches)\n",
    "    \n",
    "    # Loop over target batches: start, stop, step\n",
    "    for i in range(0, n_batches, 1):\n",
    "        each_X_norm = np.concatenate((X_non[(i*strides):((i*strides)+kernel_size)], X_tgt), axis=0)\n",
    "        each_Y = np.concatenate((Y_non[(i*strides):((i*strides)+kernel_size)], Y_tgt), axis=0)\n",
    "        each_Y_onehot = one_hot(labels=(each_Y+1).reshape(-1), n_class=2)\n",
    "#         print('each_X_norm.shape, each_Y_onehot.shape', each_X_norm.shape, each_Y_onehot.shape)\n",
    "#         print('np.mean(each_Y==0), np.mean(each_Y==1)', np.mean(each_Y==0), np.mean(each_Y==1))\n",
    "        yield each_X_norm, each_Y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object get_batches2 at 0x7feae4553f10>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batches2(X_norm=X_valid_norm, Y_labels=Y_valid, kernel_size_ratio=1, strides_ratio=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object get_batches2 at 0x7feae4553e08>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batches2(X_norm=X_train_norm, Y_labels=Y_train, kernel_size_ratio=1, strides_ratio=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len, n_channels 205 16\n",
      "n_classes [2]\n"
     ]
    }
   ],
   "source": [
    "## Hyperparameters\n",
    "# Input data\n",
    "# batch_size = X_train_norm.shape[0]// 100 # minibatch size & number of minibatches\n",
    "seq_len = X_train_norm.shape[1] # Number of steps: each trial length\n",
    "n_channels = X_train_norm.shape[2] # number of channels in each trial\n",
    "# print('batch_size, seq_len, n_channels', batch_size, seq_len, n_channels)\n",
    "print('seq_len, n_channels', seq_len, n_channels)\n",
    "\n",
    "# Output labels\n",
    "n_classes = Y_train_valid.max(axis=0)+1\n",
    "assert Y_train_valid.max(axis=0) == Y_test.max(axis=0)\n",
    "print('n_classes', n_classes)\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = 0.0001 #1e-4\n",
    "keep_prob = 0.50 # 90% neurons are kept and 10% are dropped out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.3.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_.shape, labels_.shape (?, 205, 16) (?, 2)\n"
     ]
    }
   ],
   "source": [
    "# Feed the data from python/numpy to tensorflow framework\n",
    "inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs_')\n",
    "labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels_')\n",
    "keep_prob_ = tf.placeholder(tf.float32, name = 'keep_prob_')\n",
    "learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate_')\n",
    "print('inputs_.shape, labels_.shape', inputs_.shape, labels_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_.shape, conv1.shape, max_pool_1.shape (?, 205, 16) (?, 204, 32) (?, 102, 32)\n",
      "max_pool_1.shape, conv2.shape, max_pool_2.shape (?, 102, 32) (?, 102, 64) (?, 51, 64)\n",
      "max_pool_2.shape, conv3.shape, max_pool_3.shape (?, 51, 64) (?, 50, 128) (?, 25, 128)\n",
      "max_pool_3.shape, conv4.shape, max_pool_4.shape (?, 25, 128) (?, 24, 256) (?, 12, 256)\n",
      "max_pool_4.shape, flat.shape, dense.shape: (?, 12, 256) (?, 3072) (?, 6144)\n",
      "dense.shape, logits.shape: (?, 6144) (?, 2)\n"
     ]
    }
   ],
   "source": [
    "# inputs_.shape, labels_.shape (?, 205, 16) (?, 2)\n",
    "\n",
    "# (batch, 205, 16) --> (batch, 102, 32)\n",
    "conv1 = tf.layers.conv1d(inputs=inputs_, filters=32, kernel_size=2, strides=1, padding='valid')\n",
    "conv1 = tf.layers.batch_normalization(inputs=conv1)\n",
    "conv1 = tf.nn.relu(features=conv1)\n",
    "max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=2, strides=2, padding='same')\n",
    "max_pool_1 = tf.nn.dropout(x=max_pool_1, keep_prob=keep_prob_)\n",
    "print('inputs_.shape, conv1.shape, max_pool_1.shape', inputs_.shape, conv1.shape, max_pool_1.shape)\n",
    "\n",
    "# (batch, 102, 32) --> (batch, 51, 64)\n",
    "conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=64, kernel_size=2, strides=1, padding='same')\n",
    "conv2 = tf.layers.batch_normalization(inputs=conv2)\n",
    "conv2 = tf.nn.relu(features=conv2)\n",
    "max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=2, padding='same')\n",
    "max_pool_2 = tf.nn.dropout(max_pool_2, keep_prob=keep_prob_)\n",
    "print('max_pool_1.shape, conv2.shape, max_pool_2.shape', max_pool_1.shape, conv2.shape, max_pool_2.shape)\n",
    "\n",
    "# (batch, 51, 64) --> (batch, 25, 128)\n",
    "conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=128, kernel_size=2, strides=1, padding='valid')\n",
    "conv3 = tf.layers.batch_normalization(inputs=conv3)\n",
    "conv3 = tf.nn.relu(features=conv3)\n",
    "max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=2, strides=2, padding='same')\n",
    "max_pool_3 = tf.nn.dropout(max_pool_3, keep_prob=keep_prob_)\n",
    "print('max_pool_2.shape, conv3.shape, max_pool_3.shape', max_pool_2.shape, conv3.shape, max_pool_3.shape)\n",
    "\n",
    "# (batch, 25, 128) --> (batch, 12, 256)\n",
    "conv4 = tf.layers.conv1d(inputs=max_pool_3, filters=256, kernel_size=2, strides=1, padding='valid')\n",
    "conv4 = tf.layers.batch_normalization(inputs=conv4)\n",
    "conv4 = tf.nn.relu(features=conv4)\n",
    "max_pool_4 = tf.layers.max_pooling1d(inputs=conv4, pool_size=2, strides=2, padding='same')\n",
    "max_pool_4 = tf.nn.dropout(max_pool_4, keep_prob=keep_prob_)\n",
    "print('max_pool_3.shape, conv4.shape, max_pool_4.shape', max_pool_3.shape, conv4.shape, max_pool_4.shape)\n",
    "\n",
    "# (batch, 12, 256) --> (batch, 12*256) --> (batch, 12*256*2)\n",
    "flat = tf.reshape(max_pool_4, (-1, 12*256))\n",
    "dense = tf.layers.dense(flat, 12*256*2)\n",
    "dense = tf.layers.batch_normalization(inputs=dense)\n",
    "dense = tf.nn.relu(features=dense)\n",
    "dense = tf.nn.dropout(dense, keep_prob=keep_prob_)\n",
    "print('max_pool_4.shape, flat.shape, dense.shape:', max_pool_4.shape, flat.shape, dense.shape)\n",
    "\n",
    "# (batch, 12*256*2) --> (batch, 2)\n",
    "logits = tf.layers.dense(dense, n_classes)\n",
    "probs = tf.nn.softmax(logits=logits)\n",
    "print('dense.shape, logits.shape:', dense.shape, logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost, cost_ave: Tensor(\"Reshape_11:0\", shape=(?,), dtype=float32) Tensor(\"Mean_4:0\", shape=(), dtype=float32)\n",
      "optimizer: name: \"Adam_2\"\n",
      "op: \"NoOp\"\n",
      "input: \"^Adam_2/update_conv1d_8/kernel/ApplyAdam\"\n",
      "input: \"^Adam_2/update_conv1d_8/bias/ApplyAdam\"\n",
      "input: \"^Adam_2/update_batch_normalization_10/beta/ApplyAdam\"\n",
      "input: \"^Adam_2/update_batch_normalization_10/gamma/ApplyAdam\"\n",
      "input: \"^Adam_2/update_conv1d_9/kernel/ApplyAdam\"\n",
      "input: \"^Adam_2/update_conv1d_9/bias/ApplyAdam\"\n",
      "input: \"^Adam_2/update_batch_normalization_11/beta/ApplyAdam\"\n",
      "input: \"^Adam_2/update_batch_normalization_11/gamma/ApplyAdam\"\n",
      "input: \"^Adam_2/update_conv1d_10/kernel/ApplyAdam\"\n",
      "input: \"^Adam_2/update_conv1d_10/bias/ApplyAdam\"\n",
      "input: \"^Adam_2/update_batch_normalization_12/beta/ApplyAdam\"\n",
      "input: \"^Adam_2/update_batch_normalization_12/gamma/ApplyAdam\"\n",
      "input: \"^Adam_2/update_conv1d_11/kernel/ApplyAdam\"\n",
      "input: \"^Adam_2/update_conv1d_11/bias/ApplyAdam\"\n",
      "input: \"^Adam_2/update_batch_normalization_13/beta/ApplyAdam\"\n",
      "input: \"^Adam_2/update_batch_normalization_13/gamma/ApplyAdam\"\n",
      "input: \"^Adam_2/update_dense_4/kernel/ApplyAdam\"\n",
      "input: \"^Adam_2/update_dense_4/bias/ApplyAdam\"\n",
      "input: \"^Adam_2/update_batch_normalization_14/beta/ApplyAdam\"\n",
      "input: \"^Adam_2/update_batch_normalization_14/gamma/ApplyAdam\"\n",
      "input: \"^Adam_2/update_dense_5/kernel/ApplyAdam\"\n",
      "input: \"^Adam_2/update_dense_5/bias/ApplyAdam\"\n",
      "input: \"^Adam_2/Assign\"\n",
      "input: \"^Adam_2/Assign_1\"\n",
      "\n",
      "correct_pred, correct_pred_ave: Tensor(\"Equal_2:0\", shape=(?,), dtype=bool) Tensor(\"Mean_5:0\", shape=(), dtype=float32)\n",
      "confusion Tensor(\"confusion_matrix_2/SparseTensorDenseAdd:0\", shape=(?, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Backward pass: error backpropagation\n",
    "# Cost function\n",
    "cost = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_)\n",
    "cost_ave = tf.reduce_mean(input_tensor=cost)\n",
    "print('cost, cost_ave:', cost, cost_ave)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_).minimize(cost)\n",
    "print('optimizer:', optimizer)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(x=tf.argmax(input=probs, axis=1), y=tf.argmax(input=labels_, axis=1))\n",
    "correct_pred_ave = tf.reduce_mean(input_tensor=tf.cast(x=correct_pred, dtype=tf.float32))\n",
    "print('correct_pred, correct_pred_ave:', correct_pred, correct_pred_ave)\n",
    "\n",
    "# Confusion matrix\n",
    "confusion = tf.confusion_matrix(predictions=tf.argmax(input=probs, axis=1), \n",
    "                                labels=tf.argmax(input=labels_, axis=1), dtype=tf.float32)\n",
    "print('confusion', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1000 Train loss: 1.541189 Valid loss: 0.767307 Train acc: 0.499428 Valid acc: 0.500000\n",
      "Epoch: 2/1000 Train loss: 1.422981 Valid loss: 0.740371 Train acc: 0.500531 Valid acc: 0.503815\n",
      "Epoch: 3/1000 Train loss: 1.312075 Valid loss: 0.727962 Train acc: 0.501171 Valid acc: 0.503921\n",
      "Epoch: 4/1000 Train loss: 1.223123 Valid loss: 0.720068 Train acc: 0.501103 Valid acc: 0.503793\n",
      "Epoch: 5/1000 Train loss: 1.140298 Valid loss: 0.714903 Train acc: 0.502533 Valid acc: 0.504161\n",
      "Epoch: 6/1000 Train loss: 1.083531 Valid loss: 0.710772 Train acc: 0.504003 Valid acc: 0.509766\n",
      "Epoch: 7/1000 Train loss: 1.039944 Valid loss: 0.707583 Train acc: 0.504902 Valid acc: 0.514340\n",
      "Epoch: 8/1000 Train loss: 1.003322 Valid loss: 0.705173 Train acc: 0.506495 Valid acc: 0.516255\n",
      "Epoch: 9/1000 Train loss: 0.974504 Valid loss: 0.703205 Train acc: 0.508515 Valid acc: 0.518910\n",
      "Epoch: 10/1000 Train loss: 0.950856 Valid loss: 0.701481 Train acc: 0.509387 Valid acc: 0.532022\n",
      "Epoch: 11/1000 Train loss: 0.930176 Valid loss: 0.700040 Train acc: 0.510740 Valid acc: 0.536977\n",
      "Epoch: 12/1000 Train loss: 0.912344 Valid loss: 0.698816 Train acc: 0.512507 Valid acc: 0.537639\n",
      "Epoch: 13/1000 Train loss: 0.896904 Valid loss: 0.697686 Train acc: 0.514605 Valid acc: 0.545589\n",
      "Epoch: 14/1000 Train loss: 0.883480 Valid loss: 0.696680 Train acc: 0.516264 Valid acc: 0.550836\n",
      "Epoch: 15/1000 Train loss: 0.872143 Valid loss: 0.695785 Train acc: 0.517604 Valid acc: 0.551962\n",
      "Epoch: 16/1000 Train loss: 0.862401 Valid loss: 0.694932 Train acc: 0.518781 Valid acc: 0.557370\n",
      "Epoch: 17/1000 Train loss: 0.853247 Valid loss: 0.694134 Train acc: 0.519795 Valid acc: 0.562890\n",
      "Epoch: 18/1000 Train loss: 0.845117 Valid loss: 0.693402 Train acc: 0.521215 Valid acc: 0.564347\n",
      "Epoch: 19/1000 Train loss: 0.837725 Valid loss: 0.692694 Train acc: 0.522652 Valid acc: 0.567219\n",
      "Epoch: 20/1000 Train loss: 0.830782 Valid loss: 0.692008 Train acc: 0.523930 Valid acc: 0.571462\n",
      "Epoch: 21/1000 Train loss: 0.824370 Valid loss: 0.691352 Train acc: 0.525615 Valid acc: 0.574221\n",
      "Epoch: 22/1000 Train loss: 0.818528 Valid loss: 0.690713 Train acc: 0.527135 Valid acc: 0.576868\n",
      "Epoch: 23/1000 Train loss: 0.813503 Valid loss: 0.690084 Train acc: 0.528080 Valid acc: 0.580220\n",
      "Epoch: 24/1000 Train loss: 0.808393 Valid loss: 0.689471 Train acc: 0.529517 Valid acc: 0.582790\n",
      "Epoch: 25/1000 Train loss: 0.803737 Valid loss: 0.688870 Train acc: 0.530667 Valid acc: 0.584644\n",
      "Epoch: 26/1000 Train loss: 0.799380 Valid loss: 0.688269 Train acc: 0.531941 Valid acc: 0.587312\n",
      "Epoch: 27/1000 Train loss: 0.795185 Valid loss: 0.687670 Train acc: 0.533355 Valid acc: 0.590185\n",
      "Epoch: 28/1000 Train loss: 0.791266 Valid loss: 0.687078 Train acc: 0.534603 Valid acc: 0.592306\n",
      "Epoch: 29/1000 Train loss: 0.787534 Valid loss: 0.686480 Train acc: 0.536348 Valid acc: 0.594752\n",
      "Epoch: 30/1000 Train loss: 0.784064 Valid loss: 0.685873 Train acc: 0.537922 Valid acc: 0.597585\n",
      "Epoch: 31/1000 Train loss: 0.780753 Valid loss: 0.685263 Train acc: 0.539316 Valid acc: 0.599894\n",
      "Epoch: 32/1000 Train loss: 0.777625 Valid loss: 0.684645 Train acc: 0.540942 Valid acc: 0.602125\n",
      "Epoch: 33/1000 Train loss: 0.774525 Valid loss: 0.684010 Train acc: 0.542511 Valid acc: 0.604567\n",
      "Epoch: 34/1000 Train loss: 0.771509 Valid loss: 0.683359 Train acc: 0.544029 Valid acc: 0.606933\n",
      "Epoch: 35/1000 Train loss: 0.768555 Valid loss: 0.682685 Train acc: 0.545745 Valid acc: 0.609296\n",
      "Epoch: 36/1000 Train loss: 0.765733 Valid loss: 0.681986 Train acc: 0.547290 Valid acc: 0.611641\n",
      "Epoch: 37/1000 Train loss: 0.763100 Valid loss: 0.681263 Train acc: 0.548755 Valid acc: 0.613902\n",
      "Epoch: 38/1000 Train loss: 0.760671 Valid loss: 0.680510 Train acc: 0.550322 Valid acc: 0.616222\n",
      "Epoch: 39/1000 Train loss: 0.758213 Valid loss: 0.679731 Train acc: 0.552038 Valid acc: 0.618546\n",
      "Epoch: 40/1000 Train loss: 0.755875 Valid loss: 0.678926 Train acc: 0.553775 Valid acc: 0.620838\n",
      "Epoch: 41/1000 Train loss: 0.753372 Valid loss: 0.678088 Train acc: 0.555578 Valid acc: 0.623201\n",
      "Epoch: 42/1000 Train loss: 0.750978 Valid loss: 0.677224 Train acc: 0.557234 Valid acc: 0.625380\n",
      "Epoch: 43/1000 Train loss: 0.748649 Valid loss: 0.676321 Train acc: 0.558989 Valid acc: 0.627604\n",
      "Epoch: 44/1000 Train loss: 0.746421 Valid loss: 0.675379 Train acc: 0.560814 Valid acc: 0.629874\n",
      "Epoch: 45/1000 Train loss: 0.744157 Valid loss: 0.674407 Train acc: 0.562667 Valid acc: 0.632021\n",
      "Epoch: 46/1000 Train loss: 0.741929 Valid loss: 0.673399 Train acc: 0.564388 Valid acc: 0.634147\n",
      "Epoch: 47/1000 Train loss: 0.739657 Valid loss: 0.672354 Train acc: 0.566309 Valid acc: 0.636310\n",
      "Epoch: 48/1000 Train loss: 0.737483 Valid loss: 0.671281 Train acc: 0.568147 Valid acc: 0.638359\n",
      "Epoch: 49/1000 Train loss: 0.735318 Valid loss: 0.670170 Train acc: 0.569898 Valid acc: 0.640456\n",
      "Epoch: 50/1000 Train loss: 0.733183 Valid loss: 0.669025 Train acc: 0.571708 Valid acc: 0.642541\n",
      "Epoch: 51/1000 Train loss: 0.731048 Valid loss: 0.667857 Train acc: 0.573518 Valid acc: 0.644532\n",
      "Epoch: 52/1000 Train loss: 0.728996 Valid loss: 0.666660 Train acc: 0.575258 Valid acc: 0.646516\n",
      "Epoch: 53/1000 Train loss: 0.726962 Valid loss: 0.665437 Train acc: 0.577084 Valid acc: 0.648468\n",
      "Epoch: 54/1000 Train loss: 0.724990 Valid loss: 0.664197 Train acc: 0.578793 Valid acc: 0.650322\n",
      "Epoch: 55/1000 Train loss: 0.723078 Valid loss: 0.662942 Train acc: 0.580432 Valid acc: 0.652169\n",
      "Epoch: 56/1000 Train loss: 0.721161 Valid loss: 0.661674 Train acc: 0.582115 Valid acc: 0.653998\n",
      "Epoch: 57/1000 Train loss: 0.719277 Valid loss: 0.660403 Train acc: 0.583778 Valid acc: 0.655794\n",
      "Epoch: 58/1000 Train loss: 0.717388 Valid loss: 0.659134 Train acc: 0.585435 Valid acc: 0.657542\n",
      "Epoch: 59/1000 Train loss: 0.715535 Valid loss: 0.657863 Train acc: 0.587139 Valid acc: 0.659273\n",
      "Epoch: 60/1000 Train loss: 0.713725 Valid loss: 0.656596 Train acc: 0.588772 Valid acc: 0.660953\n",
      "Epoch: 61/1000 Train loss: 0.711842 Valid loss: 0.655338 Train acc: 0.590406 Valid acc: 0.662547\n",
      "Epoch: 62/1000 Train loss: 0.710042 Valid loss: 0.654083 Train acc: 0.592025 Valid acc: 0.664184\n",
      "Epoch: 63/1000 Train loss: 0.708288 Valid loss: 0.652837 Train acc: 0.593668 Valid acc: 0.665729\n",
      "Epoch: 64/1000 Train loss: 0.706582 Valid loss: 0.651596 Train acc: 0.595176 Valid acc: 0.667249\n",
      "Epoch: 65/1000 Train loss: 0.704845 Valid loss: 0.650356 Train acc: 0.596736 Valid acc: 0.668772\n",
      "Epoch: 66/1000 Train loss: 0.703076 Valid loss: 0.649123 Train acc: 0.598308 Valid acc: 0.670242\n",
      "Epoch: 67/1000 Train loss: 0.701451 Valid loss: 0.647892 Train acc: 0.599745 Valid acc: 0.671690\n",
      "Epoch: 68/1000 Train loss: 0.699852 Valid loss: 0.646662 Train acc: 0.601185 Valid acc: 0.673117\n",
      "Epoch: 69/1000 Train loss: 0.698207 Valid loss: 0.645443 Train acc: 0.602694 Valid acc: 0.674496\n",
      "Epoch: 70/1000 Train loss: 0.696476 Valid loss: 0.644229 Train acc: 0.604135 Valid acc: 0.675852\n",
      "Epoch: 71/1000 Train loss: 0.694816 Valid loss: 0.643026 Train acc: 0.605603 Valid acc: 0.677180\n",
      "Epoch: 72/1000 Train loss: 0.693251 Valid loss: 0.641831 Train acc: 0.606983 Valid acc: 0.678482\n",
      "Epoch: 73/1000 Train loss: 0.691736 Valid loss: 0.640643 Train acc: 0.608380 Valid acc: 0.679753\n",
      "Epoch: 74/1000 Train loss: 0.690232 Valid loss: 0.639476 Train acc: 0.609728 Valid acc: 0.681003\n",
      "Epoch: 75/1000 Train loss: 0.688682 Valid loss: 0.638323 Train acc: 0.611061 Valid acc: 0.682243\n",
      "Epoch: 76/1000 Train loss: 0.687120 Valid loss: 0.637173 Train acc: 0.612438 Valid acc: 0.683469\n",
      "Epoch: 77/1000 Train loss: 0.685625 Valid loss: 0.636031 Train acc: 0.613722 Valid acc: 0.684659\n",
      "Epoch: 78/1000 Train loss: 0.684183 Valid loss: 0.634895 Train acc: 0.614975 Valid acc: 0.685851\n",
      "Epoch: 79/1000 Train loss: 0.682752 Valid loss: 0.633768 Train acc: 0.616256 Valid acc: 0.687022\n",
      "Epoch: 80/1000 Train loss: 0.681263 Valid loss: 0.632658 Train acc: 0.617494 Valid acc: 0.688169\n",
      "Epoch: 81/1000 Train loss: 0.679795 Valid loss: 0.631556 Train acc: 0.618750 Valid acc: 0.689313\n",
      "Epoch: 82/1000 Train loss: 0.678380 Valid loss: 0.630455 Train acc: 0.619960 Valid acc: 0.690446\n",
      "Epoch: 83/1000 Train loss: 0.677091 Valid loss: 0.629369 Train acc: 0.621152 Valid acc: 0.691540\n",
      "Epoch: 84/1000 Train loss: 0.675698 Valid loss: 0.628293 Train acc: 0.622350 Valid acc: 0.692640\n",
      "Epoch: 85/1000 Train loss: 0.674221 Valid loss: 0.627225 Train acc: 0.623657 Valid acc: 0.693730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86/1000 Train loss: 0.672838 Valid loss: 0.626162 Train acc: 0.624852 Valid acc: 0.694781\n",
      "Epoch: 87/1000 Train loss: 0.671572 Valid loss: 0.625101 Train acc: 0.626014 Valid acc: 0.695877\n",
      "Epoch: 88/1000 Train loss: 0.670233 Valid loss: 0.624053 Train acc: 0.627119 Valid acc: 0.696915\n",
      "Epoch: 89/1000 Train loss: 0.668981 Valid loss: 0.623018 Train acc: 0.628251 Valid acc: 0.697944\n",
      "Epoch: 90/1000 Train loss: 0.667629 Valid loss: 0.621996 Train acc: 0.629419 Valid acc: 0.698968\n",
      "Epoch: 91/1000 Train loss: 0.666286 Valid loss: 0.620989 Train acc: 0.630517 Valid acc: 0.699933\n",
      "Epoch: 92/1000 Train loss: 0.664933 Valid loss: 0.619980 Train acc: 0.631633 Valid acc: 0.700921\n",
      "Epoch: 93/1000 Train loss: 0.663620 Valid loss: 0.618974 Train acc: 0.632752 Valid acc: 0.701885\n",
      "Epoch: 94/1000 Train loss: 0.662363 Valid loss: 0.617974 Train acc: 0.633791 Valid acc: 0.702819\n",
      "Epoch: 95/1000 Train loss: 0.661032 Valid loss: 0.616970 Train acc: 0.634956 Valid acc: 0.703783\n",
      "Epoch: 96/1000 Train loss: 0.659797 Valid loss: 0.615971 Train acc: 0.636020 Valid acc: 0.704721\n",
      "Epoch: 97/1000 Train loss: 0.658533 Valid loss: 0.614978 Train acc: 0.637070 Valid acc: 0.705667\n",
      "Epoch: 98/1000 Train loss: 0.657287 Valid loss: 0.613996 Train acc: 0.638106 Valid acc: 0.706587\n",
      "Epoch: 99/1000 Train loss: 0.656116 Valid loss: 0.613024 Train acc: 0.639113 Valid acc: 0.707476\n",
      "Epoch: 100/1000 Train loss: 0.654915 Valid loss: 0.612060 Train acc: 0.640150 Valid acc: 0.708372\n",
      "Epoch: 101/1000 Train loss: 0.653692 Valid loss: 0.611117 Train acc: 0.641198 Valid acc: 0.709224\n",
      "Epoch: 102/1000 Train loss: 0.652513 Valid loss: 0.610176 Train acc: 0.642167 Valid acc: 0.710075\n",
      "Epoch: 103/1000 Train loss: 0.651326 Valid loss: 0.609236 Train acc: 0.643121 Valid acc: 0.710941\n",
      "Epoch: 104/1000 Train loss: 0.650178 Valid loss: 0.608304 Train acc: 0.644116 Valid acc: 0.711759\n",
      "Epoch: 105/1000 Train loss: 0.649002 Valid loss: 0.607369 Train acc: 0.645057 Valid acc: 0.712588\n",
      "Epoch: 106/1000 Train loss: 0.647801 Valid loss: 0.606435 Train acc: 0.646050 Valid acc: 0.713410\n",
      "Epoch: 107/1000 Train loss: 0.646665 Valid loss: 0.605501 Train acc: 0.646994 Valid acc: 0.714224\n",
      "Epoch: 108/1000 Train loss: 0.645541 Valid loss: 0.604571 Train acc: 0.647899 Valid acc: 0.715038\n",
      "Epoch: 109/1000 Train loss: 0.644418 Valid loss: 0.603651 Train acc: 0.648831 Valid acc: 0.715831\n",
      "Epoch: 110/1000 Train loss: 0.643260 Valid loss: 0.602741 Train acc: 0.649785 Valid acc: 0.716613\n",
      "Epoch: 111/1000 Train loss: 0.642141 Valid loss: 0.601838 Train acc: 0.650708 Valid acc: 0.717373\n",
      "Epoch: 112/1000 Train loss: 0.641054 Valid loss: 0.600937 Train acc: 0.651613 Valid acc: 0.718133\n",
      "Epoch: 113/1000 Train loss: 0.639928 Valid loss: 0.600046 Train acc: 0.652525 Valid acc: 0.718870\n",
      "Epoch: 114/1000 Train loss: 0.638846 Valid loss: 0.599162 Train acc: 0.653423 Valid acc: 0.719616\n",
      "Epoch: 115/1000 Train loss: 0.637729 Valid loss: 0.598288 Train acc: 0.654291 Valid acc: 0.720322\n",
      "Epoch: 116/1000 Train loss: 0.636645 Valid loss: 0.597416 Train acc: 0.655166 Valid acc: 0.721023\n",
      "Epoch: 117/1000 Train loss: 0.635565 Valid loss: 0.596539 Train acc: 0.656038 Valid acc: 0.721759\n",
      "Epoch: 118/1000 Train loss: 0.634469 Valid loss: 0.595668 Train acc: 0.656897 Valid acc: 0.722463\n",
      "Epoch: 119/1000 Train loss: 0.633366 Valid loss: 0.594791 Train acc: 0.657775 Valid acc: 0.723176\n",
      "Epoch: 120/1000 Train loss: 0.632368 Valid loss: 0.593917 Train acc: 0.658606 Valid acc: 0.723885\n",
      "Epoch: 121/1000 Train loss: 0.631270 Valid loss: 0.593053 Train acc: 0.659494 Valid acc: 0.724570\n",
      "Epoch: 122/1000 Train loss: 0.630166 Valid loss: 0.592184 Train acc: 0.660368 Valid acc: 0.725267\n",
      "Epoch: 123/1000 Train loss: 0.629117 Valid loss: 0.591312 Train acc: 0.661215 Valid acc: 0.725951\n",
      "Epoch: 124/1000 Train loss: 0.628080 Valid loss: 0.590441 Train acc: 0.662049 Valid acc: 0.726630\n",
      "Epoch: 125/1000 Train loss: 0.626991 Valid loss: 0.589570 Train acc: 0.662897 Valid acc: 0.727314\n",
      "Epoch: 126/1000 Train loss: 0.625961 Valid loss: 0.588705 Train acc: 0.663717 Valid acc: 0.727987\n",
      "Epoch: 127/1000 Train loss: 0.624935 Valid loss: 0.587848 Train acc: 0.664493 Valid acc: 0.728650\n",
      "Epoch: 128/1000 Train loss: 0.623938 Valid loss: 0.586998 Train acc: 0.665283 Valid acc: 0.729311\n",
      "Epoch: 129/1000 Train loss: 0.622901 Valid loss: 0.586161 Train acc: 0.666110 Valid acc: 0.729961\n",
      "Epoch: 130/1000 Train loss: 0.621855 Valid loss: 0.585332 Train acc: 0.666907 Valid acc: 0.730590\n",
      "Epoch: 131/1000 Train loss: 0.620795 Valid loss: 0.584502 Train acc: 0.667685 Valid acc: 0.731192\n",
      "Epoch: 132/1000 Train loss: 0.619767 Valid loss: 0.583680 Train acc: 0.668472 Valid acc: 0.731783\n",
      "Epoch: 133/1000 Train loss: 0.618813 Valid loss: 0.582859 Train acc: 0.669261 Valid acc: 0.732358\n",
      "Epoch: 134/1000 Train loss: 0.617762 Valid loss: 0.582041 Train acc: 0.670082 Valid acc: 0.732930\n",
      "Epoch: 135/1000 Train loss: 0.616754 Valid loss: 0.581231 Train acc: 0.670848 Valid acc: 0.733501\n",
      "Epoch: 136/1000 Train loss: 0.615710 Valid loss: 0.580417 Train acc: 0.671623 Valid acc: 0.734068\n",
      "Epoch: 137/1000 Train loss: 0.614717 Valid loss: 0.579599 Train acc: 0.672361 Valid acc: 0.734640\n",
      "Epoch: 138/1000 Train loss: 0.613724 Valid loss: 0.578794 Train acc: 0.673122 Valid acc: 0.735200\n",
      "Epoch: 139/1000 Train loss: 0.612759 Valid loss: 0.577997 Train acc: 0.673841 Valid acc: 0.735754\n",
      "Epoch: 140/1000 Train loss: 0.611796 Valid loss: 0.577207 Train acc: 0.674579 Valid acc: 0.736295\n",
      "Epoch: 141/1000 Train loss: 0.610809 Valid loss: 0.576426 Train acc: 0.675322 Valid acc: 0.736831\n",
      "Epoch: 142/1000 Train loss: 0.609815 Valid loss: 0.575639 Train acc: 0.676059 Valid acc: 0.737373\n",
      "Epoch: 143/1000 Train loss: 0.608827 Valid loss: 0.574847 Train acc: 0.676806 Valid acc: 0.737893\n",
      "Epoch: 144/1000 Train loss: 0.607889 Valid loss: 0.574062 Train acc: 0.677518 Valid acc: 0.738421\n",
      "Epoch: 145/1000 Train loss: 0.606926 Valid loss: 0.573284 Train acc: 0.678244 Valid acc: 0.738934\n",
      "Epoch: 146/1000 Train loss: 0.605974 Valid loss: 0.572516 Train acc: 0.678962 Valid acc: 0.739438\n",
      "Epoch: 147/1000 Train loss: 0.605034 Valid loss: 0.571757 Train acc: 0.679675 Valid acc: 0.739935\n",
      "Epoch: 148/1000 Train loss: 0.604072 Valid loss: 0.571006 Train acc: 0.680367 Valid acc: 0.740431\n",
      "Epoch: 149/1000 Train loss: 0.603095 Valid loss: 0.570263 Train acc: 0.681108 Valid acc: 0.740927\n",
      "Epoch: 150/1000 Train loss: 0.602164 Valid loss: 0.569521 Train acc: 0.681777 Valid acc: 0.741412\n",
      "Epoch: 151/1000 Train loss: 0.601242 Valid loss: 0.568778 Train acc: 0.682453 Valid acc: 0.741895\n",
      "Epoch: 152/1000 Train loss: 0.600312 Valid loss: 0.568039 Train acc: 0.683108 Valid acc: 0.742371\n",
      "Epoch: 153/1000 Train loss: 0.599437 Valid loss: 0.567305 Train acc: 0.683783 Valid acc: 0.742834\n",
      "Epoch: 154/1000 Train loss: 0.598565 Valid loss: 0.566584 Train acc: 0.684450 Valid acc: 0.743287\n",
      "Epoch: 155/1000 Train loss: 0.597662 Valid loss: 0.565872 Train acc: 0.685116 Valid acc: 0.743741\n",
      "Epoch: 156/1000 Train loss: 0.596790 Valid loss: 0.565162 Train acc: 0.685785 Valid acc: 0.744182\n",
      "Epoch: 157/1000 Train loss: 0.595881 Valid loss: 0.564455 Train acc: 0.686457 Valid acc: 0.744612\n",
      "Epoch: 158/1000 Train loss: 0.594992 Valid loss: 0.563754 Train acc: 0.687114 Valid acc: 0.745037\n",
      "Epoch: 159/1000 Train loss: 0.594141 Valid loss: 0.563061 Train acc: 0.687751 Valid acc: 0.745456\n",
      "Epoch: 160/1000 Train loss: 0.593279 Valid loss: 0.562382 Train acc: 0.688408 Valid acc: 0.745858\n",
      "Epoch: 161/1000 Train loss: 0.592441 Valid loss: 0.561708 Train acc: 0.689041 Valid acc: 0.746263\n",
      "Epoch: 162/1000 Train loss: 0.591600 Valid loss: 0.561045 Train acc: 0.689659 Valid acc: 0.746661\n",
      "Epoch: 163/1000 Train loss: 0.590756 Valid loss: 0.560383 Train acc: 0.690289 Valid acc: 0.747064\n",
      "Epoch: 164/1000 Train loss: 0.589882 Valid loss: 0.559723 Train acc: 0.690912 Valid acc: 0.747467\n",
      "Epoch: 165/1000 Train loss: 0.589068 Valid loss: 0.559069 Train acc: 0.691513 Valid acc: 0.747854\n",
      "Epoch: 166/1000 Train loss: 0.588234 Valid loss: 0.558409 Train acc: 0.692123 Valid acc: 0.748244\n",
      "Epoch: 167/1000 Train loss: 0.587411 Valid loss: 0.557761 Train acc: 0.692725 Valid acc: 0.748617\n",
      "Epoch: 168/1000 Train loss: 0.586563 Valid loss: 0.557116 Train acc: 0.693327 Valid acc: 0.748996\n",
      "Epoch: 169/1000 Train loss: 0.585763 Valid loss: 0.556480 Train acc: 0.693900 Valid acc: 0.749373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 170/1000 Train loss: 0.584973 Valid loss: 0.555852 Train acc: 0.694499 Valid acc: 0.749745\n",
      "Epoch: 171/1000 Train loss: 0.584150 Valid loss: 0.555232 Train acc: 0.695099 Valid acc: 0.750114\n",
      "Epoch: 172/1000 Train loss: 0.583381 Valid loss: 0.554618 Train acc: 0.695648 Valid acc: 0.750471\n",
      "Epoch: 173/1000 Train loss: 0.582564 Valid loss: 0.554008 Train acc: 0.696250 Valid acc: 0.750818\n",
      "Epoch: 174/1000 Train loss: 0.581737 Valid loss: 0.553397 Train acc: 0.696849 Valid acc: 0.751170\n",
      "Epoch: 175/1000 Train loss: 0.580936 Valid loss: 0.552783 Train acc: 0.697417 Valid acc: 0.751508\n",
      "Epoch: 176/1000 Train loss: 0.580168 Valid loss: 0.552180 Train acc: 0.697975 Valid acc: 0.751846\n",
      "Epoch: 177/1000 Train loss: 0.579374 Valid loss: 0.551584 Train acc: 0.698538 Valid acc: 0.752177\n",
      "Epoch: 178/1000 Train loss: 0.578616 Valid loss: 0.550998 Train acc: 0.699076 Valid acc: 0.752499\n",
      "Epoch: 179/1000 Train loss: 0.577848 Valid loss: 0.550427 Train acc: 0.699632 Valid acc: 0.752801\n",
      "Epoch: 180/1000 Train loss: 0.577078 Valid loss: 0.549857 Train acc: 0.700194 Valid acc: 0.753111\n",
      "Epoch: 181/1000 Train loss: 0.576344 Valid loss: 0.549298 Train acc: 0.700719 Valid acc: 0.753416\n",
      "Epoch: 182/1000 Train loss: 0.575605 Valid loss: 0.548739 Train acc: 0.701246 Valid acc: 0.753718\n",
      "Epoch: 183/1000 Train loss: 0.574860 Valid loss: 0.548185 Train acc: 0.701783 Valid acc: 0.754013\n",
      "Epoch: 184/1000 Train loss: 0.574119 Valid loss: 0.547645 Train acc: 0.702312 Valid acc: 0.754289\n",
      "Epoch: 185/1000 Train loss: 0.573390 Valid loss: 0.547098 Train acc: 0.702836 Valid acc: 0.754587\n",
      "Epoch: 186/1000 Train loss: 0.572626 Valid loss: 0.546560 Train acc: 0.703351 Valid acc: 0.754858\n",
      "Epoch: 187/1000 Train loss: 0.571870 Valid loss: 0.546013 Train acc: 0.703879 Valid acc: 0.755140\n",
      "Epoch: 188/1000 Train loss: 0.571134 Valid loss: 0.545460 Train acc: 0.704388 Valid acc: 0.755427\n",
      "Epoch: 189/1000 Train loss: 0.570412 Valid loss: 0.544915 Train acc: 0.704900 Valid acc: 0.755705\n",
      "Epoch: 190/1000 Train loss: 0.569685 Valid loss: 0.544370 Train acc: 0.705412 Valid acc: 0.755989\n",
      "Epoch: 191/1000 Train loss: 0.568982 Valid loss: 0.543847 Train acc: 0.705912 Valid acc: 0.756261\n",
      "Epoch: 192/1000 Train loss: 0.568281 Valid loss: 0.543327 Train acc: 0.706410 Valid acc: 0.756543\n",
      "Epoch: 193/1000 Train loss: 0.567573 Valid loss: 0.542812 Train acc: 0.706899 Valid acc: 0.756823\n",
      "Epoch: 194/1000 Train loss: 0.566861 Valid loss: 0.542306 Train acc: 0.707406 Valid acc: 0.757093\n",
      "Epoch: 195/1000 Train loss: 0.566169 Valid loss: 0.541795 Train acc: 0.707885 Valid acc: 0.757365\n",
      "Epoch: 196/1000 Train loss: 0.565473 Valid loss: 0.541293 Train acc: 0.708360 Valid acc: 0.757624\n",
      "Epoch: 197/1000 Train loss: 0.564790 Valid loss: 0.540791 Train acc: 0.708838 Valid acc: 0.757886\n",
      "Epoch: 198/1000 Train loss: 0.564106 Valid loss: 0.540299 Train acc: 0.709316 Valid acc: 0.758134\n",
      "Epoch: 199/1000 Train loss: 0.563431 Valid loss: 0.539809 Train acc: 0.709783 Valid acc: 0.758389\n",
      "Epoch: 200/1000 Train loss: 0.562774 Valid loss: 0.539328 Train acc: 0.710249 Valid acc: 0.758636\n",
      "Epoch: 201/1000 Train loss: 0.562145 Valid loss: 0.538854 Train acc: 0.710683 Valid acc: 0.758880\n",
      "Epoch: 202/1000 Train loss: 0.561483 Valid loss: 0.538376 Train acc: 0.711146 Valid acc: 0.759136\n",
      "Epoch: 203/1000 Train loss: 0.560818 Valid loss: 0.537908 Train acc: 0.711614 Valid acc: 0.759384\n",
      "Epoch: 204/1000 Train loss: 0.560180 Valid loss: 0.537434 Train acc: 0.712045 Valid acc: 0.759640\n",
      "Epoch: 205/1000 Train loss: 0.559517 Valid loss: 0.536961 Train acc: 0.712516 Valid acc: 0.759900\n",
      "Epoch: 206/1000 Train loss: 0.558881 Valid loss: 0.536495 Train acc: 0.712990 Valid acc: 0.760142\n",
      "Epoch: 207/1000 Train loss: 0.558226 Valid loss: 0.536024 Train acc: 0.713428 Valid acc: 0.760402\n",
      "Epoch: 208/1000 Train loss: 0.557571 Valid loss: 0.535556 Train acc: 0.713894 Valid acc: 0.760660\n",
      "Epoch: 209/1000 Train loss: 0.556939 Valid loss: 0.535090 Train acc: 0.714334 Valid acc: 0.760920\n",
      "Epoch: 210/1000 Train loss: 0.556324 Valid loss: 0.534630 Train acc: 0.714767 Valid acc: 0.761174\n",
      "Epoch: 211/1000 Train loss: 0.555688 Valid loss: 0.534175 Train acc: 0.715189 Valid acc: 0.761422\n",
      "Epoch: 212/1000 Train loss: 0.555058 Valid loss: 0.533726 Train acc: 0.715633 Valid acc: 0.761670\n",
      "Epoch: 213/1000 Train loss: 0.554438 Valid loss: 0.533280 Train acc: 0.716068 Valid acc: 0.761917\n",
      "Epoch: 214/1000 Train loss: 0.553820 Valid loss: 0.532839 Train acc: 0.716478 Valid acc: 0.762164\n",
      "Epoch: 215/1000 Train loss: 0.553192 Valid loss: 0.532400 Train acc: 0.716907 Valid acc: 0.762408\n",
      "Epoch: 216/1000 Train loss: 0.552570 Valid loss: 0.531961 Train acc: 0.717337 Valid acc: 0.762645\n",
      "Epoch: 217/1000 Train loss: 0.551925 Valid loss: 0.531525 Train acc: 0.717781 Valid acc: 0.762874\n",
      "Epoch: 218/1000 Train loss: 0.551288 Valid loss: 0.531084 Train acc: 0.718206 Valid acc: 0.763109\n",
      "Epoch: 219/1000 Train loss: 0.550679 Valid loss: 0.530650 Train acc: 0.718637 Valid acc: 0.763334\n",
      "Epoch: 220/1000 Train loss: 0.550089 Valid loss: 0.530223 Train acc: 0.719042 Valid acc: 0.763560\n",
      "Epoch: 221/1000 Train loss: 0.549507 Valid loss: 0.529800 Train acc: 0.719443 Valid acc: 0.763800\n",
      "Epoch: 222/1000 Train loss: 0.548914 Valid loss: 0.529390 Train acc: 0.719840 Valid acc: 0.764024\n",
      "Epoch: 223/1000 Train loss: 0.548293 Valid loss: 0.528978 Train acc: 0.720275 Valid acc: 0.764261\n",
      "Epoch: 224/1000 Train loss: 0.547728 Valid loss: 0.528566 Train acc: 0.720657 Valid acc: 0.764483\n",
      "Epoch: 225/1000 Train loss: 0.547134 Valid loss: 0.528159 Train acc: 0.721076 Valid acc: 0.764690\n",
      "Epoch: 226/1000 Train loss: 0.546543 Valid loss: 0.527747 Train acc: 0.721487 Valid acc: 0.764909\n",
      "Epoch: 227/1000 Train loss: 0.545966 Valid loss: 0.527338 Train acc: 0.721880 Valid acc: 0.765127\n",
      "Epoch: 228/1000 Train loss: 0.545391 Valid loss: 0.526933 Train acc: 0.722268 Valid acc: 0.765345\n",
      "Epoch: 229/1000 Train loss: 0.544839 Valid loss: 0.526528 Train acc: 0.722641 Valid acc: 0.765556\n",
      "Epoch: 230/1000 Train loss: 0.544241 Valid loss: 0.526140 Train acc: 0.723047 Valid acc: 0.765758\n",
      "Epoch: 231/1000 Train loss: 0.543681 Valid loss: 0.525746 Train acc: 0.723431 Valid acc: 0.765965\n",
      "Epoch: 232/1000 Train loss: 0.543097 Valid loss: 0.525359 Train acc: 0.723825 Valid acc: 0.766161\n",
      "Epoch: 233/1000 Train loss: 0.542538 Valid loss: 0.524970 Train acc: 0.724212 Valid acc: 0.766369\n",
      "Epoch: 234/1000 Train loss: 0.541993 Valid loss: 0.524580 Train acc: 0.724580 Valid acc: 0.766574\n",
      "Epoch: 235/1000 Train loss: 0.541436 Valid loss: 0.524193 Train acc: 0.724951 Valid acc: 0.766772\n",
      "Epoch: 236/1000 Train loss: 0.540918 Valid loss: 0.523806 Train acc: 0.725315 Valid acc: 0.766980\n",
      "Epoch: 237/1000 Train loss: 0.540380 Valid loss: 0.523430 Train acc: 0.725683 Valid acc: 0.767188\n",
      "Epoch: 238/1000 Train loss: 0.539834 Valid loss: 0.523054 Train acc: 0.726066 Valid acc: 0.767392\n",
      "Epoch: 239/1000 Train loss: 0.539286 Valid loss: 0.522689 Train acc: 0.726448 Valid acc: 0.767599\n",
      "Epoch: 240/1000 Train loss: 0.538767 Valid loss: 0.522326 Train acc: 0.726805 Valid acc: 0.767803\n",
      "Epoch: 241/1000 Train loss: 0.538227 Valid loss: 0.521961 Train acc: 0.727157 Valid acc: 0.768003\n",
      "Epoch: 242/1000 Train loss: 0.537702 Valid loss: 0.521603 Train acc: 0.727526 Valid acc: 0.768192\n",
      "Epoch: 243/1000 Train loss: 0.537178 Valid loss: 0.521244 Train acc: 0.727887 Valid acc: 0.768380\n",
      "Epoch: 244/1000 Train loss: 0.536644 Valid loss: 0.520890 Train acc: 0.728240 Valid acc: 0.768564\n",
      "Epoch: 245/1000 Train loss: 0.536091 Valid loss: 0.520533 Train acc: 0.728620 Valid acc: 0.768756\n",
      "Epoch: 246/1000 Train loss: 0.535561 Valid loss: 0.520172 Train acc: 0.728982 Valid acc: 0.768947\n",
      "Epoch: 247/1000 Train loss: 0.535036 Valid loss: 0.519816 Train acc: 0.729346 Valid acc: 0.769129\n",
      "Epoch: 248/1000 Train loss: 0.534509 Valid loss: 0.519460 Train acc: 0.729697 Valid acc: 0.769317\n",
      "Epoch: 249/1000 Train loss: 0.533984 Valid loss: 0.519119 Train acc: 0.730055 Valid acc: 0.769490\n",
      "Epoch: 250/1000 Train loss: 0.533465 Valid loss: 0.518771 Train acc: 0.730407 Valid acc: 0.769669\n",
      "Epoch: 251/1000 Train loss: 0.532938 Valid loss: 0.518416 Train acc: 0.730759 Valid acc: 0.769853\n",
      "Epoch: 252/1000 Train loss: 0.532434 Valid loss: 0.518058 Train acc: 0.731110 Valid acc: 0.770043\n",
      "Epoch: 253/1000 Train loss: 0.531922 Valid loss: 0.517703 Train acc: 0.731457 Valid acc: 0.770234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 254/1000 Train loss: 0.531417 Valid loss: 0.517352 Train acc: 0.731793 Valid acc: 0.770422\n",
      "Epoch: 255/1000 Train loss: 0.530902 Valid loss: 0.517006 Train acc: 0.732143 Valid acc: 0.770611\n",
      "Epoch: 256/1000 Train loss: 0.530395 Valid loss: 0.516664 Train acc: 0.732488 Valid acc: 0.770796\n",
      "Epoch: 257/1000 Train loss: 0.529898 Valid loss: 0.516326 Train acc: 0.732826 Valid acc: 0.770974\n",
      "Epoch: 258/1000 Train loss: 0.529393 Valid loss: 0.515992 Train acc: 0.733169 Valid acc: 0.771150\n",
      "Epoch: 259/1000 Train loss: 0.528889 Valid loss: 0.515667 Train acc: 0.733504 Valid acc: 0.771315\n",
      "Epoch: 260/1000 Train loss: 0.528408 Valid loss: 0.515346 Train acc: 0.733837 Valid acc: 0.771478\n",
      "Epoch: 261/1000 Train loss: 0.527918 Valid loss: 0.515023 Train acc: 0.734160 Valid acc: 0.771647\n",
      "Epoch: 262/1000 Train loss: 0.527452 Valid loss: 0.514701 Train acc: 0.734461 Valid acc: 0.771805\n",
      "Epoch: 263/1000 Train loss: 0.526977 Valid loss: 0.514373 Train acc: 0.734795 Valid acc: 0.771981\n",
      "Epoch: 264/1000 Train loss: 0.526478 Valid loss: 0.514052 Train acc: 0.735122 Valid acc: 0.772151\n",
      "Epoch: 265/1000 Train loss: 0.526001 Valid loss: 0.513733 Train acc: 0.735434 Valid acc: 0.772318\n",
      "Epoch: 266/1000 Train loss: 0.525508 Valid loss: 0.513415 Train acc: 0.735748 Valid acc: 0.772488\n",
      "Epoch: 267/1000 Train loss: 0.525035 Valid loss: 0.513098 Train acc: 0.736055 Valid acc: 0.772660\n",
      "Epoch: 268/1000 Train loss: 0.524563 Valid loss: 0.512783 Train acc: 0.736358 Valid acc: 0.772824\n",
      "Epoch: 269/1000 Train loss: 0.524066 Valid loss: 0.512473 Train acc: 0.736681 Valid acc: 0.772984\n",
      "Epoch: 270/1000 Train loss: 0.523592 Valid loss: 0.512167 Train acc: 0.736990 Valid acc: 0.773136\n",
      "Epoch: 271/1000 Train loss: 0.523120 Valid loss: 0.511864 Train acc: 0.737296 Valid acc: 0.773286\n",
      "Epoch: 272/1000 Train loss: 0.522631 Valid loss: 0.511555 Train acc: 0.737620 Valid acc: 0.773450\n",
      "Epoch: 273/1000 Train loss: 0.522148 Valid loss: 0.511244 Train acc: 0.737926 Valid acc: 0.773616\n",
      "Epoch: 274/1000 Train loss: 0.521691 Valid loss: 0.510936 Train acc: 0.738229 Valid acc: 0.773780\n",
      "Epoch: 275/1000 Train loss: 0.521200 Valid loss: 0.510633 Train acc: 0.738550 Valid acc: 0.773938\n",
      "Epoch: 276/1000 Train loss: 0.520748 Valid loss: 0.510328 Train acc: 0.738845 Valid acc: 0.774102\n",
      "Epoch: 277/1000 Train loss: 0.520286 Valid loss: 0.510032 Train acc: 0.739142 Valid acc: 0.774256\n",
      "Epoch: 278/1000 Train loss: 0.519837 Valid loss: 0.509744 Train acc: 0.739441 Valid acc: 0.774406\n",
      "Epoch: 279/1000 Train loss: 0.519370 Valid loss: 0.509453 Train acc: 0.739741 Valid acc: 0.774561\n",
      "Epoch: 280/1000 Train loss: 0.518908 Valid loss: 0.509165 Train acc: 0.740043 Valid acc: 0.774706\n",
      "Epoch: 281/1000 Train loss: 0.518482 Valid loss: 0.508873 Train acc: 0.740326 Valid acc: 0.774851\n",
      "Epoch: 282/1000 Train loss: 0.518028 Valid loss: 0.508584 Train acc: 0.740638 Valid acc: 0.774999\n",
      "Epoch: 283/1000 Train loss: 0.517572 Valid loss: 0.508304 Train acc: 0.740947 Valid acc: 0.775138\n",
      "Epoch: 284/1000 Train loss: 0.517133 Valid loss: 0.508023 Train acc: 0.741241 Valid acc: 0.775280\n",
      "Epoch: 285/1000 Train loss: 0.516697 Valid loss: 0.507753 Train acc: 0.741533 Valid acc: 0.775417\n",
      "Epoch: 286/1000 Train loss: 0.516238 Valid loss: 0.507484 Train acc: 0.741834 Valid acc: 0.775554\n",
      "Epoch: 287/1000 Train loss: 0.515797 Valid loss: 0.507212 Train acc: 0.742117 Valid acc: 0.775687\n",
      "Epoch: 288/1000 Train loss: 0.515359 Valid loss: 0.506934 Train acc: 0.742403 Valid acc: 0.775826\n",
      "Epoch: 289/1000 Train loss: 0.514932 Valid loss: 0.506650 Train acc: 0.742691 Valid acc: 0.775966\n",
      "Epoch: 290/1000 Train loss: 0.514488 Valid loss: 0.506376 Train acc: 0.742986 Valid acc: 0.776100\n",
      "Epoch: 291/1000 Train loss: 0.514045 Valid loss: 0.506104 Train acc: 0.743277 Valid acc: 0.776230\n",
      "Epoch: 292/1000 Train loss: 0.513612 Valid loss: 0.505833 Train acc: 0.743551 Valid acc: 0.776358\n",
      "Epoch: 293/1000 Train loss: 0.513179 Valid loss: 0.505561 Train acc: 0.743839 Valid acc: 0.776489\n",
      "Epoch: 294/1000 Train loss: 0.512760 Valid loss: 0.505290 Train acc: 0.744111 Valid acc: 0.776622\n",
      "Epoch: 295/1000 Train loss: 0.512325 Valid loss: 0.505025 Train acc: 0.744387 Valid acc: 0.776747\n",
      "Epoch: 296/1000 Train loss: 0.511896 Valid loss: 0.504758 Train acc: 0.744665 Valid acc: 0.776883\n",
      "Epoch: 297/1000 Train loss: 0.511464 Valid loss: 0.504496 Train acc: 0.744936 Valid acc: 0.777010\n",
      "Epoch: 298/1000 Train loss: 0.511048 Valid loss: 0.504236 Train acc: 0.745217 Valid acc: 0.777138\n",
      "Epoch: 299/1000 Train loss: 0.510639 Valid loss: 0.503976 Train acc: 0.745485 Valid acc: 0.777268\n",
      "Epoch: 300/1000 Train loss: 0.510208 Valid loss: 0.503722 Train acc: 0.745755 Valid acc: 0.777386\n",
      "Epoch: 301/1000 Train loss: 0.509806 Valid loss: 0.503466 Train acc: 0.746029 Valid acc: 0.777511\n",
      "Epoch: 302/1000 Train loss: 0.509384 Valid loss: 0.503219 Train acc: 0.746303 Valid acc: 0.777630\n",
      "Epoch: 303/1000 Train loss: 0.508964 Valid loss: 0.502970 Train acc: 0.746566 Valid acc: 0.777757\n",
      "Epoch: 304/1000 Train loss: 0.508542 Valid loss: 0.502723 Train acc: 0.746835 Valid acc: 0.777877\n",
      "Epoch: 305/1000 Train loss: 0.508133 Valid loss: 0.502469 Train acc: 0.747108 Valid acc: 0.778008\n",
      "Epoch: 306/1000 Train loss: 0.507714 Valid loss: 0.502220 Train acc: 0.747380 Valid acc: 0.778130\n",
      "Epoch: 307/1000 Train loss: 0.507319 Valid loss: 0.501969 Train acc: 0.747628 Valid acc: 0.778256\n",
      "Epoch: 308/1000 Train loss: 0.506916 Valid loss: 0.501721 Train acc: 0.747888 Valid acc: 0.778376\n",
      "Epoch: 309/1000 Train loss: 0.506513 Valid loss: 0.501476 Train acc: 0.748144 Valid acc: 0.778493\n",
      "Epoch: 310/1000 Train loss: 0.506104 Valid loss: 0.501236 Train acc: 0.748415 Valid acc: 0.778605\n",
      "Epoch: 311/1000 Train loss: 0.505705 Valid loss: 0.500997 Train acc: 0.748668 Valid acc: 0.778720\n",
      "Epoch: 312/1000 Train loss: 0.505305 Valid loss: 0.500757 Train acc: 0.748928 Valid acc: 0.778842\n",
      "Epoch: 313/1000 Train loss: 0.504908 Valid loss: 0.500524 Train acc: 0.749185 Valid acc: 0.778951\n",
      "Epoch: 314/1000 Train loss: 0.504519 Valid loss: 0.500287 Train acc: 0.749427 Valid acc: 0.779068\n",
      "Epoch: 315/1000 Train loss: 0.504130 Valid loss: 0.500056 Train acc: 0.749689 Valid acc: 0.779179\n",
      "Epoch: 316/1000 Train loss: 0.503729 Valid loss: 0.499823 Train acc: 0.749947 Valid acc: 0.779295\n",
      "Epoch: 317/1000 Train loss: 0.503323 Valid loss: 0.499592 Train acc: 0.750194 Valid acc: 0.779410\n",
      "Epoch: 318/1000 Train loss: 0.502925 Valid loss: 0.499363 Train acc: 0.750448 Valid acc: 0.779524\n",
      "Epoch: 319/1000 Train loss: 0.502544 Valid loss: 0.499128 Train acc: 0.750702 Valid acc: 0.779647\n",
      "Epoch: 320/1000 Train loss: 0.502154 Valid loss: 0.498904 Train acc: 0.750955 Valid acc: 0.779757\n",
      "Epoch: 321/1000 Train loss: 0.501771 Valid loss: 0.498680 Train acc: 0.751201 Valid acc: 0.779868\n",
      "Epoch: 322/1000 Train loss: 0.501399 Valid loss: 0.498460 Train acc: 0.751443 Valid acc: 0.779976\n",
      "Epoch: 323/1000 Train loss: 0.501018 Valid loss: 0.498243 Train acc: 0.751695 Valid acc: 0.780090\n",
      "Epoch: 324/1000 Train loss: 0.500630 Valid loss: 0.498020 Train acc: 0.751943 Valid acc: 0.780211\n",
      "Epoch: 325/1000 Train loss: 0.500229 Valid loss: 0.497796 Train acc: 0.752198 Valid acc: 0.780328\n",
      "Epoch: 326/1000 Train loss: 0.499846 Valid loss: 0.497573 Train acc: 0.752439 Valid acc: 0.780442\n",
      "Epoch: 327/1000 Train loss: 0.499452 Valid loss: 0.497351 Train acc: 0.752686 Valid acc: 0.780550\n",
      "Epoch: 328/1000 Train loss: 0.499064 Valid loss: 0.497132 Train acc: 0.752929 Valid acc: 0.780653\n",
      "Epoch: 329/1000 Train loss: 0.498665 Valid loss: 0.496908 Train acc: 0.753186 Valid acc: 0.780759\n",
      "Epoch: 330/1000 Train loss: 0.498274 Valid loss: 0.496685 Train acc: 0.753436 Valid acc: 0.780863\n",
      "Epoch: 331/1000 Train loss: 0.497899 Valid loss: 0.496462 Train acc: 0.753670 Valid acc: 0.780970\n",
      "Epoch: 332/1000 Train loss: 0.497519 Valid loss: 0.496243 Train acc: 0.753907 Valid acc: 0.781077\n",
      "Epoch: 333/1000 Train loss: 0.497135 Valid loss: 0.496023 Train acc: 0.754154 Valid acc: 0.781181\n",
      "Epoch: 334/1000 Train loss: 0.496756 Valid loss: 0.495808 Train acc: 0.754399 Valid acc: 0.781283\n",
      "Epoch: 335/1000 Train loss: 0.496385 Valid loss: 0.495593 Train acc: 0.754636 Valid acc: 0.781379\n",
      "Epoch: 336/1000 Train loss: 0.496002 Valid loss: 0.495385 Train acc: 0.754873 Valid acc: 0.781474\n",
      "Epoch: 337/1000 Train loss: 0.495641 Valid loss: 0.495175 Train acc: 0.755094 Valid acc: 0.781567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 338/1000 Train loss: 0.495272 Valid loss: 0.494969 Train acc: 0.755328 Valid acc: 0.781670\n",
      "Epoch: 339/1000 Train loss: 0.494895 Valid loss: 0.494765 Train acc: 0.755577 Valid acc: 0.781772\n",
      "Epoch: 340/1000 Train loss: 0.494513 Valid loss: 0.494559 Train acc: 0.755814 Valid acc: 0.781865\n",
      "Epoch: 341/1000 Train loss: 0.494151 Valid loss: 0.494355 Train acc: 0.756036 Valid acc: 0.781956\n",
      "Epoch: 342/1000 Train loss: 0.493782 Valid loss: 0.494153 Train acc: 0.756262 Valid acc: 0.782050\n",
      "Epoch: 343/1000 Train loss: 0.493421 Valid loss: 0.493950 Train acc: 0.756488 Valid acc: 0.782153\n",
      "Epoch: 344/1000 Train loss: 0.493048 Valid loss: 0.493753 Train acc: 0.756717 Valid acc: 0.782248\n",
      "Epoch: 345/1000 Train loss: 0.492675 Valid loss: 0.493560 Train acc: 0.756952 Valid acc: 0.782344\n",
      "Epoch: 346/1000 Train loss: 0.492306 Valid loss: 0.493369 Train acc: 0.757182 Valid acc: 0.782429\n",
      "Epoch: 347/1000 Train loss: 0.491941 Valid loss: 0.493172 Train acc: 0.757412 Valid acc: 0.782517\n",
      "Epoch: 348/1000 Train loss: 0.491564 Valid loss: 0.492972 Train acc: 0.757642 Valid acc: 0.782607\n",
      "Epoch: 349/1000 Train loss: 0.491192 Valid loss: 0.492777 Train acc: 0.757869 Valid acc: 0.782695\n",
      "Epoch: 350/1000 Train loss: 0.490830 Valid loss: 0.492580 Train acc: 0.758093 Valid acc: 0.782778\n",
      "Epoch: 351/1000 Train loss: 0.490481 Valid loss: 0.492393 Train acc: 0.758308 Valid acc: 0.782864\n",
      "Epoch: 352/1000 Train loss: 0.490106 Valid loss: 0.492202 Train acc: 0.758544 Valid acc: 0.782954\n",
      "Epoch: 353/1000 Train loss: 0.489750 Valid loss: 0.492014 Train acc: 0.758759 Valid acc: 0.783042\n",
      "Epoch: 354/1000 Train loss: 0.489386 Valid loss: 0.491821 Train acc: 0.758987 Valid acc: 0.783130\n",
      "Epoch: 355/1000 Train loss: 0.489008 Valid loss: 0.491628 Train acc: 0.759207 Valid acc: 0.783222\n",
      "Epoch: 356/1000 Train loss: 0.488651 Valid loss: 0.491438 Train acc: 0.759425 Valid acc: 0.783307\n",
      "Epoch: 357/1000 Train loss: 0.488297 Valid loss: 0.491254 Train acc: 0.759639 Valid acc: 0.783388\n",
      "Epoch: 358/1000 Train loss: 0.487941 Valid loss: 0.491068 Train acc: 0.759871 Valid acc: 0.783475\n",
      "Epoch: 359/1000 Train loss: 0.487581 Valid loss: 0.490876 Train acc: 0.760084 Valid acc: 0.783561\n",
      "Epoch: 360/1000 Train loss: 0.487230 Valid loss: 0.490688 Train acc: 0.760294 Valid acc: 0.783647\n",
      "Epoch: 361/1000 Train loss: 0.486879 Valid loss: 0.490497 Train acc: 0.760507 Valid acc: 0.783730\n",
      "Epoch: 362/1000 Train loss: 0.486526 Valid loss: 0.490315 Train acc: 0.760722 Valid acc: 0.783814\n",
      "Epoch: 363/1000 Train loss: 0.486181 Valid loss: 0.490136 Train acc: 0.760946 Valid acc: 0.783897\n",
      "Epoch: 364/1000 Train loss: 0.485826 Valid loss: 0.489956 Train acc: 0.761168 Valid acc: 0.783981\n",
      "Epoch: 365/1000 Train loss: 0.485476 Valid loss: 0.489768 Train acc: 0.761380 Valid acc: 0.784064\n",
      "Epoch: 366/1000 Train loss: 0.485121 Valid loss: 0.489587 Train acc: 0.761607 Valid acc: 0.784139\n",
      "Epoch: 367/1000 Train loss: 0.484771 Valid loss: 0.489406 Train acc: 0.761817 Valid acc: 0.784222\n",
      "Epoch: 368/1000 Train loss: 0.484424 Valid loss: 0.489230 Train acc: 0.762033 Valid acc: 0.784306\n",
      "Epoch: 369/1000 Train loss: 0.484086 Valid loss: 0.489054 Train acc: 0.762241 Valid acc: 0.784386\n",
      "Epoch: 370/1000 Train loss: 0.483743 Valid loss: 0.488874 Train acc: 0.762459 Valid acc: 0.784462\n",
      "Epoch: 371/1000 Train loss: 0.483399 Valid loss: 0.488696 Train acc: 0.762665 Valid acc: 0.784544\n",
      "Epoch: 372/1000 Train loss: 0.483059 Valid loss: 0.488515 Train acc: 0.762875 Valid acc: 0.784629\n",
      "Epoch: 373/1000 Train loss: 0.482717 Valid loss: 0.488340 Train acc: 0.763091 Valid acc: 0.784714\n",
      "Epoch: 374/1000 Train loss: 0.482365 Valid loss: 0.488163 Train acc: 0.763306 Valid acc: 0.784792\n",
      "Epoch: 375/1000 Train loss: 0.482013 Valid loss: 0.487995 Train acc: 0.763516 Valid acc: 0.784865\n",
      "Epoch: 376/1000 Train loss: 0.481670 Valid loss: 0.487827 Train acc: 0.763724 Valid acc: 0.784935\n",
      "Epoch: 377/1000 Train loss: 0.481331 Valid loss: 0.487655 Train acc: 0.763928 Valid acc: 0.785007\n",
      "Epoch: 378/1000 Train loss: 0.481001 Valid loss: 0.487482 Train acc: 0.764140 Valid acc: 0.785085\n",
      "Epoch: 379/1000 Train loss: 0.480653 Valid loss: 0.487307 Train acc: 0.764359 Valid acc: 0.785168\n",
      "Epoch: 380/1000 Train loss: 0.480311 Valid loss: 0.487140 Train acc: 0.764576 Valid acc: 0.785246\n",
      "Epoch: 381/1000 Train loss: 0.479970 Valid loss: 0.486975 Train acc: 0.764776 Valid acc: 0.785318\n",
      "Epoch: 382/1000 Train loss: 0.479630 Valid loss: 0.486814 Train acc: 0.764978 Valid acc: 0.785387\n",
      "Epoch: 383/1000 Train loss: 0.479291 Valid loss: 0.486646 Train acc: 0.765192 Valid acc: 0.785455\n",
      "Epoch: 384/1000 Train loss: 0.478945 Valid loss: 0.486479 Train acc: 0.765400 Valid acc: 0.785524\n",
      "Epoch: 385/1000 Train loss: 0.478593 Valid loss: 0.486313 Train acc: 0.765615 Valid acc: 0.785595\n",
      "Epoch: 386/1000 Train loss: 0.478246 Valid loss: 0.486149 Train acc: 0.765829 Valid acc: 0.785665\n",
      "Epoch: 387/1000 Train loss: 0.477908 Valid loss: 0.485985 Train acc: 0.766032 Valid acc: 0.785730\n",
      "Epoch: 388/1000 Train loss: 0.477559 Valid loss: 0.485822 Train acc: 0.766243 Valid acc: 0.785794\n",
      "Epoch: 389/1000 Train loss: 0.477224 Valid loss: 0.485661 Train acc: 0.766451 Valid acc: 0.785858\n",
      "Epoch: 390/1000 Train loss: 0.476884 Valid loss: 0.485502 Train acc: 0.766656 Valid acc: 0.785917\n",
      "Epoch: 391/1000 Train loss: 0.476550 Valid loss: 0.485338 Train acc: 0.766855 Valid acc: 0.785981\n",
      "Epoch: 392/1000 Train loss: 0.476213 Valid loss: 0.485174 Train acc: 0.767062 Valid acc: 0.786041\n",
      "Epoch: 393/1000 Train loss: 0.475874 Valid loss: 0.485007 Train acc: 0.767273 Valid acc: 0.786108\n",
      "Epoch: 394/1000 Train loss: 0.475526 Valid loss: 0.484843 Train acc: 0.767486 Valid acc: 0.786175\n",
      "Epoch: 395/1000 Train loss: 0.475190 Valid loss: 0.484685 Train acc: 0.767693 Valid acc: 0.786236\n",
      "Epoch: 396/1000 Train loss: 0.474839 Valid loss: 0.484527 Train acc: 0.767903 Valid acc: 0.786296\n",
      "Epoch: 397/1000 Train loss: 0.474509 Valid loss: 0.484361 Train acc: 0.768107 Valid acc: 0.786362\n",
      "Epoch: 398/1000 Train loss: 0.474181 Valid loss: 0.484201 Train acc: 0.768307 Valid acc: 0.786421\n",
      "Epoch: 399/1000 Train loss: 0.473845 Valid loss: 0.484049 Train acc: 0.768502 Valid acc: 0.786480\n",
      "Epoch: 400/1000 Train loss: 0.473509 Valid loss: 0.483911 Train acc: 0.768706 Valid acc: 0.786526\n",
      "Epoch: 401/1000 Train loss: 0.473180 Valid loss: 0.483762 Train acc: 0.768909 Valid acc: 0.786574\n",
      "Epoch: 402/1000 Train loss: 0.472858 Valid loss: 0.483612 Train acc: 0.769100 Valid acc: 0.786624\n",
      "Epoch: 403/1000 Train loss: 0.472545 Valid loss: 0.483462 Train acc: 0.769294 Valid acc: 0.786677\n",
      "Epoch: 404/1000 Train loss: 0.472217 Valid loss: 0.483316 Train acc: 0.769496 Valid acc: 0.786736\n",
      "Epoch: 405/1000 Train loss: 0.471897 Valid loss: 0.483174 Train acc: 0.769685 Valid acc: 0.786793\n",
      "Epoch: 406/1000 Train loss: 0.471546 Valid loss: 0.483031 Train acc: 0.769888 Valid acc: 0.786847\n",
      "Epoch: 407/1000 Train loss: 0.471232 Valid loss: 0.482878 Train acc: 0.770087 Valid acc: 0.786904\n",
      "Epoch: 408/1000 Train loss: 0.470913 Valid loss: 0.482731 Train acc: 0.770271 Valid acc: 0.786961\n",
      "Epoch: 409/1000 Train loss: 0.470588 Valid loss: 0.482584 Train acc: 0.770464 Valid acc: 0.787020\n",
      "Epoch: 410/1000 Train loss: 0.470256 Valid loss: 0.482433 Train acc: 0.770666 Valid acc: 0.787083\n",
      "Epoch: 411/1000 Train loss: 0.469922 Valid loss: 0.482281 Train acc: 0.770869 Valid acc: 0.787146\n",
      "Epoch: 412/1000 Train loss: 0.469596 Valid loss: 0.482133 Train acc: 0.771064 Valid acc: 0.787209\n",
      "Epoch: 413/1000 Train loss: 0.469268 Valid loss: 0.481989 Train acc: 0.771262 Valid acc: 0.787273\n",
      "Epoch: 414/1000 Train loss: 0.468941 Valid loss: 0.481844 Train acc: 0.771458 Valid acc: 0.787332\n",
      "Epoch: 415/1000 Train loss: 0.468611 Valid loss: 0.481695 Train acc: 0.771650 Valid acc: 0.787391\n",
      "Epoch: 416/1000 Train loss: 0.468294 Valid loss: 0.481549 Train acc: 0.771845 Valid acc: 0.787451\n",
      "Epoch: 417/1000 Train loss: 0.467966 Valid loss: 0.481409 Train acc: 0.772036 Valid acc: 0.787509\n",
      "Epoch: 418/1000 Train loss: 0.467633 Valid loss: 0.481264 Train acc: 0.772235 Valid acc: 0.787576\n",
      "Epoch: 419/1000 Train loss: 0.467306 Valid loss: 0.481120 Train acc: 0.772442 Valid acc: 0.787635\n",
      "Epoch: 420/1000 Train loss: 0.466995 Valid loss: 0.480974 Train acc: 0.772631 Valid acc: 0.787696\n",
      "Epoch: 421/1000 Train loss: 0.466669 Valid loss: 0.480830 Train acc: 0.772824 Valid acc: 0.787754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 422/1000 Train loss: 0.466343 Valid loss: 0.480686 Train acc: 0.773021 Valid acc: 0.787809\n",
      "Epoch: 423/1000 Train loss: 0.466008 Valid loss: 0.480545 Train acc: 0.773218 Valid acc: 0.787865\n",
      "Epoch: 424/1000 Train loss: 0.465688 Valid loss: 0.480406 Train acc: 0.773413 Valid acc: 0.787922\n",
      "Epoch: 425/1000 Train loss: 0.465370 Valid loss: 0.480272 Train acc: 0.773604 Valid acc: 0.787981\n",
      "Epoch: 426/1000 Train loss: 0.465049 Valid loss: 0.480135 Train acc: 0.773794 Valid acc: 0.788044\n",
      "Epoch: 427/1000 Train loss: 0.464732 Valid loss: 0.480003 Train acc: 0.773979 Valid acc: 0.788100\n",
      "Epoch: 428/1000 Train loss: 0.464415 Valid loss: 0.479866 Train acc: 0.774171 Valid acc: 0.788163\n",
      "Epoch: 429/1000 Train loss: 0.464097 Valid loss: 0.479723 Train acc: 0.774362 Valid acc: 0.788225\n",
      "Epoch: 430/1000 Train loss: 0.463789 Valid loss: 0.479581 Train acc: 0.774550 Valid acc: 0.788282\n",
      "Epoch: 431/1000 Train loss: 0.463469 Valid loss: 0.479444 Train acc: 0.774746 Valid acc: 0.788336\n",
      "Epoch: 432/1000 Train loss: 0.463145 Valid loss: 0.479307 Train acc: 0.774930 Valid acc: 0.788391\n",
      "Epoch: 433/1000 Train loss: 0.462822 Valid loss: 0.479166 Train acc: 0.775121 Valid acc: 0.788452\n",
      "Epoch: 434/1000 Train loss: 0.462505 Valid loss: 0.479023 Train acc: 0.775310 Valid acc: 0.788514\n",
      "Epoch: 435/1000 Train loss: 0.462187 Valid loss: 0.478887 Train acc: 0.775497 Valid acc: 0.788567\n",
      "Epoch: 436/1000 Train loss: 0.461876 Valid loss: 0.478760 Train acc: 0.775684 Valid acc: 0.788621\n",
      "Epoch: 437/1000 Train loss: 0.461559 Valid loss: 0.478636 Train acc: 0.775872 Valid acc: 0.788671\n",
      "Epoch: 438/1000 Train loss: 0.461228 Valid loss: 0.478503 Train acc: 0.776070 Valid acc: 0.788730\n",
      "Epoch: 439/1000 Train loss: 0.460918 Valid loss: 0.478376 Train acc: 0.776248 Valid acc: 0.788783\n",
      "Epoch: 440/1000 Train loss: 0.460614 Valid loss: 0.478251 Train acc: 0.776426 Valid acc: 0.788840\n",
      "Epoch: 441/1000 Train loss: 0.460302 Valid loss: 0.478135 Train acc: 0.776607 Valid acc: 0.788889\n",
      "Epoch: 442/1000 Train loss: 0.459986 Valid loss: 0.478011 Train acc: 0.776799 Valid acc: 0.788945\n",
      "Epoch: 443/1000 Train loss: 0.459667 Valid loss: 0.477883 Train acc: 0.776993 Valid acc: 0.789001\n",
      "Epoch: 444/1000 Train loss: 0.459346 Valid loss: 0.477756 Train acc: 0.777182 Valid acc: 0.789057\n",
      "Epoch: 445/1000 Train loss: 0.459027 Valid loss: 0.477636 Train acc: 0.777365 Valid acc: 0.789107\n",
      "Epoch: 446/1000 Train loss: 0.458713 Valid loss: 0.477514 Train acc: 0.777544 Valid acc: 0.789161\n",
      "Epoch: 447/1000 Train loss: 0.458392 Valid loss: 0.477384 Train acc: 0.777735 Valid acc: 0.789220\n",
      "Epoch: 448/1000 Train loss: 0.458082 Valid loss: 0.477255 Train acc: 0.777919 Valid acc: 0.789277\n",
      "Epoch: 449/1000 Train loss: 0.457769 Valid loss: 0.477134 Train acc: 0.778107 Valid acc: 0.789325\n",
      "Epoch: 450/1000 Train loss: 0.457460 Valid loss: 0.477020 Train acc: 0.778288 Valid acc: 0.789370\n",
      "Epoch: 451/1000 Train loss: 0.457153 Valid loss: 0.476909 Train acc: 0.778468 Valid acc: 0.789415\n",
      "Epoch: 452/1000 Train loss: 0.456830 Valid loss: 0.476792 Train acc: 0.778653 Valid acc: 0.789464\n",
      "Epoch: 453/1000 Train loss: 0.456523 Valid loss: 0.476671 Train acc: 0.778843 Valid acc: 0.789516\n",
      "Epoch: 454/1000 Train loss: 0.456226 Valid loss: 0.476560 Train acc: 0.779019 Valid acc: 0.789558\n",
      "Epoch: 455/1000 Train loss: 0.455908 Valid loss: 0.476444 Train acc: 0.779204 Valid acc: 0.789605\n",
      "Epoch: 456/1000 Train loss: 0.455596 Valid loss: 0.476322 Train acc: 0.779388 Valid acc: 0.789646\n",
      "Epoch: 457/1000 Train loss: 0.455285 Valid loss: 0.476198 Train acc: 0.779563 Valid acc: 0.789698\n",
      "Epoch: 458/1000 Train loss: 0.454956 Valid loss: 0.476082 Train acc: 0.779756 Valid acc: 0.789747\n",
      "Epoch: 459/1000 Train loss: 0.454649 Valid loss: 0.475963 Train acc: 0.779935 Valid acc: 0.789792\n",
      "Epoch: 460/1000 Train loss: 0.454341 Valid loss: 0.475842 Train acc: 0.780118 Valid acc: 0.789844\n",
      "Epoch: 461/1000 Train loss: 0.454019 Valid loss: 0.475718 Train acc: 0.780307 Valid acc: 0.789897\n",
      "Epoch: 462/1000 Train loss: 0.453703 Valid loss: 0.475601 Train acc: 0.780494 Valid acc: 0.789943\n",
      "Epoch: 463/1000 Train loss: 0.453386 Valid loss: 0.475488 Train acc: 0.780671 Valid acc: 0.789986\n",
      "Epoch: 464/1000 Train loss: 0.453079 Valid loss: 0.475377 Train acc: 0.780854 Valid acc: 0.790027\n",
      "Epoch: 465/1000 Train loss: 0.452769 Valid loss: 0.475260 Train acc: 0.781034 Valid acc: 0.790078\n",
      "Epoch: 466/1000 Train loss: 0.452460 Valid loss: 0.475143 Train acc: 0.781210 Valid acc: 0.790131\n",
      "Epoch: 467/1000 Train loss: 0.452164 Valid loss: 0.475025 Train acc: 0.781385 Valid acc: 0.790182\n",
      "Epoch: 468/1000 Train loss: 0.451847 Valid loss: 0.474913 Train acc: 0.781563 Valid acc: 0.790226\n",
      "Epoch: 469/1000 Train loss: 0.451534 Valid loss: 0.474807 Train acc: 0.781745 Valid acc: 0.790269\n",
      "Epoch: 470/1000 Train loss: 0.451226 Valid loss: 0.474699 Train acc: 0.781921 Valid acc: 0.790312\n",
      "Epoch: 471/1000 Train loss: 0.450917 Valid loss: 0.474596 Train acc: 0.782101 Valid acc: 0.790352\n",
      "Epoch: 472/1000 Train loss: 0.450610 Valid loss: 0.474489 Train acc: 0.782278 Valid acc: 0.790392\n",
      "Epoch: 473/1000 Train loss: 0.450312 Valid loss: 0.474377 Train acc: 0.782454 Valid acc: 0.790439\n",
      "Epoch: 474/1000 Train loss: 0.450008 Valid loss: 0.474265 Train acc: 0.782631 Valid acc: 0.790485\n",
      "Epoch: 475/1000 Train loss: 0.449695 Valid loss: 0.474164 Train acc: 0.782813 Valid acc: 0.790527\n",
      "Epoch: 476/1000 Train loss: 0.449393 Valid loss: 0.474063 Train acc: 0.782992 Valid acc: 0.790563\n",
      "Epoch: 477/1000 Train loss: 0.449087 Valid loss: 0.473961 Train acc: 0.783175 Valid acc: 0.790600\n",
      "Epoch: 478/1000 Train loss: 0.448776 Valid loss: 0.473857 Train acc: 0.783351 Valid acc: 0.790643\n",
      "Epoch: 479/1000 Train loss: 0.448464 Valid loss: 0.473753 Train acc: 0.783538 Valid acc: 0.790685\n",
      "Epoch: 480/1000 Train loss: 0.448162 Valid loss: 0.473653 Train acc: 0.783713 Valid acc: 0.790727\n",
      "Epoch: 481/1000 Train loss: 0.447849 Valid loss: 0.473557 Train acc: 0.783897 Valid acc: 0.790764\n",
      "Epoch: 482/1000 Train loss: 0.447544 Valid loss: 0.473460 Train acc: 0.784072 Valid acc: 0.790797\n",
      "Epoch: 483/1000 Train loss: 0.447237 Valid loss: 0.473367 Train acc: 0.784257 Valid acc: 0.790833\n",
      "Epoch: 484/1000 Train loss: 0.446928 Valid loss: 0.473273 Train acc: 0.784431 Valid acc: 0.790870\n",
      "Epoch: 485/1000 Train loss: 0.446613 Valid loss: 0.473172 Train acc: 0.784616 Valid acc: 0.790911\n",
      "Epoch: 486/1000 Train loss: 0.446298 Valid loss: 0.473073 Train acc: 0.784802 Valid acc: 0.790951\n",
      "Epoch: 487/1000 Train loss: 0.445993 Valid loss: 0.472983 Train acc: 0.784982 Valid acc: 0.790993\n",
      "Epoch: 488/1000 Train loss: 0.445694 Valid loss: 0.472900 Train acc: 0.785154 Valid acc: 0.791021\n",
      "Epoch: 489/1000 Train loss: 0.445388 Valid loss: 0.472807 Train acc: 0.785331 Valid acc: 0.791057\n",
      "Epoch: 490/1000 Train loss: 0.445073 Valid loss: 0.472710 Train acc: 0.785514 Valid acc: 0.791093\n",
      "Epoch: 491/1000 Train loss: 0.444763 Valid loss: 0.472619 Train acc: 0.785695 Valid acc: 0.791131\n",
      "Epoch: 492/1000 Train loss: 0.444454 Valid loss: 0.472527 Train acc: 0.785872 Valid acc: 0.791169\n",
      "Epoch: 493/1000 Train loss: 0.444140 Valid loss: 0.472429 Train acc: 0.786052 Valid acc: 0.791211\n",
      "Epoch: 494/1000 Train loss: 0.443841 Valid loss: 0.472335 Train acc: 0.786226 Valid acc: 0.791252\n",
      "Epoch: 495/1000 Train loss: 0.443541 Valid loss: 0.472249 Train acc: 0.786404 Valid acc: 0.791285\n",
      "Epoch: 496/1000 Train loss: 0.443238 Valid loss: 0.472161 Train acc: 0.786578 Valid acc: 0.791323\n",
      "Epoch: 497/1000 Train loss: 0.442931 Valid loss: 0.472075 Train acc: 0.786755 Valid acc: 0.791357\n",
      "Epoch: 498/1000 Train loss: 0.442623 Valid loss: 0.471989 Train acc: 0.786934 Valid acc: 0.791392\n",
      "Epoch: 499/1000 Train loss: 0.442307 Valid loss: 0.471901 Train acc: 0.787115 Valid acc: 0.791429\n",
      "Epoch: 500/1000 Train loss: 0.441991 Valid loss: 0.471817 Train acc: 0.787296 Valid acc: 0.791463\n",
      "Epoch: 501/1000 Train loss: 0.441684 Valid loss: 0.471742 Train acc: 0.787481 Valid acc: 0.791490\n",
      "Epoch: 502/1000 Train loss: 0.441375 Valid loss: 0.471667 Train acc: 0.787665 Valid acc: 0.791518\n",
      "Epoch: 503/1000 Train loss: 0.441072 Valid loss: 0.471585 Train acc: 0.787840 Valid acc: 0.791552\n",
      "Epoch: 504/1000 Train loss: 0.440769 Valid loss: 0.471502 Train acc: 0.788015 Valid acc: 0.791585\n",
      "Epoch: 505/1000 Train loss: 0.440459 Valid loss: 0.471418 Train acc: 0.788197 Valid acc: 0.791620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 506/1000 Train loss: 0.440156 Valid loss: 0.471335 Train acc: 0.788368 Valid acc: 0.791656\n",
      "Epoch: 507/1000 Train loss: 0.439851 Valid loss: 0.471251 Train acc: 0.788548 Valid acc: 0.791699\n",
      "Epoch: 508/1000 Train loss: 0.439543 Valid loss: 0.471169 Train acc: 0.788721 Valid acc: 0.791739\n",
      "Epoch: 509/1000 Train loss: 0.439237 Valid loss: 0.471095 Train acc: 0.788898 Valid acc: 0.791769\n",
      "Epoch: 510/1000 Train loss: 0.438928 Valid loss: 0.471018 Train acc: 0.789068 Valid acc: 0.791796\n",
      "Epoch: 511/1000 Train loss: 0.438625 Valid loss: 0.470939 Train acc: 0.789237 Valid acc: 0.791823\n",
      "Epoch: 512/1000 Train loss: 0.438319 Valid loss: 0.470857 Train acc: 0.789416 Valid acc: 0.791854\n",
      "Epoch: 513/1000 Train loss: 0.438006 Valid loss: 0.470781 Train acc: 0.789597 Valid acc: 0.791886\n",
      "Epoch: 514/1000 Train loss: 0.437693 Valid loss: 0.470711 Train acc: 0.789777 Valid acc: 0.791915\n",
      "Epoch: 515/1000 Train loss: 0.437389 Valid loss: 0.470645 Train acc: 0.789950 Valid acc: 0.791943\n",
      "Epoch: 516/1000 Train loss: 0.437093 Valid loss: 0.470576 Train acc: 0.790113 Valid acc: 0.791971\n",
      "Epoch: 517/1000 Train loss: 0.436785 Valid loss: 0.470506 Train acc: 0.790283 Valid acc: 0.791999\n",
      "Epoch: 518/1000 Train loss: 0.436486 Valid loss: 0.470434 Train acc: 0.790456 Valid acc: 0.792027\n",
      "Epoch: 519/1000 Train loss: 0.436175 Valid loss: 0.470359 Train acc: 0.790630 Valid acc: 0.792054\n",
      "Epoch: 520/1000 Train loss: 0.435873 Valid loss: 0.470282 Train acc: 0.790798 Valid acc: 0.792082\n",
      "Epoch: 521/1000 Train loss: 0.435558 Valid loss: 0.470205 Train acc: 0.790973 Valid acc: 0.792112\n",
      "Epoch: 522/1000 Train loss: 0.435241 Valid loss: 0.470128 Train acc: 0.791153 Valid acc: 0.792142\n",
      "Epoch: 523/1000 Train loss: 0.434938 Valid loss: 0.470064 Train acc: 0.791328 Valid acc: 0.792169\n",
      "Epoch: 524/1000 Train loss: 0.434636 Valid loss: 0.470009 Train acc: 0.791499 Valid acc: 0.792196\n",
      "Epoch: 525/1000 Train loss: 0.434332 Valid loss: 0.469951 Train acc: 0.791667 Valid acc: 0.792228\n",
      "Epoch: 526/1000 Train loss: 0.434028 Valid loss: 0.469883 Train acc: 0.791844 Valid acc: 0.792260\n",
      "Epoch: 527/1000 Train loss: 0.433730 Valid loss: 0.469813 Train acc: 0.792015 Valid acc: 0.792291\n",
      "Epoch: 528/1000 Train loss: 0.433424 Valid loss: 0.469751 Train acc: 0.792192 Valid acc: 0.792318\n",
      "Epoch: 529/1000 Train loss: 0.433125 Valid loss: 0.469688 Train acc: 0.792366 Valid acc: 0.792343\n",
      "Epoch: 530/1000 Train loss: 0.432813 Valid loss: 0.469618 Train acc: 0.792541 Valid acc: 0.792365\n",
      "Epoch: 531/1000 Train loss: 0.432505 Valid loss: 0.469549 Train acc: 0.792713 Valid acc: 0.792385\n",
      "Epoch: 532/1000 Train loss: 0.432199 Valid loss: 0.469491 Train acc: 0.792885 Valid acc: 0.792397\n",
      "Epoch: 533/1000 Train loss: 0.431889 Valid loss: 0.469434 Train acc: 0.793063 Valid acc: 0.792413\n",
      "Epoch: 534/1000 Train loss: 0.431585 Valid loss: 0.469374 Train acc: 0.793235 Valid acc: 0.792438\n",
      "Epoch: 535/1000 Train loss: 0.431281 Valid loss: 0.469315 Train acc: 0.793405 Valid acc: 0.792465\n",
      "Epoch: 536/1000 Train loss: 0.430972 Valid loss: 0.469257 Train acc: 0.793582 Valid acc: 0.792491\n",
      "Epoch: 537/1000 Train loss: 0.430666 Valid loss: 0.469206 Train acc: 0.793757 Valid acc: 0.792519\n",
      "Epoch: 538/1000 Train loss: 0.430364 Valid loss: 0.469151 Train acc: 0.793927 Valid acc: 0.792546\n",
      "Epoch: 539/1000 Train loss: 0.430055 Valid loss: 0.469092 Train acc: 0.794100 Valid acc: 0.792574\n",
      "Epoch: 540/1000 Train loss: 0.429757 Valid loss: 0.469040 Train acc: 0.794271 Valid acc: 0.792598\n",
      "Epoch: 541/1000 Train loss: 0.429459 Valid loss: 0.468993 Train acc: 0.794440 Valid acc: 0.792620\n",
      "Epoch: 542/1000 Train loss: 0.429162 Valid loss: 0.468941 Train acc: 0.794607 Valid acc: 0.792640\n",
      "Epoch: 543/1000 Train loss: 0.428853 Valid loss: 0.468886 Train acc: 0.794781 Valid acc: 0.792664\n",
      "Epoch: 544/1000 Train loss: 0.428551 Valid loss: 0.468835 Train acc: 0.794954 Valid acc: 0.792688\n",
      "Epoch: 545/1000 Train loss: 0.428261 Valid loss: 0.468787 Train acc: 0.795116 Valid acc: 0.792711\n",
      "Epoch: 546/1000 Train loss: 0.427955 Valid loss: 0.468741 Train acc: 0.795289 Valid acc: 0.792731\n",
      "Epoch: 547/1000 Train loss: 0.427639 Valid loss: 0.468694 Train acc: 0.795466 Valid acc: 0.792752\n",
      "Epoch: 548/1000 Train loss: 0.427330 Valid loss: 0.468644 Train acc: 0.795644 Valid acc: 0.792777\n",
      "Epoch: 549/1000 Train loss: 0.427028 Valid loss: 0.468589 Train acc: 0.795815 Valid acc: 0.792802\n",
      "Epoch: 550/1000 Train loss: 0.426726 Valid loss: 0.468536 Train acc: 0.795980 Valid acc: 0.792821\n",
      "Epoch: 551/1000 Train loss: 0.426422 Valid loss: 0.468487 Train acc: 0.796151 Valid acc: 0.792837\n",
      "Epoch: 552/1000 Train loss: 0.426118 Valid loss: 0.468444 Train acc: 0.796322 Valid acc: 0.792857\n",
      "Epoch: 553/1000 Train loss: 0.425811 Valid loss: 0.468401 Train acc: 0.796497 Valid acc: 0.792878\n",
      "Epoch: 554/1000 Train loss: 0.425504 Valid loss: 0.468360 Train acc: 0.796663 Valid acc: 0.792900\n",
      "Epoch: 555/1000 Train loss: 0.425189 Valid loss: 0.468314 Train acc: 0.796841 Valid acc: 0.792924\n",
      "Epoch: 556/1000 Train loss: 0.424890 Valid loss: 0.468269 Train acc: 0.797012 Valid acc: 0.792944\n",
      "Epoch: 557/1000 Train loss: 0.424584 Valid loss: 0.468232 Train acc: 0.797189 Valid acc: 0.792962\n",
      "Epoch: 558/1000 Train loss: 0.424286 Valid loss: 0.468199 Train acc: 0.797361 Valid acc: 0.792977\n",
      "Epoch: 559/1000 Train loss: 0.423980 Valid loss: 0.468165 Train acc: 0.797535 Valid acc: 0.792997\n",
      "Epoch: 560/1000 Train loss: 0.423675 Valid loss: 0.468132 Train acc: 0.797704 Valid acc: 0.793014\n",
      "Epoch: 561/1000 Train loss: 0.423362 Valid loss: 0.468095 Train acc: 0.797876 Valid acc: 0.793033\n",
      "Epoch: 562/1000 Train loss: 0.423053 Valid loss: 0.468052 Train acc: 0.798053 Valid acc: 0.793052\n",
      "Epoch: 563/1000 Train loss: 0.422743 Valid loss: 0.468015 Train acc: 0.798227 Valid acc: 0.793071\n",
      "Epoch: 564/1000 Train loss: 0.422437 Valid loss: 0.467984 Train acc: 0.798396 Valid acc: 0.793088\n",
      "Epoch: 565/1000 Train loss: 0.422128 Valid loss: 0.467953 Train acc: 0.798569 Valid acc: 0.793106\n",
      "Epoch: 566/1000 Train loss: 0.421830 Valid loss: 0.467914 Train acc: 0.798739 Valid acc: 0.793128\n",
      "Epoch: 567/1000 Train loss: 0.421532 Valid loss: 0.467886 Train acc: 0.798910 Valid acc: 0.793145\n",
      "Epoch: 568/1000 Train loss: 0.421229 Valid loss: 0.467860 Train acc: 0.799082 Valid acc: 0.793161\n",
      "Epoch: 569/1000 Train loss: 0.420916 Valid loss: 0.467827 Train acc: 0.799253 Valid acc: 0.793184\n",
      "Epoch: 570/1000 Train loss: 0.420606 Valid loss: 0.467790 Train acc: 0.799424 Valid acc: 0.793210\n",
      "Epoch: 571/1000 Train loss: 0.420302 Valid loss: 0.467763 Train acc: 0.799594 Valid acc: 0.793228\n",
      "Epoch: 572/1000 Train loss: 0.419994 Valid loss: 0.467735 Train acc: 0.799767 Valid acc: 0.793246\n",
      "Epoch: 573/1000 Train loss: 0.419690 Valid loss: 0.467710 Train acc: 0.799938 Valid acc: 0.793266\n",
      "Epoch: 574/1000 Train loss: 0.419385 Valid loss: 0.467681 Train acc: 0.800108 Valid acc: 0.793287\n",
      "Epoch: 575/1000 Train loss: 0.419081 Valid loss: 0.467658 Train acc: 0.800280 Valid acc: 0.793307\n",
      "Epoch: 576/1000 Train loss: 0.418778 Valid loss: 0.467627 Train acc: 0.800449 Valid acc: 0.793330\n",
      "Epoch: 577/1000 Train loss: 0.418464 Valid loss: 0.467599 Train acc: 0.800624 Valid acc: 0.793351\n",
      "Epoch: 578/1000 Train loss: 0.418165 Valid loss: 0.467581 Train acc: 0.800791 Valid acc: 0.793368\n",
      "Epoch: 579/1000 Train loss: 0.417858 Valid loss: 0.467564 Train acc: 0.800965 Valid acc: 0.793386\n",
      "Epoch: 580/1000 Train loss: 0.417544 Valid loss: 0.467541 Train acc: 0.801141 Valid acc: 0.793410\n",
      "Epoch: 581/1000 Train loss: 0.417234 Valid loss: 0.467521 Train acc: 0.801313 Valid acc: 0.793435\n",
      "Epoch: 582/1000 Train loss: 0.416919 Valid loss: 0.467504 Train acc: 0.801490 Valid acc: 0.793455\n",
      "Epoch: 583/1000 Train loss: 0.416606 Valid loss: 0.467490 Train acc: 0.801665 Valid acc: 0.793470\n",
      "Epoch: 584/1000 Train loss: 0.416299 Valid loss: 0.467471 Train acc: 0.801834 Valid acc: 0.793486\n",
      "Epoch: 585/1000 Train loss: 0.415998 Valid loss: 0.467455 Train acc: 0.802008 Valid acc: 0.793503\n",
      "Epoch: 586/1000 Train loss: 0.415688 Valid loss: 0.467440 Train acc: 0.802175 Valid acc: 0.793522\n",
      "Epoch: 587/1000 Train loss: 0.415369 Valid loss: 0.467426 Train acc: 0.802351 Valid acc: 0.793537\n",
      "Epoch: 588/1000 Train loss: 0.415066 Valid loss: 0.467419 Train acc: 0.802519 Valid acc: 0.793549\n",
      "Epoch: 589/1000 Train loss: 0.414751 Valid loss: 0.467414 Train acc: 0.802693 Valid acc: 0.793559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 590/1000 Train loss: 0.414456 Valid loss: 0.467402 Train acc: 0.802857 Valid acc: 0.793575\n",
      "Epoch: 591/1000 Train loss: 0.414153 Valid loss: 0.467390 Train acc: 0.803029 Valid acc: 0.793591\n",
      "Epoch: 592/1000 Train loss: 0.413854 Valid loss: 0.467386 Train acc: 0.803199 Valid acc: 0.793604\n",
      "Epoch: 593/1000 Train loss: 0.413547 Valid loss: 0.467393 Train acc: 0.803371 Valid acc: 0.793611\n",
      "Epoch: 594/1000 Train loss: 0.413251 Valid loss: 0.467400 Train acc: 0.803541 Valid acc: 0.793617\n",
      "Epoch: 595/1000 Train loss: 0.412945 Valid loss: 0.467404 Train acc: 0.803712 Valid acc: 0.793625\n",
      "Epoch: 596/1000 Train loss: 0.412635 Valid loss: 0.467395 Train acc: 0.803887 Valid acc: 0.793632\n",
      "Epoch: 597/1000 Train loss: 0.412324 Valid loss: 0.467399 Train acc: 0.804058 Valid acc: 0.793641\n",
      "Epoch: 598/1000 Train loss: 0.412024 Valid loss: 0.467396 Train acc: 0.804218 Valid acc: 0.793655\n",
      "Epoch: 599/1000 Train loss: 0.411718 Valid loss: 0.467386 Train acc: 0.804381 Valid acc: 0.793671\n",
      "Epoch: 600/1000 Train loss: 0.411409 Valid loss: 0.467376 Train acc: 0.804551 Valid acc: 0.793685\n",
      "Epoch: 601/1000 Train loss: 0.411103 Valid loss: 0.467383 Train acc: 0.804717 Valid acc: 0.793694\n",
      "Epoch: 602/1000 Train loss: 0.410803 Valid loss: 0.467384 Train acc: 0.804886 Valid acc: 0.793703\n",
      "Epoch: 603/1000 Train loss: 0.410491 Valid loss: 0.467389 Train acc: 0.805061 Valid acc: 0.793708\n",
      "Epoch: 604/1000 Train loss: 0.410171 Valid loss: 0.467391 Train acc: 0.805237 Valid acc: 0.793720\n",
      "Epoch: 605/1000 Train loss: 0.409861 Valid loss: 0.467400 Train acc: 0.805411 Valid acc: 0.793737\n",
      "Epoch: 606/1000 Train loss: 0.409549 Valid loss: 0.467407 Train acc: 0.805581 Valid acc: 0.793756\n",
      "Epoch: 607/1000 Train loss: 0.409256 Valid loss: 0.467407 Train acc: 0.805745 Valid acc: 0.793764\n",
      "Epoch: 608/1000 Train loss: 0.408948 Valid loss: 0.467416 Train acc: 0.805913 Valid acc: 0.793769\n",
      "Epoch: 609/1000 Train loss: 0.408641 Valid loss: 0.467433 Train acc: 0.806082 Valid acc: 0.793776\n",
      "Epoch: 610/1000 Train loss: 0.408330 Valid loss: 0.467447 Train acc: 0.806254 Valid acc: 0.793786\n",
      "Epoch: 611/1000 Train loss: 0.408025 Valid loss: 0.467454 Train acc: 0.806423 Valid acc: 0.793797\n",
      "Epoch: 612/1000 Train loss: 0.407727 Valid loss: 0.467459 Train acc: 0.806587 Valid acc: 0.793812\n",
      "Epoch: 613/1000 Train loss: 0.407416 Valid loss: 0.467471 Train acc: 0.806755 Valid acc: 0.793820\n",
      "Epoch: 614/1000 Train loss: 0.407102 Valid loss: 0.467489 Train acc: 0.806934 Valid acc: 0.793832\n",
      "Epoch: 615/1000 Train loss: 0.406788 Valid loss: 0.467503 Train acc: 0.807105 Valid acc: 0.793847\n",
      "Epoch: 616/1000 Train loss: 0.406483 Valid loss: 0.467515 Train acc: 0.807271 Valid acc: 0.793865\n",
      "Epoch: 617/1000 Train loss: 0.406166 Valid loss: 0.467531 Train acc: 0.807443 Valid acc: 0.793878\n",
      "Epoch: 618/1000 Train loss: 0.405854 Valid loss: 0.467547 Train acc: 0.807616 Valid acc: 0.793891\n",
      "Epoch: 619/1000 Train loss: 0.405541 Valid loss: 0.467570 Train acc: 0.807789 Valid acc: 0.793905\n",
      "Epoch: 620/1000 Train loss: 0.405220 Valid loss: 0.467589 Train acc: 0.807964 Valid acc: 0.793916\n",
      "Epoch: 621/1000 Train loss: 0.404900 Valid loss: 0.467615 Train acc: 0.808142 Valid acc: 0.793930\n",
      "Epoch: 622/1000 Train loss: 0.404602 Valid loss: 0.467647 Train acc: 0.808308 Valid acc: 0.793941\n",
      "Epoch: 623/1000 Train loss: 0.404298 Valid loss: 0.467681 Train acc: 0.808475 Valid acc: 0.793950\n",
      "Epoch: 624/1000 Train loss: 0.403977 Valid loss: 0.467718 Train acc: 0.808647 Valid acc: 0.793957\n",
      "Epoch: 625/1000 Train loss: 0.403674 Valid loss: 0.467755 Train acc: 0.808817 Valid acc: 0.793965\n",
      "Epoch: 626/1000 Train loss: 0.403358 Valid loss: 0.467787 Train acc: 0.808987 Valid acc: 0.793975\n",
      "Epoch: 627/1000 Train loss: 0.403051 Valid loss: 0.467824 Train acc: 0.809153 Valid acc: 0.793989\n",
      "Epoch: 628/1000 Train loss: 0.402748 Valid loss: 0.467844 Train acc: 0.809323 Valid acc: 0.794007\n",
      "Epoch: 629/1000 Train loss: 0.402441 Valid loss: 0.467874 Train acc: 0.809491 Valid acc: 0.794026\n",
      "Epoch: 630/1000 Train loss: 0.402139 Valid loss: 0.467901 Train acc: 0.809655 Valid acc: 0.794045\n",
      "Epoch: 631/1000 Train loss: 0.401827 Valid loss: 0.467940 Train acc: 0.809826 Valid acc: 0.794058\n",
      "Epoch: 632/1000 Train loss: 0.401523 Valid loss: 0.467975 Train acc: 0.809994 Valid acc: 0.794077\n",
      "Epoch: 633/1000 Train loss: 0.401216 Valid loss: 0.468015 Train acc: 0.810161 Valid acc: 0.794091\n",
      "Epoch: 634/1000 Train loss: 0.400907 Valid loss: 0.468061 Train acc: 0.810329 Valid acc: 0.794099\n",
      "Epoch: 635/1000 Train loss: 0.400599 Valid loss: 0.468105 Train acc: 0.810501 Valid acc: 0.794112\n",
      "Epoch: 636/1000 Train loss: 0.400289 Valid loss: 0.468143 Train acc: 0.810671 Valid acc: 0.794123\n",
      "Epoch: 637/1000 Train loss: 0.399984 Valid loss: 0.468189 Train acc: 0.810841 Valid acc: 0.794130\n",
      "Epoch: 638/1000 Train loss: 0.399669 Valid loss: 0.468235 Train acc: 0.811014 Valid acc: 0.794134\n",
      "Epoch: 639/1000 Train loss: 0.399354 Valid loss: 0.468276 Train acc: 0.811185 Valid acc: 0.794144\n",
      "Epoch: 640/1000 Train loss: 0.399040 Valid loss: 0.468312 Train acc: 0.811358 Valid acc: 0.794160\n",
      "Epoch: 641/1000 Train loss: 0.398735 Valid loss: 0.468355 Train acc: 0.811528 Valid acc: 0.794172\n",
      "Epoch: 642/1000 Train loss: 0.398419 Valid loss: 0.468402 Train acc: 0.811698 Valid acc: 0.794181\n",
      "Epoch: 643/1000 Train loss: 0.398102 Valid loss: 0.468443 Train acc: 0.811873 Valid acc: 0.794192\n",
      "Epoch: 644/1000 Train loss: 0.397785 Valid loss: 0.468488 Train acc: 0.812044 Valid acc: 0.794203\n",
      "Epoch: 645/1000 Train loss: 0.397470 Valid loss: 0.468535 Train acc: 0.812215 Valid acc: 0.794216\n",
      "Epoch: 646/1000 Train loss: 0.397155 Valid loss: 0.468586 Train acc: 0.812385 Valid acc: 0.794229\n",
      "Epoch: 647/1000 Train loss: 0.396849 Valid loss: 0.468639 Train acc: 0.812551 Valid acc: 0.794239\n",
      "Epoch: 648/1000 Train loss: 0.396542 Valid loss: 0.468697 Train acc: 0.812716 Valid acc: 0.794249\n",
      "Epoch: 649/1000 Train loss: 0.396218 Valid loss: 0.468763 Train acc: 0.812892 Valid acc: 0.794258\n",
      "Epoch: 650/1000 Train loss: 0.395904 Valid loss: 0.468819 Train acc: 0.813057 Valid acc: 0.794268\n",
      "Epoch: 651/1000 Train loss: 0.395595 Valid loss: 0.468879 Train acc: 0.813222 Valid acc: 0.794278\n",
      "Epoch: 652/1000 Train loss: 0.395282 Valid loss: 0.468930 Train acc: 0.813388 Valid acc: 0.794290\n",
      "Epoch: 653/1000 Train loss: 0.394968 Valid loss: 0.468985 Train acc: 0.813559 Valid acc: 0.794303\n",
      "Epoch: 654/1000 Train loss: 0.394644 Valid loss: 0.469047 Train acc: 0.813736 Valid acc: 0.794316\n",
      "Epoch: 655/1000 Train loss: 0.394331 Valid loss: 0.469101 Train acc: 0.813909 Valid acc: 0.794330\n",
      "Epoch: 656/1000 Train loss: 0.394021 Valid loss: 0.469160 Train acc: 0.814075 Valid acc: 0.794340\n",
      "Epoch: 657/1000 Train loss: 0.393701 Valid loss: 0.469215 Train acc: 0.814248 Valid acc: 0.794348\n",
      "Epoch: 658/1000 Train loss: 0.393383 Valid loss: 0.469271 Train acc: 0.814416 Valid acc: 0.794356\n",
      "Epoch: 659/1000 Train loss: 0.393072 Valid loss: 0.469332 Train acc: 0.814583 Valid acc: 0.794367\n",
      "Epoch: 660/1000 Train loss: 0.392758 Valid loss: 0.469394 Train acc: 0.814749 Valid acc: 0.794375\n",
      "Epoch: 661/1000 Train loss: 0.392461 Valid loss: 0.469466 Train acc: 0.814910 Valid acc: 0.794381\n",
      "Epoch: 662/1000 Train loss: 0.392154 Valid loss: 0.469522 Train acc: 0.815080 Valid acc: 0.794386\n",
      "Epoch: 663/1000 Train loss: 0.391839 Valid loss: 0.469581 Train acc: 0.815247 Valid acc: 0.794390\n",
      "Epoch: 664/1000 Train loss: 0.391532 Valid loss: 0.469647 Train acc: 0.815411 Valid acc: 0.794392\n",
      "Epoch: 665/1000 Train loss: 0.391222 Valid loss: 0.469721 Train acc: 0.815578 Valid acc: 0.794397\n",
      "Epoch: 666/1000 Train loss: 0.390917 Valid loss: 0.469785 Train acc: 0.815740 Valid acc: 0.794408\n",
      "Epoch: 667/1000 Train loss: 0.390604 Valid loss: 0.469845 Train acc: 0.815907 Valid acc: 0.794421\n",
      "Epoch: 668/1000 Train loss: 0.390293 Valid loss: 0.469899 Train acc: 0.816076 Valid acc: 0.794432\n",
      "Epoch: 669/1000 Train loss: 0.389984 Valid loss: 0.469966 Train acc: 0.816244 Valid acc: 0.794436\n",
      "Epoch: 670/1000 Train loss: 0.389670 Valid loss: 0.470033 Train acc: 0.816415 Valid acc: 0.794442\n",
      "Epoch: 671/1000 Train loss: 0.389358 Valid loss: 0.470097 Train acc: 0.816585 Valid acc: 0.794452\n",
      "Epoch: 672/1000 Train loss: 0.389053 Valid loss: 0.470155 Train acc: 0.816748 Valid acc: 0.794466\n",
      "Epoch: 673/1000 Train loss: 0.388751 Valid loss: 0.470232 Train acc: 0.816911 Valid acc: 0.794477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 674/1000 Train loss: 0.388433 Valid loss: 0.470307 Train acc: 0.817086 Valid acc: 0.794482\n",
      "Epoch: 675/1000 Train loss: 0.388116 Valid loss: 0.470388 Train acc: 0.817255 Valid acc: 0.794486\n",
      "Epoch: 676/1000 Train loss: 0.387816 Valid loss: 0.470472 Train acc: 0.817413 Valid acc: 0.794487\n",
      "Epoch: 677/1000 Train loss: 0.387503 Valid loss: 0.470550 Train acc: 0.817575 Valid acc: 0.794495\n",
      "Epoch: 678/1000 Train loss: 0.387190 Valid loss: 0.470630 Train acc: 0.817745 Valid acc: 0.794505\n",
      "Epoch: 679/1000 Train loss: 0.386887 Valid loss: 0.470701 Train acc: 0.817906 Valid acc: 0.794512\n",
      "Epoch: 680/1000 Train loss: 0.386569 Valid loss: 0.470775 Train acc: 0.818072 Valid acc: 0.794515\n",
      "Epoch: 681/1000 Train loss: 0.386256 Valid loss: 0.470855 Train acc: 0.818238 Valid acc: 0.794522\n",
      "Epoch: 682/1000 Train loss: 0.385936 Valid loss: 0.470936 Train acc: 0.818411 Valid acc: 0.794530\n",
      "Epoch: 683/1000 Train loss: 0.385629 Valid loss: 0.471013 Train acc: 0.818577 Valid acc: 0.794539\n",
      "Epoch: 684/1000 Train loss: 0.385320 Valid loss: 0.471090 Train acc: 0.818744 Valid acc: 0.794545\n",
      "Epoch: 685/1000 Train loss: 0.385012 Valid loss: 0.471167 Train acc: 0.818907 Valid acc: 0.794553\n",
      "Epoch: 686/1000 Train loss: 0.384705 Valid loss: 0.471242 Train acc: 0.819070 Valid acc: 0.794563\n",
      "Epoch: 687/1000 Train loss: 0.384388 Valid loss: 0.471317 Train acc: 0.819238 Valid acc: 0.794571\n",
      "Epoch: 688/1000 Train loss: 0.384072 Valid loss: 0.471397 Train acc: 0.819405 Valid acc: 0.794580\n",
      "Epoch: 689/1000 Train loss: 0.383761 Valid loss: 0.471480 Train acc: 0.819574 Valid acc: 0.794590\n",
      "Epoch: 690/1000 Train loss: 0.383450 Valid loss: 0.471570 Train acc: 0.819737 Valid acc: 0.794601\n",
      "Epoch: 691/1000 Train loss: 0.383127 Valid loss: 0.471655 Train acc: 0.819908 Valid acc: 0.794614\n",
      "Epoch: 692/1000 Train loss: 0.382818 Valid loss: 0.471737 Train acc: 0.820073 Valid acc: 0.794625\n",
      "Epoch: 693/1000 Train loss: 0.382505 Valid loss: 0.471822 Train acc: 0.820239 Valid acc: 0.794635\n",
      "Epoch: 694/1000 Train loss: 0.382193 Valid loss: 0.471909 Train acc: 0.820406 Valid acc: 0.794642\n",
      "Epoch: 695/1000 Train loss: 0.381877 Valid loss: 0.472005 Train acc: 0.820576 Valid acc: 0.794647\n",
      "Epoch: 696/1000 Train loss: 0.381547 Valid loss: 0.472103 Train acc: 0.820747 Valid acc: 0.794653\n",
      "Epoch: 697/1000 Train loss: 0.381226 Valid loss: 0.472199 Train acc: 0.820916 Valid acc: 0.794665\n",
      "Epoch: 698/1000 Train loss: 0.380897 Valid loss: 0.472297 Train acc: 0.821088 Valid acc: 0.794677\n",
      "Epoch: 699/1000 Train loss: 0.380584 Valid loss: 0.472396 Train acc: 0.821252 Valid acc: 0.794684\n",
      "Epoch: 700/1000 Train loss: 0.380273 Valid loss: 0.472490 Train acc: 0.821418 Valid acc: 0.794691\n",
      "Epoch: 701/1000 Train loss: 0.379960 Valid loss: 0.472590 Train acc: 0.821584 Valid acc: 0.794701\n",
      "Epoch: 702/1000 Train loss: 0.379646 Valid loss: 0.472696 Train acc: 0.821748 Valid acc: 0.794708\n",
      "Epoch: 703/1000 Train loss: 0.379331 Valid loss: 0.472811 Train acc: 0.821912 Valid acc: 0.794712\n",
      "Epoch: 704/1000 Train loss: 0.379015 Valid loss: 0.472917 Train acc: 0.822078 Valid acc: 0.794720\n",
      "Epoch: 705/1000 Train loss: 0.378704 Valid loss: 0.473027 Train acc: 0.822242 Valid acc: 0.794725\n",
      "Epoch: 706/1000 Train loss: 0.378394 Valid loss: 0.473133 Train acc: 0.822407 Valid acc: 0.794731\n",
      "Epoch: 707/1000 Train loss: 0.378084 Valid loss: 0.473232 Train acc: 0.822571 Valid acc: 0.794742\n",
      "Epoch: 708/1000 Train loss: 0.377770 Valid loss: 0.473327 Train acc: 0.822737 Valid acc: 0.794753\n",
      "Epoch: 709/1000 Train loss: 0.377454 Valid loss: 0.473435 Train acc: 0.822903 Valid acc: 0.794762\n",
      "Epoch: 710/1000 Train loss: 0.377136 Valid loss: 0.473533 Train acc: 0.823067 Valid acc: 0.794771\n",
      "Epoch: 711/1000 Train loss: 0.376812 Valid loss: 0.473651 Train acc: 0.823238 Valid acc: 0.794777\n",
      "Epoch: 712/1000 Train loss: 0.376502 Valid loss: 0.473754 Train acc: 0.823401 Valid acc: 0.794782\n",
      "Epoch: 713/1000 Train loss: 0.376191 Valid loss: 0.473874 Train acc: 0.823566 Valid acc: 0.794780\n",
      "Epoch: 714/1000 Train loss: 0.375875 Valid loss: 0.473990 Train acc: 0.823732 Valid acc: 0.794780\n",
      "Epoch: 715/1000 Train loss: 0.375561 Valid loss: 0.474112 Train acc: 0.823897 Valid acc: 0.794784\n",
      "Epoch: 716/1000 Train loss: 0.375255 Valid loss: 0.474223 Train acc: 0.824058 Valid acc: 0.794787\n",
      "Epoch: 717/1000 Train loss: 0.374949 Valid loss: 0.474336 Train acc: 0.824218 Valid acc: 0.794790\n",
      "Epoch: 718/1000 Train loss: 0.374641 Valid loss: 0.474451 Train acc: 0.824377 Valid acc: 0.794793\n",
      "Epoch: 719/1000 Train loss: 0.374323 Valid loss: 0.474560 Train acc: 0.824540 Valid acc: 0.794795\n",
      "Epoch: 720/1000 Train loss: 0.374016 Valid loss: 0.474672 Train acc: 0.824704 Valid acc: 0.794795\n",
      "Epoch: 721/1000 Train loss: 0.373706 Valid loss: 0.474783 Train acc: 0.824871 Valid acc: 0.794799\n",
      "Epoch: 722/1000 Train loss: 0.373391 Valid loss: 0.474908 Train acc: 0.825033 Valid acc: 0.794798\n",
      "Epoch: 723/1000 Train loss: 0.373085 Valid loss: 0.475024 Train acc: 0.825193 Valid acc: 0.794797\n",
      "Epoch: 724/1000 Train loss: 0.372774 Valid loss: 0.475155 Train acc: 0.825357 Valid acc: 0.794793\n",
      "Epoch: 725/1000 Train loss: 0.372464 Valid loss: 0.475272 Train acc: 0.825518 Valid acc: 0.794792\n",
      "Epoch: 726/1000 Train loss: 0.372159 Valid loss: 0.475392 Train acc: 0.825681 Valid acc: 0.794790\n",
      "Epoch: 727/1000 Train loss: 0.371850 Valid loss: 0.475509 Train acc: 0.825843 Valid acc: 0.794793\n",
      "Epoch: 728/1000 Train loss: 0.371542 Valid loss: 0.475625 Train acc: 0.826007 Valid acc: 0.794798\n",
      "Epoch: 729/1000 Train loss: 0.371229 Valid loss: 0.475754 Train acc: 0.826172 Valid acc: 0.794796\n",
      "Epoch: 730/1000 Train loss: 0.370914 Valid loss: 0.475889 Train acc: 0.826335 Valid acc: 0.794792\n",
      "Epoch: 731/1000 Train loss: 0.370602 Valid loss: 0.476016 Train acc: 0.826500 Valid acc: 0.794796\n",
      "Epoch: 732/1000 Train loss: 0.370294 Valid loss: 0.476146 Train acc: 0.826660 Valid acc: 0.794798\n",
      "Epoch: 733/1000 Train loss: 0.369988 Valid loss: 0.476268 Train acc: 0.826819 Valid acc: 0.794798\n",
      "Epoch: 734/1000 Train loss: 0.369679 Valid loss: 0.476418 Train acc: 0.826979 Valid acc: 0.794794\n",
      "Epoch: 735/1000 Train loss: 0.369366 Valid loss: 0.476549 Train acc: 0.827140 Valid acc: 0.794797\n",
      "Epoch: 736/1000 Train loss: 0.369050 Valid loss: 0.476688 Train acc: 0.827301 Valid acc: 0.794798\n",
      "Epoch: 737/1000 Train loss: 0.368740 Valid loss: 0.476812 Train acc: 0.827464 Valid acc: 0.794804\n",
      "Epoch: 738/1000 Train loss: 0.368427 Valid loss: 0.476960 Train acc: 0.827626 Valid acc: 0.794808\n",
      "Epoch: 739/1000 Train loss: 0.368114 Valid loss: 0.477089 Train acc: 0.827788 Valid acc: 0.794808\n",
      "Epoch: 740/1000 Train loss: 0.367795 Valid loss: 0.477245 Train acc: 0.827955 Valid acc: 0.794804\n",
      "Epoch: 741/1000 Train loss: 0.367479 Valid loss: 0.477387 Train acc: 0.828115 Valid acc: 0.794802\n",
      "Epoch: 742/1000 Train loss: 0.367170 Valid loss: 0.477523 Train acc: 0.828276 Valid acc: 0.794805\n",
      "Epoch: 743/1000 Train loss: 0.366861 Valid loss: 0.477667 Train acc: 0.828439 Valid acc: 0.794810\n",
      "Epoch: 744/1000 Train loss: 0.366547 Valid loss: 0.477812 Train acc: 0.828600 Valid acc: 0.794816\n",
      "Epoch: 745/1000 Train loss: 0.366243 Valid loss: 0.477959 Train acc: 0.828758 Valid acc: 0.794817\n",
      "Epoch: 746/1000 Train loss: 0.365935 Valid loss: 0.478108 Train acc: 0.828915 Valid acc: 0.794817\n",
      "Epoch: 747/1000 Train loss: 0.365627 Valid loss: 0.478264 Train acc: 0.829072 Valid acc: 0.794813\n",
      "Epoch: 748/1000 Train loss: 0.365312 Valid loss: 0.478413 Train acc: 0.829234 Valid acc: 0.794815\n",
      "Epoch: 749/1000 Train loss: 0.365008 Valid loss: 0.478556 Train acc: 0.829394 Valid acc: 0.794819\n",
      "Epoch: 750/1000 Train loss: 0.364701 Valid loss: 0.478688 Train acc: 0.829553 Valid acc: 0.794825\n",
      "Epoch: 751/1000 Train loss: 0.364388 Valid loss: 0.478829 Train acc: 0.829717 Valid acc: 0.794822\n",
      "Epoch: 752/1000 Train loss: 0.364080 Valid loss: 0.478978 Train acc: 0.829877 Valid acc: 0.794817\n",
      "Epoch: 753/1000 Train loss: 0.363771 Valid loss: 0.479116 Train acc: 0.830038 Valid acc: 0.794818\n",
      "Epoch: 754/1000 Train loss: 0.363461 Valid loss: 0.479258 Train acc: 0.830198 Valid acc: 0.794820\n",
      "Epoch: 755/1000 Train loss: 0.363155 Valid loss: 0.479414 Train acc: 0.830357 Valid acc: 0.794814\n",
      "Epoch: 756/1000 Train loss: 0.362854 Valid loss: 0.479567 Train acc: 0.830514 Valid acc: 0.794814\n",
      "Epoch: 757/1000 Train loss: 0.362548 Valid loss: 0.479722 Train acc: 0.830674 Valid acc: 0.794812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 758/1000 Train loss: 0.362241 Valid loss: 0.479887 Train acc: 0.830832 Valid acc: 0.794809\n",
      "Epoch: 759/1000 Train loss: 0.361932 Valid loss: 0.480046 Train acc: 0.830994 Valid acc: 0.794805\n",
      "Epoch: 760/1000 Train loss: 0.361631 Valid loss: 0.480214 Train acc: 0.831150 Valid acc: 0.794799\n",
      "Epoch: 761/1000 Train loss: 0.361320 Valid loss: 0.480361 Train acc: 0.831308 Valid acc: 0.794800\n",
      "Epoch: 762/1000 Train loss: 0.361015 Valid loss: 0.480532 Train acc: 0.831469 Valid acc: 0.794795\n",
      "Epoch: 763/1000 Train loss: 0.360715 Valid loss: 0.480701 Train acc: 0.831622 Valid acc: 0.794791\n",
      "Epoch: 764/1000 Train loss: 0.360413 Valid loss: 0.480863 Train acc: 0.831780 Valid acc: 0.794790\n",
      "Epoch: 765/1000 Train loss: 0.360105 Valid loss: 0.481018 Train acc: 0.831937 Valid acc: 0.794785\n",
      "Epoch: 766/1000 Train loss: 0.359794 Valid loss: 0.481177 Train acc: 0.832098 Valid acc: 0.794781\n",
      "Epoch: 767/1000 Train loss: 0.359485 Valid loss: 0.481333 Train acc: 0.832257 Valid acc: 0.794775\n",
      "Epoch: 768/1000 Train loss: 0.359179 Valid loss: 0.481491 Train acc: 0.832417 Valid acc: 0.794771\n",
      "Epoch: 769/1000 Train loss: 0.358869 Valid loss: 0.481633 Train acc: 0.832575 Valid acc: 0.794771\n",
      "Epoch: 770/1000 Train loss: 0.358561 Valid loss: 0.481790 Train acc: 0.832733 Valid acc: 0.794768\n",
      "Epoch: 771/1000 Train loss: 0.358256 Valid loss: 0.481941 Train acc: 0.832888 Valid acc: 0.794767\n",
      "Epoch: 772/1000 Train loss: 0.357944 Valid loss: 0.482109 Train acc: 0.833047 Valid acc: 0.794764\n",
      "Epoch: 773/1000 Train loss: 0.357639 Valid loss: 0.482273 Train acc: 0.833204 Valid acc: 0.794765\n",
      "Epoch: 774/1000 Train loss: 0.357338 Valid loss: 0.482439 Train acc: 0.833362 Valid acc: 0.794765\n",
      "Epoch: 775/1000 Train loss: 0.357033 Valid loss: 0.482628 Train acc: 0.833518 Valid acc: 0.794763\n",
      "Epoch: 776/1000 Train loss: 0.356730 Valid loss: 0.482806 Train acc: 0.833672 Valid acc: 0.794765\n",
      "Epoch: 777/1000 Train loss: 0.356423 Valid loss: 0.482982 Train acc: 0.833830 Valid acc: 0.794766\n",
      "Epoch: 778/1000 Train loss: 0.356121 Valid loss: 0.483153 Train acc: 0.833985 Valid acc: 0.794768\n",
      "Epoch: 779/1000 Train loss: 0.355818 Valid loss: 0.483316 Train acc: 0.834141 Valid acc: 0.794771\n",
      "Epoch: 780/1000 Train loss: 0.355515 Valid loss: 0.483488 Train acc: 0.834295 Valid acc: 0.794776\n",
      "Epoch: 781/1000 Train loss: 0.355209 Valid loss: 0.483664 Train acc: 0.834451 Valid acc: 0.794779\n",
      "Epoch: 782/1000 Train loss: 0.354898 Valid loss: 0.483839 Train acc: 0.834607 Valid acc: 0.794774\n",
      "Epoch: 783/1000 Train loss: 0.354597 Valid loss: 0.483997 Train acc: 0.834762 Valid acc: 0.794779\n",
      "Epoch: 784/1000 Train loss: 0.354296 Valid loss: 0.484182 Train acc: 0.834918 Valid acc: 0.794771\n",
      "Epoch: 785/1000 Train loss: 0.353984 Valid loss: 0.484356 Train acc: 0.835078 Valid acc: 0.794770\n",
      "Epoch: 786/1000 Train loss: 0.353679 Valid loss: 0.484548 Train acc: 0.835233 Valid acc: 0.794766\n",
      "Epoch: 787/1000 Train loss: 0.353373 Valid loss: 0.484727 Train acc: 0.835390 Valid acc: 0.794764\n",
      "Epoch: 788/1000 Train loss: 0.353071 Valid loss: 0.484926 Train acc: 0.835545 Valid acc: 0.794757\n",
      "Epoch: 789/1000 Train loss: 0.352763 Valid loss: 0.485106 Train acc: 0.835699 Valid acc: 0.794757\n",
      "Epoch: 790/1000 Train loss: 0.352450 Valid loss: 0.485292 Train acc: 0.835857 Valid acc: 0.794759\n",
      "Epoch: 791/1000 Train loss: 0.352154 Valid loss: 0.485477 Train acc: 0.836009 Valid acc: 0.794761\n",
      "Epoch: 792/1000 Train loss: 0.351850 Valid loss: 0.485648 Train acc: 0.836166 Valid acc: 0.794764\n",
      "Epoch: 793/1000 Train loss: 0.351546 Valid loss: 0.485841 Train acc: 0.836322 Valid acc: 0.794759\n",
      "Epoch: 794/1000 Train loss: 0.351240 Valid loss: 0.486029 Train acc: 0.836476 Valid acc: 0.794757\n",
      "Epoch: 795/1000 Train loss: 0.350948 Valid loss: 0.486233 Train acc: 0.836624 Valid acc: 0.794748\n",
      "Epoch: 796/1000 Train loss: 0.350644 Valid loss: 0.486429 Train acc: 0.836779 Valid acc: 0.794742\n",
      "Epoch: 797/1000 Train loss: 0.350344 Valid loss: 0.486634 Train acc: 0.836933 Valid acc: 0.794734\n",
      "Epoch: 798/1000 Train loss: 0.350041 Valid loss: 0.486825 Train acc: 0.837086 Valid acc: 0.794727\n",
      "Epoch: 799/1000 Train loss: 0.349739 Valid loss: 0.487009 Train acc: 0.837239 Valid acc: 0.794728\n",
      "Epoch: 800/1000 Train loss: 0.349442 Valid loss: 0.487204 Train acc: 0.837390 Valid acc: 0.794729\n",
      "Epoch: 801/1000 Train loss: 0.349142 Valid loss: 0.487392 Train acc: 0.837543 Valid acc: 0.794732\n",
      "Epoch: 802/1000 Train loss: 0.348844 Valid loss: 0.487601 Train acc: 0.837694 Valid acc: 0.794727\n",
      "Epoch: 803/1000 Train loss: 0.348541 Valid loss: 0.487776 Train acc: 0.837848 Valid acc: 0.794726\n",
      "Epoch: 804/1000 Train loss: 0.348246 Valid loss: 0.487976 Train acc: 0.837996 Valid acc: 0.794718\n",
      "Epoch: 805/1000 Train loss: 0.347944 Valid loss: 0.488152 Train acc: 0.838148 Valid acc: 0.794714\n",
      "Epoch: 806/1000 Train loss: 0.347648 Valid loss: 0.488355 Train acc: 0.838298 Valid acc: 0.794714\n",
      "Epoch: 807/1000 Train loss: 0.347350 Valid loss: 0.488544 Train acc: 0.838447 Valid acc: 0.794715\n",
      "Epoch: 808/1000 Train loss: 0.347046 Valid loss: 0.488734 Train acc: 0.838603 Valid acc: 0.794714\n",
      "Epoch: 809/1000 Train loss: 0.346745 Valid loss: 0.488938 Train acc: 0.838754 Valid acc: 0.794708\n",
      "Epoch: 810/1000 Train loss: 0.346451 Valid loss: 0.489128 Train acc: 0.838901 Valid acc: 0.794704\n",
      "Epoch: 811/1000 Train loss: 0.346148 Valid loss: 0.489331 Train acc: 0.839056 Valid acc: 0.794698\n",
      "Epoch: 812/1000 Train loss: 0.345857 Valid loss: 0.489523 Train acc: 0.839205 Valid acc: 0.794695\n",
      "Epoch: 813/1000 Train loss: 0.345555 Valid loss: 0.489736 Train acc: 0.839358 Valid acc: 0.794691\n",
      "Epoch: 814/1000 Train loss: 0.345259 Valid loss: 0.489931 Train acc: 0.839506 Valid acc: 0.794686\n",
      "Epoch: 815/1000 Train loss: 0.344957 Valid loss: 0.490157 Train acc: 0.839658 Valid acc: 0.794678\n",
      "Epoch: 816/1000 Train loss: 0.344656 Valid loss: 0.490357 Train acc: 0.839810 Valid acc: 0.794674\n",
      "Epoch: 817/1000 Train loss: 0.344361 Valid loss: 0.490561 Train acc: 0.839959 Valid acc: 0.794672\n",
      "Epoch: 818/1000 Train loss: 0.344065 Valid loss: 0.490755 Train acc: 0.840108 Valid acc: 0.794670\n",
      "Epoch: 819/1000 Train loss: 0.343772 Valid loss: 0.490958 Train acc: 0.840256 Valid acc: 0.794666\n",
      "Epoch: 820/1000 Train loss: 0.343477 Valid loss: 0.491156 Train acc: 0.840405 Valid acc: 0.794665\n",
      "Epoch: 821/1000 Train loss: 0.343184 Valid loss: 0.491367 Train acc: 0.840553 Valid acc: 0.794667\n",
      "Epoch: 822/1000 Train loss: 0.342884 Valid loss: 0.491566 Train acc: 0.840703 Valid acc: 0.794670\n",
      "Epoch: 823/1000 Train loss: 0.342589 Valid loss: 0.491773 Train acc: 0.840854 Valid acc: 0.794666\n",
      "Epoch: 824/1000 Train loss: 0.342295 Valid loss: 0.491978 Train acc: 0.841000 Valid acc: 0.794656\n",
      "Epoch: 825/1000 Train loss: 0.342000 Valid loss: 0.492182 Train acc: 0.841150 Valid acc: 0.794649\n",
      "Epoch: 826/1000 Train loss: 0.341705 Valid loss: 0.492393 Train acc: 0.841299 Valid acc: 0.794647\n",
      "Epoch: 827/1000 Train loss: 0.341414 Valid loss: 0.492607 Train acc: 0.841445 Valid acc: 0.794649\n",
      "Epoch: 828/1000 Train loss: 0.341119 Valid loss: 0.492829 Train acc: 0.841590 Valid acc: 0.794649\n",
      "Epoch: 829/1000 Train loss: 0.340826 Valid loss: 0.493029 Train acc: 0.841736 Valid acc: 0.794645\n",
      "Epoch: 830/1000 Train loss: 0.340531 Valid loss: 0.493247 Train acc: 0.841885 Valid acc: 0.794635\n",
      "Epoch: 831/1000 Train loss: 0.340239 Valid loss: 0.493452 Train acc: 0.842032 Valid acc: 0.794629\n",
      "Epoch: 832/1000 Train loss: 0.339950 Valid loss: 0.493667 Train acc: 0.842179 Valid acc: 0.794625\n",
      "Epoch: 833/1000 Train loss: 0.339661 Valid loss: 0.493876 Train acc: 0.842327 Valid acc: 0.794620\n",
      "Epoch: 834/1000 Train loss: 0.339368 Valid loss: 0.494070 Train acc: 0.842475 Valid acc: 0.794619\n",
      "Epoch: 835/1000 Train loss: 0.339068 Valid loss: 0.494283 Train acc: 0.842624 Valid acc: 0.794621\n",
      "Epoch: 836/1000 Train loss: 0.338772 Valid loss: 0.494483 Train acc: 0.842772 Valid acc: 0.794625\n",
      "Epoch: 837/1000 Train loss: 0.338482 Valid loss: 0.494706 Train acc: 0.842919 Valid acc: 0.794620\n",
      "Epoch: 838/1000 Train loss: 0.338188 Valid loss: 0.494934 Train acc: 0.843068 Valid acc: 0.794610\n",
      "Epoch: 839/1000 Train loss: 0.337893 Valid loss: 0.495152 Train acc: 0.843215 Valid acc: 0.794602\n",
      "Epoch: 840/1000 Train loss: 0.337604 Valid loss: 0.495387 Train acc: 0.843358 Valid acc: 0.794594\n",
      "Epoch: 841/1000 Train loss: 0.337312 Valid loss: 0.495589 Train acc: 0.843506 Valid acc: 0.794592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 842/1000 Train loss: 0.337017 Valid loss: 0.495813 Train acc: 0.843654 Valid acc: 0.794590\n",
      "Epoch: 843/1000 Train loss: 0.336735 Valid loss: 0.496034 Train acc: 0.843794 Valid acc: 0.794586\n",
      "Epoch: 844/1000 Train loss: 0.336448 Valid loss: 0.496239 Train acc: 0.843938 Valid acc: 0.794584\n",
      "Epoch: 845/1000 Train loss: 0.336158 Valid loss: 0.496466 Train acc: 0.844083 Valid acc: 0.794581\n",
      "Epoch: 846/1000 Train loss: 0.335871 Valid loss: 0.496675 Train acc: 0.844228 Valid acc: 0.794582\n",
      "Epoch: 847/1000 Train loss: 0.335576 Valid loss: 0.496896 Train acc: 0.844373 Valid acc: 0.794580\n",
      "Epoch: 848/1000 Train loss: 0.335290 Valid loss: 0.497112 Train acc: 0.844515 Valid acc: 0.794573\n",
      "Epoch: 849/1000 Train loss: 0.335007 Valid loss: 0.497326 Train acc: 0.844657 Valid acc: 0.794568\n",
      "Epoch: 850/1000 Train loss: 0.334724 Valid loss: 0.497533 Train acc: 0.844800 Valid acc: 0.794561\n",
      "Epoch: 851/1000 Train loss: 0.334435 Valid loss: 0.497747 Train acc: 0.844946 Valid acc: 0.794555\n",
      "Epoch: 852/1000 Train loss: 0.334141 Valid loss: 0.497965 Train acc: 0.845091 Valid acc: 0.794552\n",
      "Epoch: 853/1000 Train loss: 0.333849 Valid loss: 0.498174 Train acc: 0.845238 Valid acc: 0.794553\n",
      "Epoch: 854/1000 Train loss: 0.333558 Valid loss: 0.498401 Train acc: 0.845383 Valid acc: 0.794549\n",
      "Epoch: 855/1000 Train loss: 0.333268 Valid loss: 0.498628 Train acc: 0.845529 Valid acc: 0.794542\n",
      "Epoch: 856/1000 Train loss: 0.332977 Valid loss: 0.498855 Train acc: 0.845674 Valid acc: 0.794532\n",
      "Epoch: 857/1000 Train loss: 0.332686 Valid loss: 0.499081 Train acc: 0.845819 Valid acc: 0.794524\n",
      "Epoch: 858/1000 Train loss: 0.332403 Valid loss: 0.499301 Train acc: 0.845960 Valid acc: 0.794517\n",
      "Epoch: 859/1000 Train loss: 0.332113 Valid loss: 0.499522 Train acc: 0.846105 Valid acc: 0.794510\n",
      "Epoch: 860/1000 Train loss: 0.331829 Valid loss: 0.499752 Train acc: 0.846246 Valid acc: 0.794499\n",
      "Epoch: 861/1000 Train loss: 0.331537 Valid loss: 0.499993 Train acc: 0.846393 Valid acc: 0.794484\n",
      "Epoch: 862/1000 Train loss: 0.331249 Valid loss: 0.500219 Train acc: 0.846535 Valid acc: 0.794478\n",
      "Epoch: 863/1000 Train loss: 0.330969 Valid loss: 0.500464 Train acc: 0.846674 Valid acc: 0.794469\n",
      "Epoch: 864/1000 Train loss: 0.330677 Valid loss: 0.500711 Train acc: 0.846819 Valid acc: 0.794462\n",
      "Epoch: 865/1000 Train loss: 0.330387 Valid loss: 0.500948 Train acc: 0.846961 Valid acc: 0.794460\n",
      "Epoch: 866/1000 Train loss: 0.330104 Valid loss: 0.501184 Train acc: 0.847101 Valid acc: 0.794453\n",
      "Epoch: 867/1000 Train loss: 0.329822 Valid loss: 0.501431 Train acc: 0.847241 Valid acc: 0.794443\n",
      "Epoch: 868/1000 Train loss: 0.329540 Valid loss: 0.501663 Train acc: 0.847383 Valid acc: 0.794435\n",
      "Epoch: 869/1000 Train loss: 0.329252 Valid loss: 0.501909 Train acc: 0.847525 Valid acc: 0.794419\n",
      "Epoch: 870/1000 Train loss: 0.328968 Valid loss: 0.502145 Train acc: 0.847665 Valid acc: 0.794411\n",
      "Epoch: 871/1000 Train loss: 0.328686 Valid loss: 0.502375 Train acc: 0.847807 Valid acc: 0.794404\n",
      "Epoch: 872/1000 Train loss: 0.328403 Valid loss: 0.502614 Train acc: 0.847946 Valid acc: 0.794398\n",
      "Epoch: 873/1000 Train loss: 0.328116 Valid loss: 0.502870 Train acc: 0.848087 Valid acc: 0.794388\n",
      "Epoch: 874/1000 Train loss: 0.327841 Valid loss: 0.503119 Train acc: 0.848227 Valid acc: 0.794379\n",
      "Epoch: 875/1000 Train loss: 0.327560 Valid loss: 0.503372 Train acc: 0.848367 Valid acc: 0.794372\n",
      "Epoch: 876/1000 Train loss: 0.327283 Valid loss: 0.503617 Train acc: 0.848505 Valid acc: 0.794367\n",
      "Epoch: 877/1000 Train loss: 0.327002 Valid loss: 0.503871 Train acc: 0.848645 Valid acc: 0.794359\n",
      "Epoch: 878/1000 Train loss: 0.326716 Valid loss: 0.504135 Train acc: 0.848788 Valid acc: 0.794349\n",
      "Epoch: 879/1000 Train loss: 0.326430 Valid loss: 0.504377 Train acc: 0.848927 Valid acc: 0.794345\n",
      "Epoch: 880/1000 Train loss: 0.326153 Valid loss: 0.504640 Train acc: 0.849064 Valid acc: 0.794336\n",
      "Epoch: 881/1000 Train loss: 0.325872 Valid loss: 0.504867 Train acc: 0.849203 Valid acc: 0.794339\n",
      "Epoch: 882/1000 Train loss: 0.325591 Valid loss: 0.505100 Train acc: 0.849343 Valid acc: 0.794335\n",
      "Epoch: 883/1000 Train loss: 0.325318 Valid loss: 0.505342 Train acc: 0.849480 Valid acc: 0.794323\n",
      "Epoch: 884/1000 Train loss: 0.325042 Valid loss: 0.505576 Train acc: 0.849618 Valid acc: 0.794315\n",
      "Epoch: 885/1000 Train loss: 0.324766 Valid loss: 0.505815 Train acc: 0.849753 Valid acc: 0.794309\n",
      "Epoch: 886/1000 Train loss: 0.324484 Valid loss: 0.506049 Train acc: 0.849893 Valid acc: 0.794302\n",
      "Epoch: 887/1000 Train loss: 0.324202 Valid loss: 0.506289 Train acc: 0.850032 Valid acc: 0.794296\n",
      "Epoch: 888/1000 Train loss: 0.323925 Valid loss: 0.506535 Train acc: 0.850169 Valid acc: 0.794290\n",
      "Epoch: 889/1000 Train loss: 0.323649 Valid loss: 0.506768 Train acc: 0.850307 Valid acc: 0.794286\n",
      "Epoch: 890/1000 Train loss: 0.323371 Valid loss: 0.507017 Train acc: 0.850444 Valid acc: 0.794279\n",
      "Epoch: 891/1000 Train loss: 0.323096 Valid loss: 0.507245 Train acc: 0.850578 Valid acc: 0.794272\n",
      "Epoch: 892/1000 Train loss: 0.322821 Valid loss: 0.507497 Train acc: 0.850713 Valid acc: 0.794256\n",
      "Epoch: 893/1000 Train loss: 0.322543 Valid loss: 0.507751 Train acc: 0.850851 Valid acc: 0.794242\n",
      "Epoch: 894/1000 Train loss: 0.322269 Valid loss: 0.507995 Train acc: 0.850986 Valid acc: 0.794235\n",
      "Epoch: 895/1000 Train loss: 0.321989 Valid loss: 0.508271 Train acc: 0.851124 Valid acc: 0.794222\n",
      "Epoch: 896/1000 Train loss: 0.321714 Valid loss: 0.508531 Train acc: 0.851259 Valid acc: 0.794210\n",
      "Epoch: 897/1000 Train loss: 0.321438 Valid loss: 0.508787 Train acc: 0.851395 Valid acc: 0.794202\n",
      "Epoch: 898/1000 Train loss: 0.321160 Valid loss: 0.509035 Train acc: 0.851532 Valid acc: 0.794196\n",
      "Epoch: 899/1000 Train loss: 0.320881 Valid loss: 0.509288 Train acc: 0.851667 Valid acc: 0.794191\n",
      "Epoch: 900/1000 Train loss: 0.320602 Valid loss: 0.509558 Train acc: 0.851805 Valid acc: 0.794184\n",
      "Epoch: 901/1000 Train loss: 0.320328 Valid loss: 0.509825 Train acc: 0.851939 Valid acc: 0.794178\n",
      "Epoch: 902/1000 Train loss: 0.320056 Valid loss: 0.510075 Train acc: 0.852073 Valid acc: 0.794174\n",
      "Epoch: 903/1000 Train loss: 0.319786 Valid loss: 0.510323 Train acc: 0.852207 Valid acc: 0.794169\n",
      "Epoch: 904/1000 Train loss: 0.319514 Valid loss: 0.510593 Train acc: 0.852341 Valid acc: 0.794159\n",
      "Epoch: 905/1000 Train loss: 0.319240 Valid loss: 0.510846 Train acc: 0.852477 Valid acc: 0.794152\n",
      "Epoch: 906/1000 Train loss: 0.318967 Valid loss: 0.511104 Train acc: 0.852610 Valid acc: 0.794144\n",
      "Epoch: 907/1000 Train loss: 0.318696 Valid loss: 0.511343 Train acc: 0.852743 Valid acc: 0.794145\n",
      "Epoch: 908/1000 Train loss: 0.318424 Valid loss: 0.511607 Train acc: 0.852876 Valid acc: 0.794136\n",
      "Epoch: 909/1000 Train loss: 0.318150 Valid loss: 0.511876 Train acc: 0.853011 Valid acc: 0.794122\n",
      "Epoch: 910/1000 Train loss: 0.317880 Valid loss: 0.512118 Train acc: 0.853142 Valid acc: 0.794109\n",
      "Epoch: 911/1000 Train loss: 0.317612 Valid loss: 0.512399 Train acc: 0.853275 Valid acc: 0.794096\n",
      "Epoch: 912/1000 Train loss: 0.317339 Valid loss: 0.512649 Train acc: 0.853409 Valid acc: 0.794088\n",
      "Epoch: 913/1000 Train loss: 0.317070 Valid loss: 0.512899 Train acc: 0.853542 Valid acc: 0.794080\n",
      "Epoch: 914/1000 Train loss: 0.316799 Valid loss: 0.513171 Train acc: 0.853675 Valid acc: 0.794068\n",
      "Epoch: 915/1000 Train loss: 0.316522 Valid loss: 0.513456 Train acc: 0.853810 Valid acc: 0.794052\n",
      "Epoch: 916/1000 Train loss: 0.316252 Valid loss: 0.513735 Train acc: 0.853944 Valid acc: 0.794039\n",
      "Epoch: 917/1000 Train loss: 0.315982 Valid loss: 0.514011 Train acc: 0.854075 Valid acc: 0.794028\n",
      "Epoch: 918/1000 Train loss: 0.315713 Valid loss: 0.514289 Train acc: 0.854207 Valid acc: 0.794018\n",
      "Epoch: 919/1000 Train loss: 0.315440 Valid loss: 0.514567 Train acc: 0.854340 Valid acc: 0.794007\n",
      "Epoch: 920/1000 Train loss: 0.315170 Valid loss: 0.514849 Train acc: 0.854472 Valid acc: 0.793997\n",
      "Epoch: 921/1000 Train loss: 0.314902 Valid loss: 0.515157 Train acc: 0.854605 Valid acc: 0.793983\n",
      "Epoch: 922/1000 Train loss: 0.314637 Valid loss: 0.515421 Train acc: 0.854736 Valid acc: 0.793981\n",
      "Epoch: 923/1000 Train loss: 0.314372 Valid loss: 0.515682 Train acc: 0.854867 Valid acc: 0.793983\n",
      "Epoch: 924/1000 Train loss: 0.314101 Valid loss: 0.515959 Train acc: 0.854999 Valid acc: 0.793982\n",
      "Epoch: 925/1000 Train loss: 0.313829 Valid loss: 0.516224 Train acc: 0.855132 Valid acc: 0.793978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 926/1000 Train loss: 0.313556 Valid loss: 0.516512 Train acc: 0.855265 Valid acc: 0.793967\n",
      "Epoch: 927/1000 Train loss: 0.313288 Valid loss: 0.516811 Train acc: 0.855396 Valid acc: 0.793958\n",
      "Epoch: 928/1000 Train loss: 0.313024 Valid loss: 0.517089 Train acc: 0.855527 Valid acc: 0.793955\n",
      "Epoch: 929/1000 Train loss: 0.312758 Valid loss: 0.517368 Train acc: 0.855655 Valid acc: 0.793952\n",
      "Epoch: 930/1000 Train loss: 0.312492 Valid loss: 0.517644 Train acc: 0.855786 Valid acc: 0.793944\n",
      "Epoch: 931/1000 Train loss: 0.312229 Valid loss: 0.517925 Train acc: 0.855916 Valid acc: 0.793936\n",
      "Epoch: 932/1000 Train loss: 0.311964 Valid loss: 0.518200 Train acc: 0.856045 Valid acc: 0.793930\n",
      "Epoch: 933/1000 Train loss: 0.311701 Valid loss: 0.518481 Train acc: 0.856174 Valid acc: 0.793921\n",
      "Epoch: 934/1000 Train loss: 0.311443 Valid loss: 0.518752 Train acc: 0.856302 Valid acc: 0.793915\n",
      "Epoch: 935/1000 Train loss: 0.311179 Valid loss: 0.519026 Train acc: 0.856431 Valid acc: 0.793905\n",
      "Epoch: 936/1000 Train loss: 0.310913 Valid loss: 0.519304 Train acc: 0.856560 Valid acc: 0.793895\n",
      "Epoch: 937/1000 Train loss: 0.310651 Valid loss: 0.519565 Train acc: 0.856690 Valid acc: 0.793888\n",
      "Epoch: 938/1000 Train loss: 0.310392 Valid loss: 0.519826 Train acc: 0.856819 Valid acc: 0.793884\n",
      "Epoch: 939/1000 Train loss: 0.310126 Valid loss: 0.520096 Train acc: 0.856949 Valid acc: 0.793877\n",
      "Epoch: 940/1000 Train loss: 0.309864 Valid loss: 0.520390 Train acc: 0.857077 Valid acc: 0.793868\n",
      "Epoch: 941/1000 Train loss: 0.309602 Valid loss: 0.520663 Train acc: 0.857207 Valid acc: 0.793860\n",
      "Epoch: 942/1000 Train loss: 0.309341 Valid loss: 0.520931 Train acc: 0.857335 Valid acc: 0.793853\n",
      "Epoch: 943/1000 Train loss: 0.309076 Valid loss: 0.521220 Train acc: 0.857464 Valid acc: 0.793844\n",
      "Epoch: 944/1000 Train loss: 0.308816 Valid loss: 0.521481 Train acc: 0.857592 Valid acc: 0.793840\n",
      "Epoch: 945/1000 Train loss: 0.308557 Valid loss: 0.521780 Train acc: 0.857718 Valid acc: 0.793829\n",
      "Epoch: 946/1000 Train loss: 0.308300 Valid loss: 0.522048 Train acc: 0.857844 Valid acc: 0.793819\n",
      "Epoch: 947/1000 Train loss: 0.308039 Valid loss: 0.522329 Train acc: 0.857971 Valid acc: 0.793805\n",
      "Epoch: 948/1000 Train loss: 0.307778 Valid loss: 0.522627 Train acc: 0.858098 Valid acc: 0.793791\n",
      "Epoch: 949/1000 Train loss: 0.307522 Valid loss: 0.522902 Train acc: 0.858224 Valid acc: 0.793783\n",
      "Epoch: 950/1000 Train loss: 0.307260 Valid loss: 0.523189 Train acc: 0.858351 Valid acc: 0.793774\n",
      "Epoch: 951/1000 Train loss: 0.307000 Valid loss: 0.523463 Train acc: 0.858477 Valid acc: 0.793764\n",
      "Epoch: 952/1000 Train loss: 0.306738 Valid loss: 0.523734 Train acc: 0.858604 Valid acc: 0.793750\n",
      "Epoch: 953/1000 Train loss: 0.306481 Valid loss: 0.524024 Train acc: 0.858730 Valid acc: 0.793738\n",
      "Epoch: 954/1000 Train loss: 0.306220 Valid loss: 0.524301 Train acc: 0.858856 Valid acc: 0.793731\n",
      "Epoch: 955/1000 Train loss: 0.305963 Valid loss: 0.524586 Train acc: 0.858982 Valid acc: 0.793724\n",
      "Epoch: 956/1000 Train loss: 0.305700 Valid loss: 0.524849 Train acc: 0.859110 Valid acc: 0.793721\n",
      "Epoch: 957/1000 Train loss: 0.305448 Valid loss: 0.525103 Train acc: 0.859233 Valid acc: 0.793716\n",
      "Epoch: 958/1000 Train loss: 0.305194 Valid loss: 0.525378 Train acc: 0.859357 Valid acc: 0.793700\n",
      "Epoch: 959/1000 Train loss: 0.304939 Valid loss: 0.525635 Train acc: 0.859482 Valid acc: 0.793688\n",
      "Epoch: 960/1000 Train loss: 0.304682 Valid loss: 0.525921 Train acc: 0.859605 Valid acc: 0.793675\n",
      "Epoch: 961/1000 Train loss: 0.304428 Valid loss: 0.526224 Train acc: 0.859726 Valid acc: 0.793663\n",
      "Epoch: 962/1000 Train loss: 0.304169 Valid loss: 0.526502 Train acc: 0.859852 Valid acc: 0.793653\n",
      "Epoch: 963/1000 Train loss: 0.303917 Valid loss: 0.526788 Train acc: 0.859973 Valid acc: 0.793643\n",
      "Epoch: 964/1000 Train loss: 0.303664 Valid loss: 0.527081 Train acc: 0.860096 Valid acc: 0.793632\n",
      "Epoch: 965/1000 Train loss: 0.303403 Valid loss: 0.527352 Train acc: 0.860223 Valid acc: 0.793627\n",
      "Epoch: 966/1000 Train loss: 0.303143 Valid loss: 0.527643 Train acc: 0.860349 Valid acc: 0.793618\n",
      "Epoch: 967/1000 Train loss: 0.302896 Valid loss: 0.527929 Train acc: 0.860470 Valid acc: 0.793610\n",
      "Epoch: 968/1000 Train loss: 0.302644 Valid loss: 0.528203 Train acc: 0.860592 Valid acc: 0.793608\n",
      "Epoch: 969/1000 Train loss: 0.302392 Valid loss: 0.528473 Train acc: 0.860715 Valid acc: 0.793603\n",
      "Epoch: 970/1000 Train loss: 0.302143 Valid loss: 0.528753 Train acc: 0.860838 Valid acc: 0.793591\n",
      "Epoch: 971/1000 Train loss: 0.301891 Valid loss: 0.529025 Train acc: 0.860959 Valid acc: 0.793580\n",
      "Epoch: 972/1000 Train loss: 0.301636 Valid loss: 0.529318 Train acc: 0.861083 Valid acc: 0.793568\n",
      "Epoch: 973/1000 Train loss: 0.301377 Valid loss: 0.529601 Train acc: 0.861207 Valid acc: 0.793559\n",
      "Epoch: 974/1000 Train loss: 0.301126 Valid loss: 0.529851 Train acc: 0.861328 Valid acc: 0.793552\n",
      "Epoch: 975/1000 Train loss: 0.300878 Valid loss: 0.530155 Train acc: 0.861451 Valid acc: 0.793539\n",
      "Epoch: 976/1000 Train loss: 0.300626 Valid loss: 0.530434 Train acc: 0.861573 Valid acc: 0.793530\n",
      "Epoch: 977/1000 Train loss: 0.300375 Valid loss: 0.530709 Train acc: 0.861695 Valid acc: 0.793522\n",
      "Epoch: 978/1000 Train loss: 0.300125 Valid loss: 0.531025 Train acc: 0.861816 Valid acc: 0.793509\n",
      "Epoch: 979/1000 Train loss: 0.299879 Valid loss: 0.531294 Train acc: 0.861935 Valid acc: 0.793502\n",
      "Epoch: 980/1000 Train loss: 0.299632 Valid loss: 0.531563 Train acc: 0.862056 Valid acc: 0.793496\n",
      "Epoch: 981/1000 Train loss: 0.299384 Valid loss: 0.531845 Train acc: 0.862176 Valid acc: 0.793485\n",
      "Epoch: 982/1000 Train loss: 0.299134 Valid loss: 0.532088 Train acc: 0.862296 Valid acc: 0.793477\n",
      "Epoch: 983/1000 Train loss: 0.298882 Valid loss: 0.532359 Train acc: 0.862418 Valid acc: 0.793469\n",
      "Epoch: 984/1000 Train loss: 0.298632 Valid loss: 0.532642 Train acc: 0.862539 Valid acc: 0.793463\n",
      "Epoch: 985/1000 Train loss: 0.298384 Valid loss: 0.532906 Train acc: 0.862659 Valid acc: 0.793457\n",
      "Epoch: 986/1000 Train loss: 0.298135 Valid loss: 0.533204 Train acc: 0.862779 Valid acc: 0.793445\n",
      "Epoch: 987/1000 Train loss: 0.297887 Valid loss: 0.533493 Train acc: 0.862898 Valid acc: 0.793433\n",
      "Epoch: 988/1000 Train loss: 0.297643 Valid loss: 0.533759 Train acc: 0.863016 Valid acc: 0.793424\n",
      "Epoch: 989/1000 Train loss: 0.297399 Valid loss: 0.534032 Train acc: 0.863134 Valid acc: 0.793413\n",
      "Epoch: 990/1000 Train loss: 0.297156 Valid loss: 0.534296 Train acc: 0.863251 Valid acc: 0.793406\n",
      "Epoch: 991/1000 Train loss: 0.296911 Valid loss: 0.534575 Train acc: 0.863371 Valid acc: 0.793399\n",
      "Epoch: 992/1000 Train loss: 0.296664 Valid loss: 0.534856 Train acc: 0.863489 Valid acc: 0.793390\n",
      "Epoch: 993/1000 Train loss: 0.296416 Valid loss: 0.535144 Train acc: 0.863610 Valid acc: 0.793382\n",
      "Epoch: 994/1000 Train loss: 0.296168 Valid loss: 0.535424 Train acc: 0.863729 Valid acc: 0.793372\n",
      "Epoch: 995/1000 Train loss: 0.295928 Valid loss: 0.535731 Train acc: 0.863845 Valid acc: 0.793359\n",
      "Epoch: 996/1000 Train loss: 0.295684 Valid loss: 0.536014 Train acc: 0.863964 Valid acc: 0.793352\n",
      "Epoch: 997/1000 Train loss: 0.295444 Valid loss: 0.536308 Train acc: 0.864081 Valid acc: 0.793340\n",
      "Epoch: 998/1000 Train loss: 0.295200 Valid loss: 0.536597 Train acc: 0.864200 Valid acc: 0.793330\n",
      "Epoch: 999/1000 Train loss: 0.294956 Valid loss: 0.536880 Train acc: 0.864318 Valid acc: 0.793322\n",
      "Epoch: 1000/1000 Train loss: 0.294713 Valid loss: 0.537170 Train acc: 0.864435 Valid acc: 0.793308\n"
     ]
    }
   ],
   "source": [
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "\n",
    "# Save the training result or trained and validated model params\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# hyperparameters\n",
    "epochs = 1000 # num iterations for updating model\n",
    "kernel_size_ratios = 1 # conv size for conv minibatching \n",
    "strides_ratios = 1 # conv stride for conv minibatching\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Minibatching techniques tgt batch size: kernel_size_ratio\n",
    "    for k in range(0, kernel_size_ratios, 1):\n",
    "\n",
    "        # Minibatching techniques sampling: strides_ratio\n",
    "        for s in range(0, strides_ratios, 1):\n",
    "\n",
    "            # Loop over epochs\n",
    "            for e in range(0, epochs, 1):\n",
    "\n",
    "                # Loop over batches\n",
    "                for x, y in get_batches2(X_norm=X_train_norm, Y_labels=Y_train):   \n",
    "\n",
    "                    ######################## Training\n",
    "                    # Feed dictionary\n",
    "                    feed = {inputs_ : x, labels_ : y, keep_prob_ : keep_prob, learning_rate_ : learning_rate}\n",
    "\n",
    "                    # Loss\n",
    "                    loss, _ , acc = sess.run(fetches=[cost_ave, optimizer, correct_pred_ave], feed_dict = feed)\n",
    "                    train_acc.append(acc)\n",
    "                    train_loss.append(loss)\n",
    "\n",
    "                    ################## Validation\n",
    "                    acc_batch = []\n",
    "                    loss_batch = []    \n",
    "                    # Loop over batches\n",
    "                    for x, y in get_batches2(X_norm=X_valid_norm, Y_labels=Y_valid):\n",
    "\n",
    "                        # Feed dictionary\n",
    "                        feed = {inputs_ : x, labels_ : y, keep_prob_ : 1.0}\n",
    "\n",
    "                        # Loss\n",
    "                        loss, acc = sess.run(fetches=[cost_ave, correct_pred_ave], feed_dict = feed)\n",
    "                        acc_batch.append(acc)\n",
    "                        loss_batch.append(loss)\n",
    "\n",
    "                    # Store\n",
    "                    valid_acc.append(np.mean(acc_batch))\n",
    "                    valid_loss.append(np.mean(loss_batch))\n",
    "\n",
    "                # Print info for every iter/epoch\n",
    "                print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "#                       \"kernel_size_ratio: {}/{}\".format(k+1, kernel_size_ratios),\n",
    "#                       \"strides_ratio: {}/{}\".format(s+1, strides_ratios),\n",
    "                      \"Train loss: {:6f}\".format(np.mean(train_loss)),\n",
    "                      \"Valid loss: {:.6f}\".format(np.mean(valid_loss)),\n",
    "                      \"Train acc: {:6f}\".format(np.mean(train_acc)),\n",
    "                      \"Valid acc: {:.6f}\".format(np.mean(valid_acc)))\n",
    "                \n",
    "    saver.save(sess,\"checkpoints/dcnn-face.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feab98e6d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as mplot\n",
    "\n",
    "mplot.plot(train_loss, label='Face train_loss')\n",
    "mplot.plot(valid_loss, label='Face valid_loss')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feae409d940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as mplot\n",
    "mplot.plot(train_acc, label='Face train_acc')\n",
    "mplot.plot(valid_acc, label='Face valid_acc')\n",
    "mplot.legend()\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/dcnn-face.ckpt\n"
     ]
    }
   ],
   "source": [
    "listcost_ave, listcorrect_pred_ave, listconfusion = [], [], []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore the validated model\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints/'))\n",
    "    \n",
    "    # Loop over batches\n",
    "    for x, y in get_batches(batch_size=100, X=X_test_norm, y=Y_test_onehot):\n",
    "\n",
    "        # Feed dictionary\n",
    "        feed = {inputs_ : x, labels_ : y, keep_prob_ : 1.0}\n",
    "\n",
    "        # Loss\n",
    "        arrcost_ave, arrcorrect_pred_ave, arrconfusion = sess.run([\n",
    "            cost_ave, correct_pred_ave, confusion], feed_dict = feed)\n",
    "        \n",
    "        listcost_ave.append(arrcost_ave) \n",
    "        listcorrect_pred_ave.append(arrcorrect_pred_ave)\n",
    "        listconfusion.append(arrconfusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 56, 56)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(listcorrect_pred_ave), len(listcost_ave), len(listconfusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost_ave_mean, correct_pred_ave_mean, confusion_mean.shape: 0.512763 0.857143 (2, 2)\n"
     ]
    }
   ],
   "source": [
    "cost_ave_mean = np.array(listcost_ave, dtype=arrcost_ave.dtype).mean(axis=0)\n",
    "correct_pred_ave_mean = np.array(listcorrect_pred_ave, dtype=arrcorrect_pred_ave.dtype).mean(axis=0)\n",
    "confusion_mean = np.array(listconfusion, dtype=arrconfusion.dtype).mean(axis=0)\n",
    "print('cost_ave_mean, correct_pred_ave_mean, confusion_mean.shape:', \n",
    "     cost_ave_mean, correct_pred_ave_mean, confusion_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 74.55357361,   9.39285755],\n",
       "       [  4.89285707,  11.16071415]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy vs correct_pred_ave_mean: 0.857143 0.857143\n"
     ]
    }
   ],
   "source": [
    "# accuracy or 1- loss\n",
    "accuracy = (confusion_mean[0, 0] + confusion_mean[1, 1])/ (confusion_mean[0, 0] + confusion_mean[0, 1] + \n",
    "                                                confusion_mean[1, 0] + confusion_mean[1, 1])\n",
    "print('accuracy vs correct_pred_ave_mean:', accuracy, correct_pred_ave_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(correct_pred_ave_mean - accuracy): 5.96046e-08\n"
     ]
    }
   ],
   "source": [
    "print('(correct_pred_ave_mean - accuracy):', correct_pred_ave_mean - accuracy)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss or (1-acc): 0.142857\n"
     ]
    }
   ],
   "source": [
    "# accuracy or 1- loss\n",
    "loss = (confusion_mean[1, 0] + confusion_mean[0, 1])/ (confusion_mean[0, 0] + confusion_mean[0, 1] + \n",
    "                                                       confusion_mean[1, 0] + confusion_mean[1, 1])\n",
    "print('loss or (1-acc):', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity: 0.938413\n"
     ]
    }
   ],
   "source": [
    "# sensitivity: TP/ (TP+FNs)  TPR or recall for multiclass\n",
    "# 1st col or true prediction condition\n",
    "sensitivity = confusion_mean[0, 0]/ (confusion_mean[0, 0] + \n",
    "                                     confusion_mean[1, 0])\n",
    "print('sensitivity:', sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specificity: 0.543006\n"
     ]
    }
   ],
   "source": [
    "# specificity: TN/ (TN + FPs) TNR or recall for multiclass\n",
    "# 2nd col or false prediction condition\n",
    "# (1st row, 2nd col)/ \n",
    "specificity = confusion_mean[1, 1]/ (confusion_mean[0, 1] + \n",
    "                                     confusion_mean[1, 1])\n",
    "print('specificity:', specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.888109\n"
     ]
    }
   ],
   "source": [
    "# precision: TP / (TP + FPs) for 1st row\n",
    "precision = confusion_mean[0, 0]/ (confusion_mean[0, 0] + confusion_mean[0, 1])\n",
    "print('precision:', precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision2: 0.304783\n"
     ]
    }
   ],
   "source": [
    "# precision: TP / (TP + FP) for 2nd row\n",
    "precision2 = confusion_mean[1, 0]/ (confusion_mean[1, 0] + confusion_mean[1, 1])\n",
    "print('precision2:', precision2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 100.0\n"
     ]
    }
   ],
   "source": [
    "print('total:', (confusion_mean[0, 0] + confusion_mean[0, 1] + confusion_mean[1, 0] + confusion_mean[1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARUAAAD8CAYAAABZ0jAcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fec0cad9e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mplot.imshow(confusion_mean)\n",
    "mplot.show()\n",
    "mplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
