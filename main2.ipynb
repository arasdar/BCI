{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL Brain project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O10test1.txt   O2valid3.txt  O4valid2.txt  O6valid1.txt  O8train.txt\r\n",
      "O10test2.txt   O3test1.txt   O4valid3.txt  O6valid2.txt  O8valid1.txt\r\n",
      "O10test3.txt   O3test2.txt   O5test1.txt   O6valid3.txt  O8valid2.txt\r\n",
      "O10train.txt   O3test3.txt   O5test2.txt   O7test1.txt   O8valid3.txt\r\n",
      "O10valid1.txt  O3train.txt   O5test3.txt   O7test2.txt   O9test1.txt\r\n",
      "O10valid2.txt  O3valid1.txt  O5train.txt   O7test3.txt   O9test2.txt\r\n",
      "O10valid3.txt  O3valid2.txt  O5valid1.txt  O7train.txt   O9test3.txt\r\n",
      "O2test1.txt    O3valid3.txt  O5valid2.txt  O7valid1.txt  O9train.txt\r\n",
      "O2test2.txt    O4test1.txt   O5valid3.txt  O7valid2.txt  O9valid1.txt\r\n",
      "O2test3.txt    O4test2.txt   O6test1.txt   O7valid3.txt  O9valid2.txt\r\n",
      "O2train.txt    O4test3.txt   O6test2.txt   O8test1.txt   O9valid3.txt\r\n",
      "O2valid1.txt   O4train.txt   O6test3.txt   O8test2.txt\r\n",
      "O2valid2.txt   O4valid1.txt  O6train.txt   O8test3.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls /home/arasdar/datasets/DL-BrainBody/Control/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P10test1.txt   P11valid3.txt  P4valid2.txt  P6valid1.txt  P8train.txt\r\n",
      "P10test2.txt   P1test1.txt    P4valid3.txt  P6valid2.txt  P8valid1.txt\r\n",
      "P10test3.txt   P1test2.txt    P5test1.txt   P6valid3.txt  P8valid2.txt\r\n",
      "P10train.txt   P1test3.txt    P5test2.txt   P7test1.txt   P8valid3.txt\r\n",
      "P10valid1.txt  P1train.txt    P5test3.txt   P7test2.txt   P9test1.txt\r\n",
      "P10valid2.txt  P1valid1.txt   P5train.txt   P7test3.txt   P9test2.txt\r\n",
      "P10valid3.txt  P1valid2.txt   P5valid1.txt  P7train.txt   P9test3.txt\r\n",
      "P11test1.txt   P1valid3.txt   P5valid2.txt  P7valid1.txt  P9train.txt\r\n",
      "P11test2.txt   P4test1.txt    P5valid3.txt  P7valid2.txt  P9valid1.txt\r\n",
      "P11test3.txt   P4test2.txt    P6test1.txt   P7valid3.txt  P9valid2.txt\r\n",
      "P11train.txt   P4test3.txt    P6test2.txt   P8test1.txt   P9valid3.txt\r\n",
      "P11valid1.txt  P4train.txt    P6test3.txt   P8test2.txt\r\n",
      "P11valid2.txt  P4valid1.txt   P6train.txt   P8test3.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls /home/arasdar/datasets/DL-BrainBody/PD/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_02train = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/Control/O2train.txt', header=None, \n",
    "                        delim_whitespace=True)\n",
    "X_03train = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/Control/O3train.txt', header=None, \n",
    "                        delim_whitespace=True)\n",
    "X_04train = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/Control/O4train.txt', header=None, \n",
    "                        delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_02val1 = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/Control/O2valid1.txt', header=None, \n",
    "                        delim_whitespace=True)\n",
    "X_02val2 = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/Control/O2valid2.txt', header=None, \n",
    "                        delim_whitespace=True)\n",
    "X_02val3 = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/Control/O2valid3.txt', header=None, \n",
    "                        delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_02test1 = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/Control/O2test1.txt', header=None, \n",
    "                       delim_whitespace=True)\n",
    "X_02test2 = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/Control/O2test2.txt', header=None, \n",
    "                       delim_whitespace=True)\n",
    "X_02test3 = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/Control/O2test3.txt', header=None, \n",
    "                       delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_10train = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/PD/P10train.txt', header=None, \n",
    "                        delim_whitespace=True)\n",
    "X_11train = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/PD/P11train.txt', header=None, \n",
    "                        delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtrain = np.concatenate([X_02train, X_03train, X_04train, X_10train, X_11train], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = np.array([X_02train.values, X_03train.values, X_04train.values, X_10train.values, X_11train.values])\n",
    "Ytrain = np.array([               0,                0,                0,                1,                1])\n",
    "# Ytrain = np.array([Controllllllllll, Controllllllllll, Controllllllllll, PDDDDDDDDDDDDDDD, PDDDDDDDDDDDDD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 81920, 56), (5,), dtype('float64'), dtype('int64'))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape, Ytrain.shape, Xtrain.dtype, Ytrain.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79872, 56), (1, 39, 8, 256, 56))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_02test1.shape, X_02test1.values.reshape([1, -1, 8, 256, 56]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79872, 56), (1, 39, 8, 256, 56))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_02test2.shape, X_02test2.values.reshape([1, -1, 8, 256, 56]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((81920, 56), (1, 40, 8, 256, 56))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_02test3.shape, X_02test3.values.reshape([1, -1, 8, 256, 56]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40960, 56), (1, 20, 8, 256, 56))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_02val1.shape, X_02val1.values.reshape([1, -1, 8, 256, 56]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40960, 56), (1, 20, 8, 256, 56))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_02val2.shape, X_02val2.values.reshape([1, -1, 8, 256, 56]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38912, 56), (1, 19, 8, 256, 56))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_02val3.shape, X_02val3.values.reshape([1, -1, 8, 256, 56]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 1, 81920, 56), (5,))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.reshape([5, 1, -1, 56]).shape, Ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_02train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_03train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79872, 56), (79872, 56), (81920, 56), (40960, 56), (40960, 56), (38912, 56))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_02train.shape, X_03train.shape, X_04train.shape, X_10train.shape, X_11train.shape, \\\n",
    "X_02test1.shape, X_02test2.shape, X_02test3.shape, X_02val1.shape, X_02val2.shape, X_02val3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_02train.values.dtype, X_03train.values.dtype, X_04train.values.dtype, X_10train.values.dtype, X_11train.values.dtype, \\\n",
    "X_02test1.values.dtype, X_02test2.values.dtype, X_02test3.values.dtype, X_02val1.values.dtype, \\\n",
    "X_02val2.values.dtype, X_02val3.values.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 81920, 56), (5,))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# five subjects, 8 activities, 10 seconds each, sr=256 samples/sec\n",
    "Xtrain.shape, Ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 40, 8, 256, 56)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 256 sampling rate: 256 samples per second\n",
    "Xtrain.reshape(5, 40, 8, 256, 56).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = Xtrain[0].shape\n",
    "input_size = (8, 256, 56) # one second of each activity which is 1024=256sample/sec*4sec\n",
    "output_size = 2 # num classes\n",
    "hidden_size = 56*2*2*2*2*2*2\n",
    "batch_size = 40\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size, output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(input_size, hidden_size, batch_size):\n",
    "    # CNN and MLP\n",
    "    inputs = tf.placeholder(dtype=tf.float32, shape=[None, *input_size], name='inputs')\n",
    "    labels = tf.placeholder(dtype=tf.int32, shape=[None], name='labels')    \n",
    "    # RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(num_units=hidden_size)\n",
    "    #cell = tf.nn.rnn_cell.LSTMCell(hidden_size)\n",
    "    cells = tf.nn.rnn_cell.MultiRNNCell(cells=[cell], state_is_tuple=True)\n",
    "    initial_state = cells.zero_state(batch_size, tf.float32)\n",
    "    # Batchnorm training\n",
    "    training = tf.placeholder(dtype=tf.bool, shape=[], name='training')\n",
    "    return inputs, labels, cells, initial_state, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs, labels, cell, initial_state = model_input(input_size=input_size, batch_size=batch_size, \n",
    "#                                                   hidden_size=hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs.shape, labels.shape, cell, initial_state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h1 = tf.layers.conv2d(inputs=inputs, filters=56*2, kernel_size=(1, 5), strides=(1, 2), padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h2 = tf.layers.conv2d(inputs=h1, filters=56*2*2, kernel_size=(1, 5), strides=(1, 2), padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h3 = tf.layers.conv2d(inputs=h2, filters=56*2*2*2, kernel_size=(1, 5), strides=(1, 2), padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NHWC format\n",
    "# h1 = tf.layers.conv2d(inputs=inputs, filters=56*2, kernel_size=[8, 3], strides=[1, 4], padding='same')\n",
    "# h2 = tf.layers.conv2d(inputs=h1, filters=56*2*2, kernel_size=[8, 3], strides=[1, 4], padding='same')\n",
    "# h3 = tf.layers.conv2d(inputs=h2, filters=56*2*2*2, kernel_size=[8, 3], strides=[1, 4], padding='same')\n",
    "# h4 = tf.layers.conv2d(inputs=h3, filters=56*2*2*2*2, kernel_size=[8, 3], strides=[1, 4], padding='same')\n",
    "# h5 = tf.layers.conv2d(inputs=h4, filters=56*2*2*2*2*2, kernel_size=[8, 3], strides=[8, 4], padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h1.shape, h2.shape, h3.shape, h4.shape, h5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(inputs, input_size, output_size, # CNN\n",
    "                  batch_size, initial_state, cells, # RNN/MLP/FC\n",
    "                  alpha=0.1, training=False, reuse=False): # NL/BN\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # CNN\n",
    "        # NHWC format\n",
    "        print(inputs.shape)\n",
    "        inputs_cnn = tf.reshape(inputs, [-1, *input_size]) # ?, 40, 8, 256, 56 -> ?, 8, 256, 56\n",
    "        print(inputs_cnn.shape)\n",
    "        h1 = tf.layers.conv2d(inputs=inputs_cnn, filters=56*2, kernel_size=[8, 3], strides=[1, 4], padding='same')\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)\n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        print(nl1.shape)\n",
    "        h2 = tf.layers.conv2d(inputs=nl1, filters=56*2*2, kernel_size=[8, 3], strides=[1, 4], padding='same')\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)\n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        print(nl2.shape)\n",
    "        h3 = tf.layers.conv2d(inputs=nl2, filters=56*2*2*2, kernel_size=[8, 3], strides=[1, 4], padding='same')\n",
    "        bn3 = tf.layers.batch_normalization(h3, training=training)\n",
    "        nl3 = tf.maximum(alpha * bn3, bn3)\n",
    "        print(nl3.shape)\n",
    "        h4 = tf.layers.conv2d(inputs=nl3, filters=56*2*2*2*2, kernel_size=[8, 3], strides=[1, 4], padding='same')\n",
    "        bn4 = tf.layers.batch_normalization(h4, training=training)\n",
    "        nl4 = tf.maximum(alpha * bn4, bn4)\n",
    "        print(nl4.shape)\n",
    "        h5 = tf.layers.conv2d(inputs=nl4, filters=56*2*2*2*2*2, kernel_size=[8, 3], strides=[8, 4], padding='same')        \n",
    "        bn5 = tf.layers.batch_normalization(h5, training=training)\n",
    "        nl5 = tf.maximum(alpha * bn5, bn5)\n",
    "        print(nl5.shape)\n",
    "\n",
    "        # RNN/MLP/Fully Connected\n",
    "        inputs_rnn = tf.reshape(tensor=nl5, shape=[-1, batch_size, 56*2*2*2*2*2]) # ?, 40, VEC\n",
    "        print(inputs_rnn.shape, initial_state)\n",
    "        # uni-directional/bi-directional\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cells, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state)\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, 56*2*2*2*2*2]) # ?x40, VEC\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Last fully connected layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=output_size)\n",
    "        print(logits.shape)\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(inputs, indices, input_size, output_size, hidden_size, batch_size, cells, initial_state, training):\n",
    "    logits, final_state = discriminator(inputs=inputs, input_size=input_size, output_size=output_size, \n",
    "                                        cells=cells, initial_state=initial_state, batch_size=batch_size, \n",
    "                                        training=training)\n",
    "    labels = tf.one_hot(indices=indices, depth=output_size, dtype=logits.dtype)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "    # If argmax label and argmax logits are equal: true/false\n",
    "    if_equal_bool = tf.equal(x=tf.argmax(axis=1, input=logits), y=tf.argmax(axis=1, input=labels))\n",
    "    accuracy = tf.reduce_mean(axis=0, input_tensor=tf.cast(dtype=tf.float32, x=if_equal_bool))\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    tvars = tf.trainable_variables()\n",
    "    gvars = [var for var in tvars if var.name.startswith('discriminator')]\n",
    "    # gvars = []\n",
    "    # for var in tvars: \n",
    "    #     if var.name.startswith('discriminator'):\n",
    "    #         gvars.append(var)\n",
    "\n",
    "    # Optimize MLP/CNN\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        #opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars) # CNN\n",
    "        #grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(loss, g_vars), clip_norm=5) # usually around 1-5\n",
    "        grads = tf.gradients(loss, gvars) # RNN\n",
    "        opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(grads, gvars))\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, input_size, output_size, hidden_size, batch_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.inputs, self.indices, cells, self.initial_state, self.training = model_input(input_size=input_size, \n",
    "                                                                                         batch_size=batch_size, \n",
    "                                                                                         hidden_size=hidden_size)\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.loss, self.acc = model_loss(inputs=self.inputs, indices=self.indices, batch_size=batch_size, \n",
    "                                         input_size=input_size, output_size=output_size, hidden_size=hidden_size,\n",
    "                                         cells=cells, initial_state=self.initial_state, training=self.training)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_batches(X, Y, batch_size):\n",
    "#     \"\"\" Return a generator for batches \"\"\"\n",
    "#     n_batches = len(X) // batch_size\n",
    "#     X, Y = X[:n_batches*batch_size], Y[:n_batches*batch_size]\n",
    "\n",
    "#     # Loop over batches and yield\n",
    "#     for b in range(0, len(X), batch_size):\n",
    "#         yield X[b:b+batch_size], Y[b:b+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Network Hyper-parameters\n",
    "\n",
    "# # Number of classes Output layer or target labels\n",
    "# assert Ytrain.max()==Ytest.max()==Yvalid.max(), 'Number of output classes is the same in training, validation and testing data.'\n",
    "\n",
    "# # Hidden layer\n",
    "# input_size = [Xvalid.shape[1], Xvalid.shape[2]]\n",
    "# hidden_size = Xvalid.shape[1]* Xvalid.shape[2]\n",
    "# output_size = Yvalid.max()\n",
    "\n",
    "# # learning parameters\n",
    "# batch_size = Xvalid.shape[0]//1  # experience mini-batch size\n",
    "# train_epochs = 1000              # max number of training episodes/epochs\n",
    "# learning_rate = 0.001            # learning rate for training/optimization/adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Yvalid.max(), Yvalid.min():', Yvalid.max(), Yvalid.min())\n",
    "# print('Yvalid.shape:', Yvalid.shape)\n",
    "# print('Xvalid.shape:', Xvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 8, 256, 56)\n",
      "(?, 8, 256, 56)\n",
      "(?, 8, 64, 112)\n",
      "(?, 8, 16, 224)\n",
      "(?, 8, 4, 448)\n",
      "(?, 8, 1, 896)\n",
      "(?, 1, 1, 1792)\n",
      "(?, 40, 1792) (<tf.Tensor 'MultiRNNCellZeroState/GRUCellZeroState/zeros:0' shape=(40, 3584) dtype=float32>,)\n",
      "(40, 40, 3584) (<tf.Tensor 'discriminator/rnn/while/Exit_3:0' shape=(40, 3584) dtype=float32>,)\n",
      "(3200, 1792)\n",
      "(3200, 2)\n"
     ]
    }
   ],
   "source": [
    "graph = tf.reset_default_graph()\n",
    "\n",
    "model = Model(input_size=input_size, hidden_size=hidden_size, output_size=output_size, \n",
    "              learning_rate=learning_rate, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 train_loss: 7.688725 train_acc: 0.2269991\n",
      "epoch: 1 valid_loss: 16.188286 valid_acc: 0.23715712\n",
      "epoch: 2 train_loss: 12.676353 train_acc: 0.2563445\n",
      "epoch: 2 valid_loss: 8.694114 valid_acc: 0.31411177\n",
      "epoch: 3 train_loss: 7.5286446 train_acc: 0.33651412\n",
      "epoch: 3 valid_loss: 5.4588995 valid_acc: 0.28032014\n",
      "epoch: 4 train_loss: 4.7111692 train_acc: 0.29109377\n",
      "epoch: 4 valid_loss: 3.5290227 valid_acc: 0.3791641\n",
      "epoch: 5 train_loss: 3.034183 train_acc: 0.4003352\n",
      "epoch: 5 valid_loss: 1.8962796 valid_acc: 0.3557699\n",
      "epoch: 6 train_loss: 1.6121278 train_acc: 0.3659621\n",
      "epoch: 6 valid_loss: 1.8903593 valid_acc: 0.3955811\n",
      "epoch: 7 train_loss: 2.0607088 train_acc: 0.35854027\n",
      "epoch: 7 valid_loss: 2.168816 valid_acc: 0.30234626\n",
      "epoch: 8 train_loss: 2.0824676 train_acc: 0.30692935\n",
      "epoch: 8 valid_loss: 1.5942453 valid_acc: 0.4086463\n",
      "epoch: 9 train_loss: 1.4510515 train_acc: 0.40820166\n",
      "epoch: 9 valid_loss: 1.1627387 valid_acc: 0.4486627\n",
      "epoch: 10 train_loss: 1.1163874 train_acc: 0.45037282\n",
      "epoch: 10 valid_loss: 1.0688372 valid_acc: 0.46131745\n",
      "epoch: 11 train_loss: 1.0201076 train_acc: 0.45427185\n",
      "epoch: 11 valid_loss: 1.0420313 valid_acc: 0.44264314\n",
      "epoch: 12 train_loss: 1.0575744 train_acc: 0.4153841\n",
      "epoch: 12 valid_loss: 1.0225041 valid_acc: 0.4733566\n",
      "epoch: 13 train_loss: 0.9590154 train_acc: 0.46901292\n",
      "epoch: 13 valid_loss: 0.8311536 valid_acc: 0.47622958\n",
      "epoch: 14 train_loss: 0.83075374 train_acc: 0.5042069\n",
      "epoch: 14 valid_loss: 0.89025265 valid_acc: 0.5053013\n",
      "epoch: 15 train_loss: 0.8962368 train_acc: 0.5021889\n",
      "epoch: 15 valid_loss: 0.84114915 valid_acc: 0.5518161\n",
      "epoch: 16 train_loss: 0.8030379 train_acc: 0.54412067\n",
      "epoch: 16 valid_loss: 0.70492876 valid_acc: 0.5685067\n",
      "epoch: 17 train_loss: 0.6958271 train_acc: 0.5689856\n",
      "epoch: 17 valid_loss: 0.67141235 valid_acc: 0.6041453\n",
      "epoch: 18 train_loss: 0.6632615 train_acc: 0.58020383\n",
      "epoch: 18 valid_loss: 0.6625925 valid_acc: 0.56570214\n",
      "epoch: 19 train_loss: 0.6566901 train_acc: 0.577023\n",
      "epoch: 19 valid_loss: 0.6607353 valid_acc: 0.6011355\n",
      "epoch: 20 train_loss: 0.64443314 train_acc: 0.60691565\n",
      "epoch: 20 valid_loss: 0.622964 valid_acc: 0.62822354\n",
      "epoch: 21 train_loss: 0.6194409 train_acc: 0.6219988\n",
      "epoch: 21 valid_loss: 0.6292451 valid_acc: 0.6014775\n",
      "epoch: 22 train_loss: 0.6200876 train_acc: 0.6134825\n",
      "epoch: 22 valid_loss: 0.5902853 valid_acc: 0.6343115\n",
      "epoch: 23 train_loss: 0.57934356 train_acc: 0.63376427\n",
      "epoch: 23 valid_loss: 0.5613397 valid_acc: 0.6790478\n",
      "epoch: 24 train_loss: 0.55400455 train_acc: 0.68544364\n",
      "epoch: 24 valid_loss: 0.5401608 valid_acc: 0.66338325\n",
      "epoch: 25 train_loss: 0.53340435 train_acc: 0.6760038\n",
      "epoch: 25 valid_loss: 0.53681517 valid_acc: 0.668924\n",
      "epoch: 26 train_loss: 0.53443193 train_acc: 0.66820574\n",
      "epoch: 26 valid_loss: 0.5381879 valid_acc: 0.64190435\n",
      "epoch: 27 train_loss: 0.53353465 train_acc: 0.6514809\n",
      "epoch: 27 valid_loss: 0.53699327 valid_acc: 0.66181\n",
      "epoch: 28 train_loss: 0.53209436 train_acc: 0.66009986\n",
      "epoch: 28 valid_loss: 0.53236794 valid_acc: 0.6672139\n",
      "epoch: 29 train_loss: 0.520653 train_acc: 0.6748409\n",
      "epoch: 29 valid_loss: 0.50723666 valid_acc: 0.675012\n",
      "epoch: 30 train_loss: 0.5009097 train_acc: 0.672139\n",
      "epoch: 30 valid_loss: 0.49115047 valid_acc: 0.66974485\n",
      "epoch: 31 train_loss: 0.48464411 train_acc: 0.6725152\n",
      "epoch: 31 valid_loss: 0.47944036 valid_acc: 0.6823312\n",
      "epoch: 32 train_loss: 0.47188488 train_acc: 0.6905397\n",
      "epoch: 32 valid_loss: 0.47367615 valid_acc: 0.69922704\n",
      "epoch: 33 train_loss: 0.46923625 train_acc: 0.7001505\n",
      "epoch: 33 valid_loss: 0.48176047 valid_acc: 0.6926602\n",
      "epoch: 34 train_loss: 0.47660026 train_acc: 0.688727\n",
      "epoch: 34 valid_loss: 0.48618898 valid_acc: 0.6798003\n",
      "epoch: 35 train_loss: 0.47679138 train_acc: 0.68174976\n",
      "epoch: 35 valid_loss: 0.4740586 valid_acc: 0.6868459\n",
      "epoch: 36 train_loss: 0.46056628 train_acc: 0.697688\n",
      "epoch: 36 valid_loss: 0.44871327 valid_acc: 0.71940625\n",
      "epoch: 37 train_loss: 0.43657553 train_acc: 0.72395515\n",
      "epoch: 37 valid_loss: 0.43228927 valid_acc: 0.728846\n",
      "epoch: 38 train_loss: 0.42354798 train_acc: 0.73373693\n",
      "epoch: 38 valid_loss: 0.4317119 valid_acc: 0.7265203\n",
      "epoch: 39 train_loss: 0.4252742 train_acc: 0.72723854\n",
      "epoch: 39 valid_loss: 0.4414016 valid_acc: 0.718859\n",
      "epoch: 40 train_loss: 0.43346572 train_acc: 0.7193036\n",
      "epoch: 40 valid_loss: 0.44154835 valid_acc: 0.71489155\n",
      "epoch: 41 train_loss: 0.4314239 train_acc: 0.71937203\n",
      "epoch: 41 valid_loss: 0.43163866 valid_acc: 0.7230317\n",
      "epoch: 42 train_loss: 0.41959894 train_acc: 0.72306585\n",
      "epoch: 42 valid_loss: 0.42002374 valid_acc: 0.7251522\n",
      "epoch: 43 train_loss: 0.412022 train_acc: 0.72142416\n",
      "epoch: 43 valid_loss: 0.42108193 valid_acc: 0.7168753\n",
      "epoch: 44 train_loss: 0.41329128 train_acc: 0.71930367\n",
      "epoch: 44 valid_loss: 0.41249672 valid_acc: 0.7215268\n",
      "epoch: 45 train_loss: 0.40055972 train_acc: 0.7280594\n",
      "epoch: 45 valid_loss: 0.39604953 valid_acc: 0.73308706\n",
      "epoch: 46 train_loss: 0.3869254 train_acc: 0.7337027\n",
      "epoch: 46 valid_loss: 0.39230663 valid_acc: 0.73390794\n",
      "epoch: 47 train_loss: 0.39042166 train_acc: 0.74030375\n",
      "epoch: 47 valid_loss: 0.39161763 valid_acc: 0.7330187\n",
      "epoch: 48 train_loss: 0.37612432 train_acc: 0.7380464\n",
      "epoch: 48 valid_loss: 0.37538576 valid_acc: 0.74293727\n",
      "epoch: 49 train_loss: 0.37618703 train_acc: 0.73028255\n",
      "epoch: 49 valid_loss: 0.35700652 valid_acc: 0.74307406\n",
      "epoch: 50 train_loss: 0.34382278 train_acc: 0.73777276\n",
      "epoch: 50 valid_loss: 0.36418486 valid_acc: 0.74594706\n",
      "epoch: 51 train_loss: 0.3670373 train_acc: 0.75094056\n",
      "epoch: 51 valid_loss: 0.3849352 valid_acc: 0.7268623\n",
      "epoch: 52 train_loss: 0.3543143 train_acc: 0.7348656\n",
      "epoch: 52 valid_loss: 0.34507903 valid_acc: 0.7618852\n",
      "epoch: 53 train_loss: 0.34170777 train_acc: 0.7586702\n",
      "epoch: 53 valid_loss: 0.36802986 valid_acc: 0.7321294\n",
      "epoch: 54 train_loss: 0.37140667 train_acc: 0.74362135\n",
      "epoch: 54 valid_loss: 0.40649456 valid_acc: 0.7252206\n",
      "epoch: 55 train_loss: 0.37997845 train_acc: 0.73106915\n",
      "epoch: 55 valid_loss: 0.37861434 valid_acc: 0.7280252\n",
      "epoch: 56 train_loss: 0.3447299 train_acc: 0.75849926\n",
      "epoch: 56 valid_loss: 0.35444483 valid_acc: 0.7833641\n",
      "epoch: 57 train_loss: 0.38244545 train_acc: 0.7396197\n",
      "epoch: 57 valid_loss: 0.32412225 valid_acc: 0.7601067\n",
      "epoch: 58 train_loss: 0.33131754 train_acc: 0.74389493\n",
      "epoch: 58 valid_loss: 0.41095886 valid_acc: 0.7399275\n",
      "epoch: 59 train_loss: 0.41294372 train_acc: 0.7408167\n",
      "epoch: 59 valid_loss: 0.38144723 valid_acc: 0.7343184\n",
      "epoch: 60 train_loss: 0.43813533 train_acc: 0.695157\n",
      "epoch: 60 valid_loss: 0.46880195 valid_acc: 0.6636569\n",
      "epoch: 61 train_loss: 0.42805934 train_acc: 0.7084616\n",
      "epoch: 61 valid_loss: 0.43943968 valid_acc: 0.705315\n",
      "epoch: 62 train_loss: 0.40416747 train_acc: 0.71157396\n",
      "epoch: 62 valid_loss: 0.40585718 valid_acc: 0.7575073\n",
      "epoch: 63 train_loss: 0.4538589 train_acc: 0.7474861\n",
      "epoch: 63 valid_loss: 0.51839036 valid_acc: 0.6391682\n",
      "epoch: 64 train_loss: 0.4353636 train_acc: 0.68188655\n",
      "epoch: 64 valid_loss: 0.6893609 valid_acc: 0.73089814\n",
      "epoch: 65 train_loss: 0.5837959 train_acc: 0.72310007\n",
      "epoch: 65 valid_loss: 0.6395315 valid_acc: 0.7430057\n",
      "epoch: 66 train_loss: 0.56158847 train_acc: 0.72816193\n",
      "epoch: 66 valid_loss: 0.6433017 valid_acc: 0.6655722\n",
      "epoch: 67 train_loss: 0.53720176 train_acc: 0.68373346\n",
      "epoch: 67 valid_loss: 0.4488582 valid_acc: 0.6991586\n",
      "epoch: 68 train_loss: 0.38719058 train_acc: 0.72785413\n",
      "epoch: 68 valid_loss: 0.62563086 valid_acc: 0.64942884\n",
      "epoch: 69 train_loss: 0.64559555 train_acc: 0.68520415\n",
      "epoch: 69 valid_loss: 0.52407974 valid_acc: 0.704631\n",
      "epoch: 70 train_loss: 0.49323493 train_acc: 0.70442575\n",
      "epoch: 70 valid_loss: 0.7409573 valid_acc: 0.71769613\n",
      "epoch: 71 train_loss: 0.72974956 train_acc: 0.668924\n",
      "epoch: 71 valid_loss: 0.64512366 valid_acc: 0.66988164\n",
      "epoch: 72 train_loss: 0.5333598 train_acc: 0.708359\n",
      "epoch: 72 valid_loss: 0.7585966 valid_acc: 0.65955263\n",
      "epoch: 73 train_loss: 0.78364 train_acc: 0.64282787\n",
      "epoch: 73 valid_loss: 1.0358253 valid_acc: 0.58574456\n",
      "epoch: 74 train_loss: 0.79849136 train_acc: 0.63140434\n",
      "epoch: 74 valid_loss: 0.83343184 valid_acc: 0.5951843\n",
      "epoch: 75 train_loss: 0.70712006 train_acc: 0.61888635\n",
      "epoch: 75 valid_loss: 0.55538124 valid_acc: 0.6555168\n",
      "epoch: 76 train_loss: 0.547967 train_acc: 0.65828717\n",
      "epoch: 76 valid_loss: 0.6199413 valid_acc: 0.69307065\n",
      "epoch: 77 train_loss: 0.6321912 train_acc: 0.6946782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 77 valid_loss: 0.55148774 valid_acc: 0.6766537\n",
      "epoch: 78 train_loss: 0.6849936 train_acc: 0.66988164\n",
      "epoch: 78 valid_loss: 1.2358253 valid_acc: 0.6388262\n",
      "epoch: 79 train_loss: 0.9872297 train_acc: 0.6188179\n",
      "epoch: 79 valid_loss: 0.959475 valid_acc: 0.6060606\n",
      "epoch: 80 train_loss: 1.2790678 train_acc: 0.5893016\n",
      "epoch: 80 valid_loss: 2.057887 valid_acc: 0.5746631\n",
      "epoch: 81 train_loss: 1.5577369 train_acc: 0.58222175\n",
      "epoch: 81 valid_loss: 1.1898404 valid_acc: 0.5579041\n",
      "epoch: 82 train_loss: 1.0625516 train_acc: 0.5848553\n",
      "epoch: 82 valid_loss: 0.87314427 valid_acc: 0.5312265\n",
      "epoch: 83 train_loss: 0.86881626 train_acc: 0.5790752\n",
      "epoch: 83 valid_loss: 0.6721019 valid_acc: 0.6366373\n",
      "epoch: 84 train_loss: 0.76674527 train_acc: 0.64508516\n",
      "epoch: 84 valid_loss: 0.51257014 valid_acc: 0.67740613\n",
      "epoch: 85 train_loss: 0.5729754 train_acc: 0.67289144\n",
      "epoch: 85 valid_loss: 0.70896477 valid_acc: 0.6455298\n",
      "epoch: 86 train_loss: 0.5685186 train_acc: 0.7157124\n",
      "epoch: 86 valid_loss: 0.8813826 valid_acc: 0.72104794\n",
      "epoch: 87 train_loss: 0.8690635 train_acc: 0.6950886\n",
      "epoch: 87 valid_loss: 0.62666035 valid_acc: 0.6840413\n",
      "epoch: 88 train_loss: 0.57105136 train_acc: 0.6784664\n",
      "epoch: 88 valid_loss: 0.9612241 valid_acc: 0.6456666\n",
      "epoch: 89 train_loss: 0.82121384 train_acc: 0.645735\n",
      "epoch: 89 valid_loss: 0.5283289 valid_acc: 0.6401943\n",
      "epoch: 90 train_loss: 0.58989596 train_acc: 0.6371161\n",
      "epoch: 90 valid_loss: 0.47052985 valid_acc: 0.7319926\n",
      "epoch: 91 train_loss: 0.51272774 train_acc: 0.70476776\n",
      "epoch: 91 valid_loss: 0.90056777 valid_acc: 0.6462138\n",
      "epoch: 92 train_loss: 0.87999415 train_acc: 0.6520966\n",
      "epoch: 92 valid_loss: 0.57646364 valid_acc: 0.7891785\n",
      "epoch: 93 train_loss: 0.63544846 train_acc: 0.7825775\n",
      "epoch: 93 valid_loss: 1.114109 valid_acc: 0.6558588\n",
      "epoch: 94 train_loss: 1.0724711 train_acc: 0.625932\n",
      "epoch: 94 valid_loss: 0.6040471 valid_acc: 0.68623024\n",
      "epoch: 95 train_loss: 0.78052294 train_acc: 0.6725836\n",
      "epoch: 95 valid_loss: 1.415177 valid_acc: 0.6303441\n",
      "epoch: 96 train_loss: 1.1037561 train_acc: 0.63738966\n",
      "epoch: 96 valid_loss: 0.611894 valid_acc: 0.69190776\n",
      "epoch: 97 train_loss: 0.53224343 train_acc: 0.7131815\n",
      "epoch: 97 valid_loss: 0.53344595 valid_acc: 0.7483412\n",
      "epoch: 98 train_loss: 0.67987186 train_acc: 0.71277106\n",
      "epoch: 98 valid_loss: 0.8117752 valid_acc: 0.6937547\n",
      "epoch: 99 train_loss: 0.81653625 train_acc: 0.7267939\n",
      "epoch: 99 valid_loss: 0.6840636 valid_acc: 0.7129763\n",
      "epoch: 100 train_loss: 1.0151424 train_acc: 0.6590054\n",
      "epoch: 100 valid_loss: 0.7960923 valid_acc: 0.66659826\n",
      "epoch: 101 train_loss: 0.6368003 train_acc: 0.69693553\n",
      "epoch: 101 valid_loss: 0.8427948 valid_acc: 0.7222108\n",
      "epoch: 102 train_loss: 0.9691807 train_acc: 0.71725154\n",
      "epoch: 102 valid_loss: 1.1389225 valid_acc: 0.622546\n",
      "epoch: 103 train_loss: 1.0557246 train_acc: 0.6395444\n",
      "epoch: 103 valid_loss: 0.91921496 valid_acc: 0.71858543\n",
      "epoch: 104 train_loss: 0.85607845 train_acc: 0.70271564\n",
      "epoch: 104 valid_loss: 0.9248829 valid_acc: 0.715302\n",
      "epoch: 105 train_loss: 0.67471087 train_acc: 0.70842737\n",
      "epoch: 105 valid_loss: 1.0591375 valid_acc: 0.68171555\n"
     ]
    }
   ],
   "source": [
    "# We should save the after training and validation\n",
    "saver = tf.train.Saver() \n",
    "\n",
    "# Loss and accuracy of the model for training and validation\n",
    "train_loss_mean, valid_loss_mean = [], []\n",
    "train_acc_mean, valid_acc_mean = [], []\n",
    "\n",
    "# now that we can calculate loss and optimize, we can start a session for calculating the error.\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize all the model parameters/variables\n",
    "    sess.run(fetches=tf.global_variables_initializer())\n",
    "    \n",
    "    #     # Restoring/loading/uploading the trained and validated model\n",
    "    #     saver.restore(sess,'checkpoints/mlp-fnirs-har.ckpt')\n",
    "    \n",
    "    # for every epoch start feeding the arrays into the tensors in the model\n",
    "    for epoch in range(train_epochs):\n",
    "        train_loss, valid_loss = [], []\n",
    "        train_acc, valid_acc = [], []\n",
    "        \n",
    "        # Training\n",
    "        for Xinputs, Yindices in get_batches(X=Xtrain, Y=Ytrain, batch_size=batch_size):\n",
    "            feed_dict = {model.Xinputs: Xinputs, model.Yindices: Yindices}\n",
    "            loss, acc, _ = sess.run(fetches=[model.loss, model.acc, model.opt], feed_dict=feed_dict)\n",
    "            train_loss.append(loss)\n",
    "            train_acc.append(acc)\n",
    "            \n",
    "        # printing out train and validation loss\n",
    "        print('epoch:', epoch+1, 'train_loss:', np.mean(train_loss), 'train_acc:', np.mean(train_acc))\n",
    "        \n",
    "        # Saving the losses for plotting\n",
    "        train_loss_mean.append(np.mean(train_loss))\n",
    "        train_acc_mean.append(np.mean(train_acc))\n",
    "        \n",
    "        # Validation\n",
    "        for Xinputs, Yindices in get_batches(X=Xvalid, Y=Yvalid, batch_size=batch_size):\n",
    "            feed_dict = {model.Xinputs: Xinputs, model.Yindices: Yindices}\n",
    "            loss, acc = sess.run(fetches=[model.loss, model.acc], feed_dict=feed_dict)\n",
    "            valid_loss.append(loss)\n",
    "            valid_acc.append(acc)\n",
    "        \n",
    "        # printing out train and validation loss\n",
    "        print('epoch:', epoch+1, 'valid_loss:', np.mean(valid_loss), 'valid_acc:', np.mean(valid_acc))\n",
    "\n",
    "        # Saving the losses for plotting\n",
    "        valid_loss_mean.append(np.mean(valid_loss))\n",
    "        valid_acc_mean.append(np.mean(valid_acc))        \n",
    "    \n",
    "    # Saving the trained and validated model\n",
    "    saver.save(sess,'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as mplot\n",
    "%matplotlib inline\n",
    "\n",
    "mplot.plot(train_loss_mean, label='train_loss_mean')\n",
    "mplot.plot(valid_loss_mean, label='valid_loss_mean')\n",
    "mplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mplot.plot(train_acc_mean, label='train_acc_mean')\n",
    "mplot.plot(valid_acc_mean, label='valid_acc_mean')\n",
    "mplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #sess.run(fetches=tf.global_variables_initializer())\n",
    "    \n",
    "    # Restoring/loading/uploading the trained and validated model\n",
    "    saver.restore(sess,'checkpoints/model.ckpt')\n",
    "    \n",
    "    # Saving the test loss for every batch/minibtch\n",
    "    test_loss, test_acc = [], []\n",
    "    \n",
    "    # Testing\n",
    "    for Xinputs, Yindices in get_batches(X=Xtest, Y=Ytest, batch_size=batch_size):\n",
    "        feed_dict = {model.Xinputs: Xinputs, model.Yindices: Yindices}\n",
    "        loss, acc = sess.run(fetches=[model.loss, model.acc], feed_dict=feed_dict)\n",
    "        test_loss.append(loss)\n",
    "        test_acc.append(acc)\n",
    "        \n",
    "    # Printing the test loss\n",
    "    print('test_loss:', np.mean(test_loss), 'test acc', np.mean(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFO:tensorflow:Restoring parameters from checkpoints/mlp-fnirs-har.ckpt\n",
    "# test_loss: 0.16514638 test acc 0.807716"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
