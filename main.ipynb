{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAR (human activity recognition) using DNN/DL on fNIRS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mP11-4-17-2018\u001b[0m/  \u001b[01;34mP13-4-17-2018\u001b[0m/  \u001b[01;34mP15-4-18-2018\u001b[0m/  \u001b[01;34mP17-4-18-2018\u001b[0m/  \u001b[01;34mP19-4-19-2018\u001b[0m/\r\n",
      "\u001b[01;34mP12-4-17-2018\u001b[0m/  \u001b[01;34mP14-4-18-2018\u001b[0m/  \u001b[01;34mP16-4-18-2018\u001b[0m/  \u001b[01;34mP18-4-19-2018\u001b[0m/  \u001b[01;34mP20-4-19-2018\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "% ls ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m1. Right Hand\u001b[0m/  \u001b[01;34m2. Both Hands\u001b[0m/  \u001b[01;34m3. Left Hand\u001b[0m/  \u001b[01;34m4. Right Leg\u001b[0m/  \u001b[01;34m5. Left Leg\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "% ls ./data/P12-4-17-2018/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fNIR_data.txt\r\n",
      "head20180417-145130.txt\r\n",
      "NIRS-2018-04-17_006_deoxyhb_T141to2511_C1to20.txt\r\n",
      "NIRS-2018-04-17_006_oxyhb_T141to2511_C1to20.txt\r\n",
      "r_hand20180417-145128.txt\r\n",
      "r_lower_arm20180417-145129.txt\r\n",
      "r_upper_arm20180417-145129.txt\r\n"
     ]
    }
   ],
   "source": [
    "% ls ./data/P12-4-17-2018/1.\\ Right\\ Hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['./data/P11-4-17-2018/1. Right Hand/fNIR_data.txt',\n",
       "  './data/P11-4-17-2018/2. Both Hands/fNIR_data.txt',\n",
       "  './data/P11-4-17-2018/3. Left hand/fNIR_data.txt',\n",
       "  './data/P11-4-17-2018/5. Left Leg/fNIR_data.txt',\n",
       "  './data/P12-4-17-2018/1. Right Hand/fNIR_data.txt',\n",
       "  './data/P12-4-17-2018/2. Both Hands/fNIR_data.txt',\n",
       "  './data/P12-4-17-2018/3. Left Hand/fNIR_data.txt',\n",
       "  './data/P12-4-17-2018/4. Right Leg/fNIR_data.txt',\n",
       "  './data/P12-4-17-2018/5. Left Leg/fNIR_data.txt',\n",
       "  './data/P13-4-17-2018/1. Right Hand/fNIR_data.txt',\n",
       "  './data/P13-4-17-2018/2. Both Hands/fNIR_data.txt',\n",
       "  './data/P13-4-17-2018/3. Left Hand/fNIR_data.txt',\n",
       "  './data/P13-4-17-2018/4. Right Leg/fNIR_data.txt',\n",
       "  './data/P13-4-17-2018/5. Left Leg/fNIR_data.txt',\n",
       "  './data/P14-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  './data/P14-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  './data/P14-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  './data/P14-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  './data/P14-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       "  './data/P15-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  './data/P15-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  './data/P15-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  './data/P15-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  './data/P15-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       "  './data/P16-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  './data/P16-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  './data/P16-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  './data/P16-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  './data/P16-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       "  './data/P17-4-18-2018/1. Right Hand/fNIR_data.txt',\n",
       "  './data/P17-4-18-2018/2. Both Hands/fNIR_data.txt',\n",
       "  './data/P17-4-18-2018/3. Left Hand/fNIR_data.txt',\n",
       "  './data/P17-4-18-2018/4. Right Leg/fNIR_data.txt',\n",
       "  './data/P17-4-18-2018/5. Left Leg/fNIR_data.txt',\n",
       "  './data/P18-4-19-2018/1. Right Hand/fNIR_data.txt',\n",
       "  './data/P18-4-19-2018/2. Both Hands/fNIR_data.txt',\n",
       "  './data/P18-4-19-2018/3. Left Hand/fNIR_data.txt',\n",
       "  './data/P18-4-19-2018/4. Right Leg/fNIR_data.txt',\n",
       "  './data/P18-4-19-2018/5. Left Leg/fNIR_data.txt',\n",
       "  './data/P19-4-19-2018/1. Right Hand/fNIR_data.txt',\n",
       "  './data/P19-4-19-2018/2. Both Hands/fNIR_data.txt',\n",
       "  './data/P19-4-19-2018/3. Left Hand/fNIR_data.txt',\n",
       "  './data/P19-4-19-2018/4. Right Leg/fNIR_data.txt',\n",
       "  './data/P19-4-19-2018/5. Left Leg/fNIR_data.txt',\n",
       "  './data/P20-4-19-2018/1. Right Hand/fNIR_data.txt',\n",
       "  './data/P20-4-19-2018/2. Both Hands/fNIR_data.txt',\n",
       "  './data/P20-4-19-2018/3. Left Hand/fNIR_data.txt',\n",
       "  './data/P20-4-19-2018/5. Left Leg/fNIR_data.txt'],\n",
       " 48)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# % find ../../datasets/fNIRs_data/ | grep fNIR_data # NOT WORKING!!\n",
    "def find_all(name, path):\n",
    "    result = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if name in files:\n",
    "            result.append(os.path.join(root, name))\n",
    "    return result\n",
    "\n",
    "allpaths = find_all(name='fNIR_data.txt', path='./data/')\n",
    "allpaths = sorted(allpaths, reverse=False)\n",
    "# print(allpaths, len(allpaths))\n",
    "allpaths, len(allpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/pandas/io/parsers.py:678: UserWarning: Duplicate names specified. This will raise an error in the future.\n",
      "  return _read(filepath_or_buffer, kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2388, 42) ./data/P11-4-17-2018/1. Right Hand/fNIR_data.txt\n",
      "(1244, 42) ./data/P11-4-17-2018/2. Both Hands/fNIR_data.txt\n",
      "(2389, 42) ./data/P11-4-17-2018/3. Left hand/fNIR_data.txt\n",
      "(1201, 42) ./data/P11-4-17-2018/5. Left Leg/fNIR_data.txt\n",
      "(2372, 42) ./data/P12-4-17-2018/1. Right Hand/fNIR_data.txt\n",
      "(1210, 42) ./data/P12-4-17-2018/2. Both Hands/fNIR_data.txt\n",
      "(2378, 42) ./data/P12-4-17-2018/3. Left Hand/fNIR_data.txt\n",
      "(1202, 42) ./data/P12-4-17-2018/4. Right Leg/fNIR_data.txt\n",
      "(1222, 42) ./data/P12-4-17-2018/5. Left Leg/fNIR_data.txt\n",
      "(2405, 42) ./data/P13-4-17-2018/1. Right Hand/fNIR_data.txt\n",
      "(1196, 42) ./data/P13-4-17-2018/2. Both Hands/fNIR_data.txt\n",
      "(2380, 42) ./data/P13-4-17-2018/3. Left Hand/fNIR_data.txt\n",
      "(1203, 42) ./data/P13-4-17-2018/4. Right Leg/fNIR_data.txt\n",
      "(1242, 42) ./data/P13-4-17-2018/5. Left Leg/fNIR_data.txt\n",
      "(2373, 42) ./data/P14-4-18-2018/1. Right Hand/fNIR_data.txt\n",
      "(1202, 42) ./data/P14-4-18-2018/2. Both Hands/fNIR_data.txt\n",
      "(2386, 42) ./data/P14-4-18-2018/3. Left Hand/fNIR_data.txt\n",
      "(1196, 42) ./data/P14-4-18-2018/4. Right Leg/fNIR_data.txt\n",
      "(1229, 42) ./data/P14-4-18-2018/5. Left Leg/fNIR_data.txt\n",
      "(2387, 42) ./data/P15-4-18-2018/1. Right Hand/fNIR_data.txt\n",
      "(1224, 42) ./data/P15-4-18-2018/2. Both Hands/fNIR_data.txt\n",
      "(2379, 42) ./data/P15-4-18-2018/3. Left Hand/fNIR_data.txt\n",
      "(1230, 42) ./data/P15-4-18-2018/4. Right Leg/fNIR_data.txt\n",
      "(1227, 42) ./data/P15-4-18-2018/5. Left Leg/fNIR_data.txt\n",
      "(2384, 42) ./data/P16-4-18-2018/1. Right Hand/fNIR_data.txt\n",
      "(1230, 42) ./data/P16-4-18-2018/2. Both Hands/fNIR_data.txt\n",
      "(2375, 42) ./data/P16-4-18-2018/3. Left Hand/fNIR_data.txt\n",
      "(1196, 42) ./data/P16-4-18-2018/4. Right Leg/fNIR_data.txt\n",
      "(1197, 42) ./data/P16-4-18-2018/5. Left Leg/fNIR_data.txt\n",
      "(2373, 42) ./data/P17-4-18-2018/1. Right Hand/fNIR_data.txt\n",
      "(1220, 42) ./data/P17-4-18-2018/2. Both Hands/fNIR_data.txt\n",
      "(2372, 42) ./data/P17-4-18-2018/3. Left Hand/fNIR_data.txt\n",
      "(1223, 42) ./data/P17-4-18-2018/4. Right Leg/fNIR_data.txt\n",
      "(1222, 42) ./data/P17-4-18-2018/5. Left Leg/fNIR_data.txt\n",
      "(2395, 42) ./data/P18-4-19-2018/1. Right Hand/fNIR_data.txt\n",
      "(1201, 42) ./data/P18-4-19-2018/2. Both Hands/fNIR_data.txt\n",
      "(2379, 42) ./data/P18-4-19-2018/3. Left Hand/fNIR_data.txt\n",
      "(1197, 42) ./data/P18-4-19-2018/4. Right Leg/fNIR_data.txt\n",
      "(1196, 42) ./data/P18-4-19-2018/5. Left Leg/fNIR_data.txt\n",
      "(2372, 42) ./data/P19-4-19-2018/1. Right Hand/fNIR_data.txt\n",
      "(1194, 42) ./data/P19-4-19-2018/2. Both Hands/fNIR_data.txt\n",
      "(2383, 42) ./data/P19-4-19-2018/3. Left Hand/fNIR_data.txt\n",
      "(1196, 42) ./data/P19-4-19-2018/4. Right Leg/fNIR_data.txt\n",
      "(1227, 42) ./data/P19-4-19-2018/5. Left Leg/fNIR_data.txt\n",
      "(2381, 42) ./data/P20-4-19-2018/1. Right Hand/fNIR_data.txt\n",
      "(1198, 42) ./data/P20-4-19-2018/2. Both Hands/fNIR_data.txt\n",
      "(2431, 42) ./data/P20-4-19-2018/3. Left Hand/fNIR_data.txt\n",
      "(1208, 42) ./data/P20-4-19-2018/5. Left Leg/fNIR_data.txt\n",
      "Number of input files: 48\n"
     ]
    }
   ],
   "source": [
    "# df: data frame object\n",
    "df = []\n",
    "for each_idx in range(len(allpaths)):\n",
    "    file = pd.read_csv(filepath_or_buffer=allpaths[each_idx], names=['time', 'sample', \n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel',\n",
    "                       'channel', 'channel', 'channel', 'channel', 'channel'],\n",
    "                         header=None)\n",
    "    df.append(file)\n",
    "    \n",
    "for each in range(len(df)):\n",
    "    print(df[each].shape, allpaths[each])\n",
    "    df[each]=df[each].drop(axis=1, columns=None, index=None, labels=['time', 'sample'])\n",
    "    df[each] = df[each].dropna()\n",
    "    df[each]['channel.39'] = df[each]['channel.39'].astype(str).str[1:-1].astype(float)\n",
    "print('Number of input files:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: float64 (2387, 40) ./data/P11-4-17-2018/1. Right Hand/fNIR_data.txt\n",
      "label: 1 ./data/P11-4-17-2018/1. Right Hand/fNIR_data.txt\n",
      "data: float64 (1243, 40) ./data/P11-4-17-2018/2. Both Hands/fNIR_data.txt\n",
      "label: 2 ./data/P11-4-17-2018/2. Both Hands/fNIR_data.txt\n",
      "data: float64 (2388, 40) ./data/P11-4-17-2018/3. Left hand/fNIR_data.txt\n",
      "label: 3 ./data/P11-4-17-2018/3. Left hand/fNIR_data.txt\n",
      "data: float64 (1200, 40) ./data/P11-4-17-2018/5. Left Leg/fNIR_data.txt\n",
      "label: 5 ./data/P11-4-17-2018/5. Left Leg/fNIR_data.txt\n",
      "data: float64 (2371, 40) ./data/P12-4-17-2018/1. Right Hand/fNIR_data.txt\n",
      "label: 1 ./data/P12-4-17-2018/1. Right Hand/fNIR_data.txt\n",
      "data: float64 (1209, 40) ./data/P12-4-17-2018/2. Both Hands/fNIR_data.txt\n",
      "label: 2 ./data/P12-4-17-2018/2. Both Hands/fNIR_data.txt\n",
      "data: float64 (2377, 40) ./data/P12-4-17-2018/3. Left Hand/fNIR_data.txt\n",
      "label: 3 ./data/P12-4-17-2018/3. Left Hand/fNIR_data.txt\n",
      "data: float64 (1201, 40) ./data/P12-4-17-2018/4. Right Leg/fNIR_data.txt\n",
      "label: 4 ./data/P12-4-17-2018/4. Right Leg/fNIR_data.txt\n",
      "data: float64 (1221, 40) ./data/P12-4-17-2018/5. Left Leg/fNIR_data.txt\n",
      "label: 5 ./data/P12-4-17-2018/5. Left Leg/fNIR_data.txt\n",
      "data: float64 (2404, 40) ./data/P13-4-17-2018/1. Right Hand/fNIR_data.txt\n",
      "label: 1 ./data/P13-4-17-2018/1. Right Hand/fNIR_data.txt\n",
      "data: float64 (1195, 40) ./data/P13-4-17-2018/2. Both Hands/fNIR_data.txt\n",
      "label: 2 ./data/P13-4-17-2018/2. Both Hands/fNIR_data.txt\n",
      "data: float64 (2379, 40) ./data/P13-4-17-2018/3. Left Hand/fNIR_data.txt\n",
      "label: 3 ./data/P13-4-17-2018/3. Left Hand/fNIR_data.txt\n",
      "data: float64 (1202, 40) ./data/P13-4-17-2018/4. Right Leg/fNIR_data.txt\n",
      "label: 4 ./data/P13-4-17-2018/4. Right Leg/fNIR_data.txt\n",
      "data: float64 (1241, 40) ./data/P13-4-17-2018/5. Left Leg/fNIR_data.txt\n",
      "label: 5 ./data/P13-4-17-2018/5. Left Leg/fNIR_data.txt\n",
      "data: float64 (2372, 40) ./data/P14-4-18-2018/1. Right Hand/fNIR_data.txt\n",
      "label: 1 ./data/P14-4-18-2018/1. Right Hand/fNIR_data.txt\n",
      "data: float64 (1201, 40) ./data/P14-4-18-2018/2. Both Hands/fNIR_data.txt\n",
      "label: 2 ./data/P14-4-18-2018/2. Both Hands/fNIR_data.txt\n",
      "data: float64 (2385, 40) ./data/P14-4-18-2018/3. Left Hand/fNIR_data.txt\n",
      "label: 3 ./data/P14-4-18-2018/3. Left Hand/fNIR_data.txt\n",
      "data: float64 (1195, 40) ./data/P14-4-18-2018/4. Right Leg/fNIR_data.txt\n",
      "label: 4 ./data/P14-4-18-2018/4. Right Leg/fNIR_data.txt\n",
      "data: float64 (1228, 40) ./data/P14-4-18-2018/5. Left Leg/fNIR_data.txt\n",
      "label: 5 ./data/P14-4-18-2018/5. Left Leg/fNIR_data.txt\n",
      "data: float64 (2386, 40) ./data/P15-4-18-2018/1. Right Hand/fNIR_data.txt\n",
      "label: 1 ./data/P15-4-18-2018/1. Right Hand/fNIR_data.txt\n",
      "data: float64 (1223, 40) ./data/P15-4-18-2018/2. Both Hands/fNIR_data.txt\n",
      "label: 2 ./data/P15-4-18-2018/2. Both Hands/fNIR_data.txt\n",
      "data: float64 (2378, 40) ./data/P15-4-18-2018/3. Left Hand/fNIR_data.txt\n",
      "label: 3 ./data/P15-4-18-2018/3. Left Hand/fNIR_data.txt\n",
      "data: float64 (1229, 40) ./data/P15-4-18-2018/4. Right Leg/fNIR_data.txt\n",
      "label: 4 ./data/P15-4-18-2018/4. Right Leg/fNIR_data.txt\n",
      "data: float64 (1226, 40) ./data/P15-4-18-2018/5. Left Leg/fNIR_data.txt\n",
      "label: 5 ./data/P15-4-18-2018/5. Left Leg/fNIR_data.txt\n",
      "data: float64 (2383, 40) ./data/P16-4-18-2018/1. Right Hand/fNIR_data.txt\n",
      "label: 1 ./data/P16-4-18-2018/1. Right Hand/fNIR_data.txt\n",
      "data: float64 (1229, 40) ./data/P16-4-18-2018/2. Both Hands/fNIR_data.txt\n",
      "label: 2 ./data/P16-4-18-2018/2. Both Hands/fNIR_data.txt\n",
      "data: float64 (2374, 40) ./data/P16-4-18-2018/3. Left Hand/fNIR_data.txt\n",
      "label: 3 ./data/P16-4-18-2018/3. Left Hand/fNIR_data.txt\n",
      "data: float64 (1195, 40) ./data/P16-4-18-2018/4. Right Leg/fNIR_data.txt\n",
      "label: 4 ./data/P16-4-18-2018/4. Right Leg/fNIR_data.txt\n",
      "data: float64 (1196, 40) ./data/P16-4-18-2018/5. Left Leg/fNIR_data.txt\n",
      "label: 5 ./data/P16-4-18-2018/5. Left Leg/fNIR_data.txt\n",
      "data: float64 (2372, 40) ./data/P17-4-18-2018/1. Right Hand/fNIR_data.txt\n",
      "label: 1 ./data/P17-4-18-2018/1. Right Hand/fNIR_data.txt\n",
      "data: float64 (1219, 40) ./data/P17-4-18-2018/2. Both Hands/fNIR_data.txt\n",
      "label: 2 ./data/P17-4-18-2018/2. Both Hands/fNIR_data.txt\n",
      "data: float64 (2371, 40) ./data/P17-4-18-2018/3. Left Hand/fNIR_data.txt\n",
      "label: 3 ./data/P17-4-18-2018/3. Left Hand/fNIR_data.txt\n",
      "data: float64 (1222, 40) ./data/P17-4-18-2018/4. Right Leg/fNIR_data.txt\n",
      "label: 4 ./data/P17-4-18-2018/4. Right Leg/fNIR_data.txt\n",
      "data: float64 (1221, 40) ./data/P17-4-18-2018/5. Left Leg/fNIR_data.txt\n",
      "label: 5 ./data/P17-4-18-2018/5. Left Leg/fNIR_data.txt\n",
      "data: float64 (2394, 40) ./data/P18-4-19-2018/1. Right Hand/fNIR_data.txt\n",
      "label: 1 ./data/P18-4-19-2018/1. Right Hand/fNIR_data.txt\n",
      "data: float64 (1200, 40) ./data/P18-4-19-2018/2. Both Hands/fNIR_data.txt\n",
      "label: 2 ./data/P18-4-19-2018/2. Both Hands/fNIR_data.txt\n",
      "data: float64 (2378, 40) ./data/P18-4-19-2018/3. Left Hand/fNIR_data.txt\n",
      "label: 3 ./data/P18-4-19-2018/3. Left Hand/fNIR_data.txt\n",
      "data: float64 (1196, 40) ./data/P18-4-19-2018/4. Right Leg/fNIR_data.txt\n",
      "label: 4 ./data/P18-4-19-2018/4. Right Leg/fNIR_data.txt\n",
      "data: float64 (1195, 40) ./data/P18-4-19-2018/5. Left Leg/fNIR_data.txt\n",
      "label: 5 ./data/P18-4-19-2018/5. Left Leg/fNIR_data.txt\n",
      "data: float64 (2371, 40) ./data/P19-4-19-2018/1. Right Hand/fNIR_data.txt\n",
      "label: 1 ./data/P19-4-19-2018/1. Right Hand/fNIR_data.txt\n",
      "data: float64 (1193, 40) ./data/P19-4-19-2018/2. Both Hands/fNIR_data.txt\n",
      "label: 2 ./data/P19-4-19-2018/2. Both Hands/fNIR_data.txt\n",
      "data: float64 (2382, 40) ./data/P19-4-19-2018/3. Left Hand/fNIR_data.txt\n",
      "label: 3 ./data/P19-4-19-2018/3. Left Hand/fNIR_data.txt\n",
      "data: float64 (1195, 40) ./data/P19-4-19-2018/4. Right Leg/fNIR_data.txt\n",
      "label: 4 ./data/P19-4-19-2018/4. Right Leg/fNIR_data.txt\n",
      "data: float64 (1226, 40) ./data/P19-4-19-2018/5. Left Leg/fNIR_data.txt\n",
      "label: 5 ./data/P19-4-19-2018/5. Left Leg/fNIR_data.txt\n",
      "data: float64 (2380, 40) ./data/P20-4-19-2018/1. Right Hand/fNIR_data.txt\n",
      "label: 1 ./data/P20-4-19-2018/1. Right Hand/fNIR_data.txt\n",
      "data: float64 (1197, 40) ./data/P20-4-19-2018/2. Both Hands/fNIR_data.txt\n",
      "label: 2 ./data/P20-4-19-2018/2. Both Hands/fNIR_data.txt\n",
      "data: float64 (2430, 40) ./data/P20-4-19-2018/3. Left Hand/fNIR_data.txt\n",
      "label: 3 ./data/P20-4-19-2018/3. Left Hand/fNIR_data.txt\n",
      "data: float64 (1207, 40) ./data/P20-4-19-2018/5. Left Leg/fNIR_data.txt\n",
      "label: 5 ./data/P20-4-19-2018/5. Left Leg/fNIR_data.txt\n",
      "Number of inputs and labels: 48 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "data, labels = [], []\n",
    "for each in range(0, len(df), 1):\n",
    "    # Input data\n",
    "    dfmat = df[each].as_matrix()\n",
    "    print('data:', dfmat.dtype, dfmat.shape, allpaths[each])\n",
    "    data.append(dfmat)\n",
    "    # Labels: WARNING: be very careful about [21:22] if you change the path this might change too!\n",
    "    label = allpaths[each][21:22]\n",
    "    print('label:', label, allpaths[each])\n",
    "    labels.append(label)\n",
    "print('Number of inputs and labels:', len(data), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is very much like a convolution for extracting the windows\n",
    "# size/width, stride/overlap, padding, dilation, num filters/out channel\n",
    "def minibatching(X, Y, stride, width):\n",
    "    Xmb, Ymb = [], []\n",
    "    print(len(X), len(Y))\n",
    "    # 1st and 1st\n",
    "    for eachX in range(len(X)):\n",
    "        num_mb = ((X[eachX].shape[0]-width)//stride)+1\n",
    "        for each in range(num_mb):\n",
    "            # The max is (num_mb-1)*stride+width==X[idx].shape[0]\n",
    "            # The last each is (num_mb-1)\n",
    "            # each = ((each-1)*stride)+width\n",
    "            each *= stride\n",
    "            Xmb.append(X[eachX][each:each+width])\n",
    "            # There is only one label for one image signal or signal window or temporal window\n",
    "            #Ymb.append(Y[eachX][each:each+1])\n",
    "            Ymb.append(Y[eachX])\n",
    "    return Xmb, Ymb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 48\n",
      "69615 69615\n",
      "(250, 40) float64\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Width is based on the sampling rate which is roughly about 233 points per window\n",
    "# for 10sec rest and 20 sec activity\n",
    "width = 250\n",
    "Xmb, Ymb = minibatching(X=data, Y=labels, stride=1, width=width)\n",
    "# for eachX, eachY in zip(Xmb, Ymb):\n",
    "#     print(eachX.shape, eachY)\n",
    "print(len(Xmb), len(Ymb))\n",
    "print(Xmb[0].shape, Xmb[0].dtype)\n",
    "print(Ymb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69615, 250, 40) float64 (69615,) int64\n"
     ]
    }
   ],
   "source": [
    "# Conversion from python list to numpy array\n",
    "X, Y=np.array(object=Xmb, dtype=float), np.array(object=Ymb, dtype=int)\n",
    "print(X.shape, X.dtype, Y.shape, Y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48730, 250, 40) (20885, 250, 40) (48730,) (20885,)\n",
      "float64 float64 int64 int64\n"
     ]
    }
   ],
   "source": [
    "# Now I should devide the data into train and test\n",
    "# Train and valid split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 30% of the training data/ entire training data is assigned to validation.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.30)\n",
    "print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)\n",
    "print(Xtrain.dtype, Xtest.dtype, Ytrain.dtype, Ytest.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # standardizing/normalizing the train and test data\n",
    "# # def standardize(train, test):\n",
    "# # \"\"\" Standardize data \"\"\"\n",
    "# # # Standardize train and test\n",
    "# # X_train = (train - np.mean(train, axis=0)[None,:,:]) / np.std(train, axis=0)[None,:,:]\n",
    "# # X_test = (test - np.mean(test, axis=0)[None,:,:]) / np.std(test, axis=0)[None,:,:]\n",
    "# # return X_train, X_test\n",
    "\n",
    "# Xtrain = (Xtrain - Xtrain.mean(axis=0))/ Xtrain.std(axis=0)\n",
    "# Xtest = (Xtest - Xtest.mean(axis=0))/ Xtest.std(axis=0)\n",
    "# print(Xtrain.shape, Xtrain.dtype)\n",
    "# print(Xtest.shape, Xtest.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Xtrain.mean(axis=0), Xtrain.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Xtest.mean(axis=0), Xtest.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34111, 250, 40) (14619, 250, 40) (20885, 250, 40) float64 float64 float64\n",
      "(34111,) (14619,) (20885,) int64 int64 int64\n"
     ]
    }
   ],
   "source": [
    "# Now separating train and validation set\n",
    "# 30% of the training data/ entire training data is assigned to validation.\n",
    "Xtrain, Xvalid, Ytrain, Yvalid = train_test_split(Xtrain, Ytrain, test_size=0.30)\n",
    "print(Xtrain.shape, Xvalid.shape, Xtest.shape, Xtrain.dtype, Xvalid.dtype, Xtest.dtype)\n",
    "print(Ytrain.shape, Yvalid.shape, Ytest.shape, Ytrain.dtype, Yvalid.dtype, Ytest.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.8.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(input_size, output_size):\n",
    "    #     N, W, Cin = Xvalid.shape[0], Xvalid.shape[1], Xvalid.shape[2]\n",
    "    Xinputs = tf.placeholder(dtype=tf.float32, shape=[None, *input_size], name='Xinputs')\n",
    "    \n",
    "    #     N, Cout = Yvalid.shape[0], Yvalid.shape[1]\n",
    "    Yindices = tf.placeholder(dtype=tf.int32, shape=[None], name='Yindices')\n",
    "    \n",
    "    # # Batchnorm mode: training and inference/testing/validation\n",
    "    # #is_bn_training = tf.placeholder(dtype=tf.bool, shape=[], name='is_bn_training')\n",
    "    # training = tf.placeholder(dtype=tf.bool, shape=[], name='training')\n",
    "\n",
    "    # returning input data/sequences, output labels/classes\n",
    "    return Xinputs, Yindices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator/ classifier/ recognizer\n",
    "def discriminator(Xinputs, input_size, output_size, hidden_size, reuse=False, alpha=0.1, training=True):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        \n",
    "        # Flatten/Vectorize the input data tensor for FC/fully connected layer/Dense Layer\n",
    "        Xinputs_vec = tf.reshape(tensor=Xinputs, shape=[-1, input_size[0]*input_size[1]])\n",
    "        \n",
    "        # First fully connected layer\n",
    "        h1 = tf.layers.dense(inputs=Xinputs_vec, units=hidden_size)\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)\n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        h2 = tf.layers.dense(inputs=nl1, units=hidden_size)\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)\n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=nl2, units=output_size)   \n",
    "        #predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "        # return output logits for loss and accuracy\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the forward propagation of the model to calculate the loss.\n",
    "def model_loss(Xinputs, Yindices, input_size, output_size, hidden_size):\n",
    "    \n",
    "    # Creating logits and labels\n",
    "    Ylogits = discriminator(Xinputs=Xinputs, input_size=input_size, output_size=output_size, \n",
    "                            hidden_size=hidden_size)\n",
    "    Ylabels = tf.one_hot(indices=Yindices, depth=output_size, dtype=Ylogits.dtype)\n",
    "    \n",
    "    # Loss using logits and labels\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Ylogits, labels=Ylabels))\n",
    "    \n",
    "    # Accuracy using logits and labels\n",
    "    acc_tensor = tf.equal(x=tf.argmax(axis=1, input=Ylogits), y=tf.argmax(axis=1, input=Ylabels))\n",
    "    acc = tf.reduce_mean(axis=0, input_tensor=tf.cast(dtype=tf.float32, x=acc_tensor))\n",
    "\n",
    "    # returning loss and accuracy\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, output_size, hidden_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.Xinputs, self.Yindices = model_input(input_size=input_size, output_size=output_size)\n",
    "\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.loss, self.acc = model_loss(Xinputs=self.Xinputs, Yindices=self.Yindices,\n",
    "                                         input_size=input_size, output_size=output_size, hidden_size=hidden_size)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        #self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)\n",
    "        self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(X, Y, batch_size):\n",
    "    \"\"\" Return a generator for batches \"\"\"\n",
    "    n_batches = len(X) // batch_size\n",
    "    X, Y = X[:n_batches*batch_size], Y[:n_batches*batch_size]\n",
    "\n",
    "    # Loop over batches and yield\n",
    "    for b in range(0, len(X), batch_size):\n",
    "        yield X[b:b+batch_size], Y[b:b+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Hyper-parameters\n",
    "\n",
    "# Number of classes Output layer or target labels\n",
    "assert Ytrain.max()==Ytest.max()==Yvalid.max(), 'Number of output classes is the same in training, validation and testing data.'\n",
    "\n",
    "# Hidden layer\n",
    "input_size = [Xvalid.shape[1], Xvalid.shape[2]]\n",
    "hidden_size = Xvalid.shape[1]* Xvalid.shape[2]\n",
    "output_size = Yvalid.max()\n",
    "\n",
    "# learning parameters\n",
    "batch_size = Xvalid.shape[0]//10 # experience mini-batch size\n",
    "train_epochs = 10              # max number of training episodes/epochs\n",
    "learning_rate = 0.001            # learning rate for training/optimization/adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yvalid.max(), Yvalid.min(): 5 1\n",
      "Yvalid.shape: (14619,)\n",
      "Xvalid.shape: (14619, 250, 40)\n"
     ]
    }
   ],
   "source": [
    "print('Yvalid.max(), Yvalid.min():', Yvalid.max(), Yvalid.min())\n",
    "print('Yvalid.shape:', Yvalid.shape)\n",
    "print('Xvalid.shape:', Xvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = MLP(input_size=input_size, hidden_size=hidden_size, output_size=output_size, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor of shape [10000,10000] and type float\n\t [[Node: discriminator/dense/kernel/Adam/Initializer/zeros = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [10000,10000] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'discriminator/dense/kernel/Adam/Initializer/zeros', defined at:\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-24-24426cedf01b>\", line 3, in <module>\n    model = MLP(input_size=input_size, hidden_size=hidden_size, output_size=output_size, learning_rate=learning_rate)\n  File \"<ipython-input-20-5a5e1f8a426b>\", line 13, in __init__\n    self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 424, in minimize\n    name=name)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 600, in apply_gradients\n    self._create_slots(var_list)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/adam.py\", line 131, in _create_slots\n    self._zeros_slot(v, \"m\", self._name)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 1150, in _zeros_slot\n    new_slot_variable = slot_creator.create_zeros_slot(var, op_name)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 181, in create_zeros_slot\n    colocate_with_primary=colocate_with_primary)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 155, in create_slot_with_initializer\n    dtype)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 65, in _create_slot_var\n    validate_shape=validate_shape)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1317, in get_variable\n    constraint=constraint)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1079, in get_variable\n    constraint=constraint)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 425, in get_variable\n    constraint=constraint)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 394, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 786, in _get_single_variable\n    use_resource=use_resource)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2220, in variable\n    use_resource=use_resource)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2210, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2193, in default_variable_creator\n    constraint=constraint)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 235, in __init__\n    constraint=constraint)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 343, in _init_from_args\n    initial_value(), name=\"initial_value\", dtype=dtype)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 770, in <lambda>\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py\", line 99, in __call__\n    return array_ops.zeros(shape, dtype)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1626, in zeros\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2717, in fill\n    \"Fill\", dims=dims, value=value, name=name)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor of shape [10000,10000] and type float\n\t [[Node: discriminator/dense/kernel/Adam/Initializer/zeros = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [10000,10000] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [10000,10000] and type float\n\t [[Node: discriminator/dense/kernel/Adam/Initializer/zeros = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [10000,10000] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-4ba61fa38a40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Initialize all the model parameters/variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#     # Restoring/loading/uploading the trained and validated model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [10000,10000] and type float\n\t [[Node: discriminator/dense/kernel/Adam/Initializer/zeros = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [10000,10000] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'discriminator/dense/kernel/Adam/Initializer/zeros', defined at:\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-24-24426cedf01b>\", line 3, in <module>\n    model = MLP(input_size=input_size, hidden_size=hidden_size, output_size=output_size, learning_rate=learning_rate)\n  File \"<ipython-input-20-5a5e1f8a426b>\", line 13, in __init__\n    self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 424, in minimize\n    name=name)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 600, in apply_gradients\n    self._create_slots(var_list)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/adam.py\", line 131, in _create_slots\n    self._zeros_slot(v, \"m\", self._name)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 1150, in _zeros_slot\n    new_slot_variable = slot_creator.create_zeros_slot(var, op_name)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 181, in create_zeros_slot\n    colocate_with_primary=colocate_with_primary)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 155, in create_slot_with_initializer\n    dtype)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 65, in _create_slot_var\n    validate_shape=validate_shape)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1317, in get_variable\n    constraint=constraint)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1079, in get_variable\n    constraint=constraint)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 425, in get_variable\n    constraint=constraint)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 394, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 786, in _get_single_variable\n    use_resource=use_resource)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2220, in variable\n    use_resource=use_resource)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2210, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 2193, in default_variable_creator\n    constraint=constraint)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 235, in __init__\n    constraint=constraint)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 343, in _init_from_args\n    initial_value(), name=\"initial_value\", dtype=dtype)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 770, in <lambda>\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py\", line 99, in __call__\n    return array_ops.zeros(shape, dtype)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1626, in zeros\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2717, in fill\n    \"Fill\", dims=dims, value=value, name=name)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor of shape [10000,10000] and type float\n\t [[Node: discriminator/dense/kernel/Adam/Initializer/zeros = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [10000,10000] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# We should save the after training and validation\n",
    "saver = tf.train.Saver() \n",
    "\n",
    "# Loss and accuracy of the model for training and validation\n",
    "train_loss_mean, valid_loss_mean = [], []\n",
    "train_acc_mean, valid_acc_mean = [], []\n",
    "\n",
    "# now that we can calculate loss and optimize, we can start a session for calculating the error.\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize all the model parameters/variables\n",
    "    sess.run(fetches=tf.global_variables_initializer())\n",
    "    \n",
    "    #     # Restoring/loading/uploading the trained and validated model\n",
    "    #     saver.restore(sess,'checkpoints/mlp-fnirs-har.ckpt')\n",
    "    \n",
    "    # for every epoch start feeding the arrays into the tensors in the model\n",
    "    for epoch in range(train_epochs):\n",
    "        train_loss, valid_loss = [], []\n",
    "        train_acc, valid_acc = [], []\n",
    "        \n",
    "        # Training\n",
    "        for Xinputs, Yindices in get_batches(X=Xtrain, Y=Ytrain, batch_size=batch_size):\n",
    "            feed_dict = {model.Xinputs: Xinputs, model.Yindices: Yindices}\n",
    "            loss, acc, _ = sess.run(fetches=[model.loss, model.acc, model.opt], feed_dict=feed_dict)\n",
    "            train_loss.append(loss)\n",
    "            train_acc.append(acc)\n",
    "            \n",
    "        # printing out train and validation loss\n",
    "        print('epoch:', epoch+1, 'train_loss:', np.mean(train_loss), 'train_acc:', np.mean(train_acc))\n",
    "        \n",
    "        # Saving the losses for plotting\n",
    "        train_loss_mean.append(np.mean(train_loss))\n",
    "        train_acc_mean.append(np.mean(train_acc))\n",
    "        \n",
    "        # Validation\n",
    "        for Xinputs, Yindices in get_batches(X=Xvalid, Y=Yvalid, batch_size=batch_size):\n",
    "            feed_dict = {model.Xinputs: Xinputs, model.Yindices: Yindices}\n",
    "            loss, acc = sess.run(fetches=[model.loss, model.acc], feed_dict=feed_dict)\n",
    "            valid_loss.append(loss)\n",
    "            valid_acc.append(acc)\n",
    "        \n",
    "        # printing out train and validation loss\n",
    "        print('epoch:', epoch+1, 'valid_loss:', np.mean(valid_loss), 'valid_acc:', np.mean(valid_acc))\n",
    "\n",
    "        # Saving the losses for plotting\n",
    "        valid_loss_mean.append(np.mean(valid_loss))\n",
    "        valid_acc_mean.append(np.mean(valid_acc))        \n",
    "    \n",
    "    # Saving the trained and validated model\n",
    "    saver.save(sess,'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as mplot\n",
    "%matplotlib inline\n",
    "\n",
    "mplot.plot(train_loss_mean, label='train_loss_mean')\n",
    "mplot.plot(valid_loss_mean, label='valid_loss_mean')\n",
    "mplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mplot.plot(train_acc_mean, label='train_acc_mean')\n",
    "mplot.plot(valid_acc_mean, label='valid_acc_mean')\n",
    "mplot.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/model.ckpt\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to get matching files on checkpoints/model.ckpt: Not found: checkpoints; No such file or directory\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-46-4ba61fa38a40>\", line 2, in <module>\n    saver = tf.train.Saver()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1338, in __init__\n    self.build()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1347, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1384, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 835, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 472, in _AddRestoreOps\n    restore_sequentially)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 886, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1463, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to get matching files on checkpoints/model.ckpt: Not found: checkpoints; No such file or directory\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to get matching files on checkpoints/model.ckpt: Not found: checkpoints; No such file or directory\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-431f2abaffbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Restoring/loading/uploading the trained and validated model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'checkpoints/model.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Saving the test loss for every batch/minibtch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1800\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1801\u001b[0m       sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1802\u001b[0;31m                {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to get matching files on checkpoints/model.ckpt: Not found: checkpoints; No such file or directory\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/asyncio/base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-46-4ba61fa38a40>\", line 2, in <module>\n    saver = tf.train.Saver()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1338, in __init__\n    self.build()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1347, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1384, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 835, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 472, in _AddRestoreOps\n    restore_sequentially)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 886, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1463, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to get matching files on checkpoints/model.ckpt: Not found: checkpoints; No such file or directory\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    #sess.run(fetches=tf.global_variables_initializer())\n",
    "    \n",
    "    # Restoring/loading/uploading the trained and validated model\n",
    "    saver.restore(sess,'checkpoints/model.ckpt')\n",
    "    \n",
    "    # Saving the test loss for every batch/minibtch\n",
    "    test_loss, test_acc = [], []\n",
    "    \n",
    "    # Testing\n",
    "    for Xinputs, Yindices in get_batches(X=Xtest, Y=Ytest, batch_size=batch_size):\n",
    "        feed_dict = {model.Xinputs: Xinputs, model.Yindices: Yindices}\n",
    "        loss, acc = sess.run(fetches=[model.loss, model.acc], feed_dict=feed_dict)\n",
    "        test_loss.append(loss)\n",
    "        test_acc.append(acc)\n",
    "        \n",
    "    # Printing the test loss\n",
    "    print('test_loss:', np.mean(test_loss), 'test acc', np.mean(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFO:tensorflow:Restoring parameters from checkpoints/mlp-fnirs-har.ckpt\n",
    "# test_loss: 0.16514638 test acc 0.807716"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
