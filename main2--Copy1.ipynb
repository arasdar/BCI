{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL Brain project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O10test1.txt   O2valid3.txt  O4valid2.txt  O6valid1.txt  O8train.txt\r\n",
      "O10test2.txt   O3test1.txt   O4valid3.txt  O6valid2.txt  O8valid1.txt\r\n",
      "O10test3.txt   O3test2.txt   O5test1.txt   O6valid3.txt  O8valid2.txt\r\n",
      "O10train.txt   O3test3.txt   O5test2.txt   O7test1.txt   O8valid3.txt\r\n",
      "O10valid1.txt  O3train.txt   O5test3.txt   O7test2.txt   O9test1.txt\r\n",
      "O10valid2.txt  O3valid1.txt  O5train.txt   O7test3.txt   O9test2.txt\r\n",
      "O10valid3.txt  O3valid2.txt  O5valid1.txt  O7train.txt   O9test3.txt\r\n",
      "O2test1.txt    O3valid3.txt  O5valid2.txt  O7valid1.txt  O9train.txt\r\n",
      "O2test2.txt    O4test1.txt   O5valid3.txt  O7valid2.txt  O9valid1.txt\r\n",
      "O2test3.txt    O4test2.txt   O6test1.txt   O7valid3.txt  O9valid2.txt\r\n",
      "O2train.txt    O4test3.txt   O6test2.txt   O8test1.txt   O9valid3.txt\r\n",
      "O2valid1.txt   O4train.txt   O6test3.txt   O8test2.txt\r\n",
      "O2valid2.txt   O4valid1.txt  O6train.txt   O8test3.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls /home/arasdar/datasets/DL-BrainBody/Control/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P10test1.txt   P11valid3.txt  P4valid2.txt  P6valid1.txt  P8train.txt\r\n",
      "P10test2.txt   P1test1.txt    P4valid3.txt  P6valid2.txt  P8valid1.txt\r\n",
      "P10test3.txt   P1test2.txt    P5test1.txt   P6valid3.txt  P8valid2.txt\r\n",
      "P10train.txt   P1test3.txt    P5test2.txt   P7test1.txt   P8valid3.txt\r\n",
      "P10valid1.txt  P1train.txt    P5test3.txt   P7test2.txt   P9test1.txt\r\n",
      "P10valid2.txt  P1valid1.txt   P5train.txt   P7test3.txt   P9test2.txt\r\n",
      "P10valid3.txt  P1valid2.txt   P5valid1.txt  P7train.txt   P9test3.txt\r\n",
      "P11test1.txt   P1valid3.txt   P5valid2.txt  P7valid1.txt  P9train.txt\r\n",
      "P11test2.txt   P4test1.txt    P5valid3.txt  P7valid2.txt  P9valid1.txt\r\n",
      "P11test3.txt   P4test2.txt    P6test1.txt   P7valid3.txt  P9valid2.txt\r\n",
      "P11train.txt   P4test3.txt    P6test2.txt   P8test1.txt   P9valid3.txt\r\n",
      "P11valid1.txt  P4train.txt    P6test3.txt   P8test2.txt\r\n",
      "P11valid2.txt  P4valid1.txt   P6train.txt   P8test3.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls /home/arasdar/datasets/DL-BrainBody/PD/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_02train = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/Control/O2train.txt', header=None, \n",
    "                        delim_whitespace=True)\n",
    "X_03train = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/Control/O3train.txt', header=None, \n",
    "                        delim_whitespace=True)\n",
    "X_04train = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/Control/O4train.txt', header=None, \n",
    "                        delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_10train = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/PD/P10train.txt', header=None, \n",
    "                        delim_whitespace=True)\n",
    "X_11train = pd.read_csv(filepath_or_buffer='/home/arasdar/datasets/DL-BrainBody/PD/P11train.txt', header=None, \n",
    "                        delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = np.array([X_02train.values, X_03train.values, X_04train.values, X_10train.values, X_11train.values])\n",
    "Ytrain = np.array([               0,                0,                0,                1,                1])\n",
    "# Ytrain = np.array([Controllllllllll, Controllllllllll, Controllllllllll, PDDDDDDDDDDDDDDD, PDDDDDDDDDDDDD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 81920, 56), (5,), dtype('float64'), dtype('int64'))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape, Ytrain.shape, Xtrain.dtype, Ytrain.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 1, 81920, 56), (5,))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.reshape([5, 1, -1, 56]).shape, Ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "Default GPU Device: \n"
     ]
    }
   ],
   "source": [
    "# In this one we should define and detect GPUs for tensorflow\n",
    "# GPUs or CPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 81920, 56), (5, 40, 8, 256, 56), (200, 8, 256, 56))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# five subjects, 8 activities, 10 seconds each, sr=256 samples/sec\n",
    "# 256 sampling rate: 256 samples per second\n",
    "Xtrain.shape, \\\n",
    "Xtrain.reshape(5, 40, 8, 256, 56).shape, \\\n",
    "Xtrain.reshape([5*40, 8, 256, 56]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 40)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Ytrain.reshape([-1, 1]) * np.ones([5, 40])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Ytrain.reshape([-1, 1]) * np.ones([5, 40])).reshape([-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = Xtrain.reshape([-1, 8, 256, 56])\n",
    "Ytrain = (Ytrain.reshape([-1, 1]) * np.ones([5, 40])).reshape([-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Xtrain.copy() # inputs\n",
    "indices = Ytrain.copy() # outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 8, 256, 56), (200,))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = Xtrain[0].shape\n",
    "input_size = (8, 256, 56) # one second of each activity which is 1024=256sample/sec*4sec\n",
    "output_size = 2 # num classes\n",
    "hidden_size = 56*2*2*2*2*2*2\n",
    "batch_size = len(indices)\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_input(input_size, hidden_size, batch_size):\n",
    "    # CNN and MLP\n",
    "    inputs = tf.placeholder(dtype=tf.float32, shape=[None, *input_size], name='inputs')\n",
    "    indices = tf.placeholder(dtype=tf.int32, shape=[None], name='indices')    \n",
    "    training = tf.placeholder(dtype=tf.bool, shape=[], name='training') # batchnorm\n",
    "    # RNN\n",
    "    cell = tf.nn.rnn_cell.GRUCell(num_units=hidden_size)\n",
    "    #cell = tf.nn.rnn_cell.LSTMCell(hidden_size)\n",
    "    cells = tf.nn.rnn_cell.MultiRNNCell(cells=[cell], state_is_tuple=True)\n",
    "    initial_state = cells.zero_state(batch_size, tf.float32) # ??? None/batch size\n",
    "    return inputs, indices, training, cells, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'inputs_4:0' shape=(?, 8, 256, 56) dtype=float32>,\n",
       " <tf.Tensor 'indices_4:0' shape=(?,) dtype=int32>,\n",
       " <tf.Tensor 'training_4:0' shape=() dtype=bool>,\n",
       " <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell at 0x7f38a12074a8>,\n",
       " (<tf.Tensor 'MultiRNNCellZeroState_4/GRUCellZeroState/zeros:0' shape=(200, 3584) dtype=float32>,))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input(input_size=input_size, hidden_size=hidden_size, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(inputs, input_size, output_size, # CNN\n",
    "                  batch_size, initial_state, cells, # RNN/MLP/FC\n",
    "                  alpha=0.1, training=False, reuse=False): # NL/BN\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # CNN\n",
    "        # NHWC format\n",
    "        #?, 8, 256, 56\n",
    "        print(inputs.shape)\n",
    "        h1 = tf.layers.conv2d(inputs=inputs, filters=56*2, kernel_size=[8, 3], strides=[1, 4], padding='same')\n",
    "        bn1 = tf.layers.batch_normalization(h1, training=training)\n",
    "        nl1 = tf.maximum(alpha * bn1, bn1)\n",
    "        print(nl1.shape)\n",
    "        h2 = tf.layers.conv2d(inputs=nl1, filters=56*2*2, kernel_size=[8, 3], strides=[1, 4], padding='same')\n",
    "        bn2 = tf.layers.batch_normalization(h2, training=training)\n",
    "        nl2 = tf.maximum(alpha * bn2, bn2)\n",
    "        print(nl2.shape)\n",
    "        h3 = tf.layers.conv2d(inputs=nl2, filters=56*2*2*2, kernel_size=[8, 3], strides=[1, 4], padding='same')\n",
    "        bn3 = tf.layers.batch_normalization(h3, training=training)\n",
    "        nl3 = tf.maximum(alpha * bn3, bn3)\n",
    "        print(nl3.shape)\n",
    "        h4 = tf.layers.conv2d(inputs=nl3, filters=56*2*2*2*2, kernel_size=[8, 3], strides=[1, 4], padding='same')\n",
    "        bn4 = tf.layers.batch_normalization(h4, training=training)\n",
    "        nl4 = tf.maximum(alpha * bn4, bn4)\n",
    "        print(nl4.shape)\n",
    "        h5 = tf.layers.conv2d(inputs=nl4, filters=56*2*2*2*2*2, kernel_size=[8, 3], strides=[8, 4], padding='same')        \n",
    "        bn5 = tf.layers.batch_normalization(h5, training=training)\n",
    "        nl5 = tf.maximum(alpha * bn5, bn5)\n",
    "        print(nl5.shape)\n",
    "\n",
    "        # RNN/MLP/Fully Connected\n",
    "        inputs_rnn = tf.reshape(tensor=nl5, shape=[batch_size, -1, 56*2*2*2*2*2]) # ?, 40, VEC\n",
    "        print(inputs_rnn.shape, initial_state[0].shape) # tuple=True\n",
    "        # uni-directional/bi-directional: dynamic/static on sequence length\n",
    "        outputs_rnn, final_state = tf.nn.dynamic_rnn(cell=cells, inputs=inputs_rnn, initial_state=initial_state)\n",
    "        print(outputs_rnn.shape, final_state[0].shape) # tuple=True\n",
    "        outputs = tf.reshape(outputs_rnn, [-1, 56*2*2*2*2*2]) # ?x40, VEC\n",
    "        print(outputs.shape)\n",
    "\n",
    "        # Output layer\n",
    "        logits = tf.layers.dense(inputs=outputs, units=output_size)\n",
    "        print(logits.shape)\n",
    "        return logits, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(inputs, indices, input_size, output_size, hidden_size, batch_size, cells, initial_state, training):\n",
    "    logits, final_state = discriminator(inputs=inputs, input_size=input_size, output_size=output_size, \n",
    "                                        cells=cells, initial_state=initial_state, batch_size=batch_size, \n",
    "                                        training=training)\n",
    "    labels = tf.one_hot(indices=indices, depth=output_size, dtype=logits.dtype)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "    # If argmax label and argmax logits are equal: true/false\n",
    "    if_equal_bool = tf.equal(x=tf.argmax(axis=1, input=logits), y=tf.argmax(axis=1, input=labels))\n",
    "    accuracy = tf.reduce_mean(axis=0, input_tensor=tf.cast(dtype=tf.float32, x=if_equal_bool))\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(loss, learning_rate):\n",
    "    tvars = tf.trainable_variables()\n",
    "    gvars = [var for var in tvars if var.name.startswith('discriminator')]\n",
    "    # gvars = []\n",
    "    # for var in tvars: \n",
    "    #     if var.name.startswith('discriminator'):\n",
    "    #         gvars.append(var)\n",
    "\n",
    "    # Optimize MLP/CNN\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): # Required for batchnorm (BN)\n",
    "        #opt = tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=g_vars) # CNN\n",
    "        #grads, _ = tf.clip_by_global_norm(t_list=tf.gradients(loss, g_vars), clip_norm=5) # usually around 1-5\n",
    "        grads = tf.gradients(loss, gvars) # RNN\n",
    "        opt = tf.train.AdamOptimizer(learning_rate).apply_gradients(grads_and_vars=zip(grads, gvars))\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, input_size, output_size, hidden_size, batch_size, learning_rate):\n",
    "\n",
    "        # Data of the Model: make the data available inside the framework\n",
    "        self.inputs, self.indices, self.training, cells, self.initial_state = model_input(input_size=input_size, \n",
    "                                                                                          batch_size=batch_size, \n",
    "                                                                                          hidden_size=hidden_size)\n",
    "        # Create the Model: calculating the loss and forwad pass\n",
    "        self.loss, self.acc = model_loss(inputs=self.inputs, indices=self.indices, batch_size=batch_size, \n",
    "                                         input_size=input_size, output_size=output_size, hidden_size=hidden_size,\n",
    "                                         cells=cells, initial_state=self.initial_state, training=self.training)\n",
    "\n",
    "        # Update the model: backward pass and backprop\n",
    "        self.opt = model_opt(loss=self.loss, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_batches(X, Y, batch_size):\n",
    "#     \"\"\" Return a generator for batches \"\"\"\n",
    "#     n_batches = len(X) // batch_size\n",
    "#     X, Y = X[:n_batches*batch_size], Y[:n_batches*batch_size]\n",
    "\n",
    "#     # Loop over batches and yield\n",
    "#     for b in range(0, len(X), batch_size):\n",
    "#         yield X[b:b+batch_size], Y[b:b+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 8, 256, 56)\n",
      "(?, 8, 64, 112)\n",
      "(?, 8, 16, 224)\n",
      "(?, 8, 4, 448)\n",
      "(?, 8, 1, 896)\n",
      "(?, 1, 1, 1792)\n",
      "(200, ?, 1792) (200, 3584)\n",
      "(200, ?, 3584) (200, 3584)\n",
      "(?, 1792)\n",
      "(?, 2)\n"
     ]
    }
   ],
   "source": [
    "graph = tf.reset_default_graph()\n",
    "\n",
    "model = Model(input_size=input_size, hidden_size=hidden_size, output_size=output_size, \n",
    "              learning_rate=learning_rate, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(None), Dimension(8), Dimension(256), Dimension(56)]),\n",
       " (200, 8, 256, 56))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs.shape, inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(None)]), (200,))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.indices.shape, indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits and labels must be same size: logits_size=[400,2] labels_size=[200,2]\n\t [[Node: softmax_cross_entropy_with_logits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](softmax_cross_entropy_with_logits/Reshape, softmax_cross_entropy_with_logits/Reshape_1)]]\n\nCaused by op 'softmax_cross_entropy_with_logits', defined at:\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 346, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 259, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 513, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2817, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2843, in _run_cell\n    return runner(coro)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3018, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3183, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3265, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-120-d44be2882686>\", line 4, in <module>\n    learning_rate=learning_rate, batch_size=batch_size)\n  File \"<ipython-input-118-d3f24f772389>\", line 11, in __init__\n    cells=cells, initial_state=self.initial_state, training=self.training)\n  File \"<ipython-input-116-f3a83f1c5330>\", line 6, in model_loss\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1875, in softmax_cross_entropy_with_logits_v2\n    precise_logits, labels, name=name)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 7142, in softmax_cross_entropy_with_logits\n    name=name)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[400,2] labels_size=[200,2]\n\t [[Node: softmax_cross_entropy_with_logits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](softmax_cross_entropy_with_logits/Reshape, softmax_cross_entropy_with_logits/Reshape_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must be same size: logits_size=[400,2] labels_size=[200,2]\n\t [[Node: softmax_cross_entropy_with_logits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](softmax_cross_entropy_with_logits/Reshape, softmax_cross_entropy_with_logits/Reshape_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-306038dc3840>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m                                            \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                            \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                                            model.initial_state: initial_state})\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must be same size: logits_size=[400,2] labels_size=[200,2]\n\t [[Node: softmax_cross_entropy_with_logits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](softmax_cross_entropy_with_logits/Reshape, softmax_cross_entropy_with_logits/Reshape_1)]]\n\nCaused by op 'softmax_cross_entropy_with_logits', defined at:\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n    handle._run()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 346, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 259, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 513, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2817, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2843, in _run_cell\n    return runner(coro)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3018, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3183, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3265, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-120-d44be2882686>\", line 4, in <module>\n    learning_rate=learning_rate, batch_size=batch_size)\n  File \"<ipython-input-118-d3f24f772389>\", line 11, in __init__\n    cells=cells, initial_state=self.initial_state, training=self.training)\n  File \"<ipython-input-116-f3a83f1c5330>\", line 6, in model_loss\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1875, in softmax_cross_entropy_with_logits_v2\n    precise_logits, labels, name=name)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 7142, in softmax_cross_entropy_with_logits\n    name=name)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"/home/arasdar/anaconda3/envs/env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[400,2] labels_size=[200,2]\n\t [[Node: softmax_cross_entropy_with_logits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](softmax_cross_entropy_with_logits/Reshape, softmax_cross_entropy_with_logits/Reshape_1)]]\n"
     ]
    }
   ],
   "source": [
    "# We should save the after training and validation\n",
    "saver = tf.train.Saver() \n",
    "\n",
    "# Loss and accuracy of the model for training and validation\n",
    "train_loss_mean, train_acc_mean = [], []\n",
    "\n",
    "# now that we can calculate loss and optimize, we can start a session for calculating the error.\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    \n",
    "    # Initialize all the model parameters/variables\n",
    "    sess.run(fetches=tf.global_variables_initializer())\n",
    "    #saver.restore(sess,'checkpoints/model.ckpt')\n",
    "    \n",
    "    # Initializing the RNN states\n",
    "    initial_state = sess.run(model.initial_state)\n",
    "    \n",
    "    # Epoch/episode\n",
    "    for epoch in range(1):\n",
    "        train_loss, valid_acc = [], []\n",
    "        \n",
    "        # Training\n",
    "        #for Xinputs, Yindices in get_batches(X=Xtrain, Y=Ytrain, batch_size=batch_size):\n",
    "        loss, acc, _ = sess.run(fetches=[model.loss, model.acc, model.opt], \n",
    "                                feed_dict={model.inputs: inputs, \n",
    "                                           model.indices: indices, \n",
    "                                           model.training: True, \n",
    "                                           model.initial_state: initial_state})\n",
    "        train_loss.append(loss)\n",
    "        train_acc.append(acc)\n",
    "        \n",
    "    # Saving the losses for plotting\n",
    "    train_loss_mean.append(np.mean(train_loss))\n",
    "    train_acc_mean.append(np.mean(train_acc))\n",
    "    print('train_loss_mean:{}'.format(train_loss_mean))\n",
    "    print('train_acc_mean:{}'.format(train_acc_mean))\n",
    "    \n",
    "    # Saving the trained and validated model\n",
    "    saver.save(sess,'checkpoints/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
